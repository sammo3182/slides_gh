---
title: "潜变量分析（基础）"
subtitle: "政务大数据应用与分析 (80700673)"
author: "胡悦"
institute: "清华大学社会科学学院" 
bibliography: t_politicalData.bib
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    number-sections: true
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    show-notes: false
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
execute: 
  echo: false
editor_options: 
  chunk_output_type: console
editor: 
  render-on-save: true
---

```{r setup}
#| include = FALSE

library(pacman)

p_load(
  psych,
  lavaan,
  drhutools,
  semPlot,
  qgraph,
  haven,
  here,
  corrplot,
  FactoMineR, 
  vcd, 
  factoextra,
  tidyverse
)

# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18),
  axis.title = element_text(size = 22),
  axis.text = element_text(size = 18)
)
```

## 提要 {.unnumbered}

:::: {.columns}

::: {.column width="50%"}

1. 概念与分析逻辑
    - 什么是潜变量
    - 怎么分析
1. 探索性因子分析
1. 验证性因子分析（与结构方程模型）*

:::

::: {.column .fragment .nonincremental width="50%"}
**操作语言**

* R
    + [`psych`](https://personality-project.org/r/psych/vignettes/intro.pdf)
    + [`FactoMineR`](http://factominer.free.fr/book/)
    + [`laavan`](https://lavaan.ugent.be/)*
:::

::::



# 潜变量概念与分析逻辑


## 潜在变量 (Latent variables)

:::{.r-hstack}

![](https://drhuyue.site:10002/sammo3182/figure/lv_angry.jpg){fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/lv_happy.jpg){.fragment fig-align="center" height=600}

:::



## Why bother

:::{.r-stack}
![Liberty leading the people](https://drhuyue.site:10002/sammo3182/figure/lv_liberty.jpg){fig-align="center" height=600}

![战狼II](https://drhuyue.site:10002/sammo3182/figure/lv_patriotism.webp){.fragment fig-align="center" height=600}
:::

:::{.notes}
同样行为背后的动机不同

- Liberty leading the people: the July Revolution of 1830 that toppled King Charles X，最终以查理十世退位导致波旁王朝灭亡、奥尔良公爵路易-菲利普继承王位告终，法国亦开始了奥尔良王朝的统治，但奥尔良王朝也不长命，亡于1848年的法国二月革命。
:::

---

> Vaild [latent variable] measurement is the [cornerstone]{.red} of successful scientific inquiry [@CarpiniKeeter1993, p. 1203].


:::: {.columns}

::: {.column width="30%"}
涵盖所有社科学科

* 抽象
* 复杂
* 综合


:::

::: {.column .fragment width="30%"}
操作性挑战

1. 不可见(Unobservable)
1. 多维度(Multidimensional)
1. 有效果(Consequential)

:::

::: {.column width="40%"}
![[@Stimson1991]](https://drhuyue.site:10002/sammo3182/figure/lv_jamesStimson.jpg){.fragment fig-align="center" height=400}
:::

::::

:::{.notes}
兜里有多少钱，不可见，但维度单一，不宜用潜变量分析，更好的办法是翻兜

The latent variable per se can't be directly measured. But its consequences in opinions and behaviors can be observed.
:::

## 分析逻辑

:::{style="text-align:center"}
共因 [&rarr;]{.red .large} 结果
:::

:::{.fragment}
🌰 个体的社会资本（social capital）
:::

:::{.fragment .nonincremental style="text-align:center; margin-top: 2em"}
指标问题（1~10）：

1. 您是否信任身边人？
1. 您在政府机关有没有亲戚？
1. 您的朋友是否和您的想法经常一致？
:::

:::{.notes}
结果可见，共因不可见，以果推因

Three dimensions of social capital:

1. Trust
1. Norms
1. Networks
:::

## 测量社会资本

:::: {.columns}

::: {.column width="50%"}
指标问题（1~10）：

1. 您是否信任身边人？
1. 您在政府机关有没有亲戚？
1. 您的朋友是否和您的想法经常一致？


累加综合法(additive scales)

$$\tilde{X} = (X_1 + X_2 + X_3)/3.$$
:::

::: {.column .fragment width="50%"}
*潜在问题*

1. 相同权重(equal weight)
1. 结果不稳(extreme value sensitivity)
1. 忽略极化(polarity ignoring)
:::

::::

:::{.fragment .large style="text-align:center; margin-top: 2em"}
*如何做得更好？*
:::


## 因子分析基本模型

:::: {.columns}

::: {.column width="50%"}
**连续因子模型**

1. 探索性因子分析(EFA)
1. 验证性因子分析(CFA)
1. 结构方程模型(SEM)
:::

::: {.column .fragment .semi-fade-out width="50%"}
**离散回应模型**

- 项目反应理论(IRT)
    - 项目反应聚合估计
:::

::::


# 因子分析


## 基本原理

:::{style="text-align:center"}
共因 [&rarr;]{.red .large} 结果

&dArr;

潜在变量 [&rarr;]{.red .large} 可见指标(indicators)
:::

:::{.fragment style="text-align:center"}
&dArr;

![Minimum factors for the variances](https://drhuyue.site:10002/sammo3182/figure/lv_jiangwei.jpg){fig-align="center" height=300}
:::

:::{.notes}
determine the minimum number of hypothetical factors or components that account for the variance between variables.
:::


## 操作挑战

:::: {.columns}

::: {.column width="60%"}

![](https://drhuyue.site:10002/sammo3182/figure/lv_efa.png){fig-align="center" height=600}
:::

::: {.column width="40%"}
- **目标**: Fewer dimensons
- **决策**: 
    - 降到几维
      - 一维是是不是最优选择
    - 如何降维
      - 克服“累加法”缺陷
    
::: {.fragment .r-fit-text}
*Exploratory* FA
:::

:::
::::


## 探索式因子分析

:::{.callout-important}
## 根本式 [@Quinn2004]

$$X^* = \Phi\Lambda' + \epsilon,$$

:::{style="text-align:center"}
**$X^*$**: 潜在变量  
**&Phi;**: 指标选择  
**&Lambda;**: 单项贡献（a.k.a., factor loading）  
**&epsilon;** 选择性误差
:::
:::


:::{.fragment style="text-align:center"}
*执行步骤*

1. 个数选择
1. 因子提取
1. Rotation
1. 因子合成
1. 结果检验
:::



## 🌰：人格测试

19,719 参与者, Big5 personality

![](https://drhuyue.site:10002/sammo3182/figure/lv_big5.jpg){fig-align="center" height=500}

## 实证数据

```{r data-big5, cache=TRUE}
df_big5 <- read_csv("https://drhuyue.site:10002/sammo3182/data/lv_dataBIG5.csv")
df_big5[df_big5 == 0] <- NA
df_big5
```


:::{.notes}
经验开放性Openness, 尽责性Conscientiousness, 外向型Extraversion, 亲和性Agreeableness, 情绪不稳定型Neuroticism

https://quantdev.ssri.psu.edu/tutorials/intro-basic-exploratory-factor-analysis
:::

:::{.fragment style="text-align:center; margin-top: 1em"}

- 降到几维
- 如何降维
:::



## 维度选择

:::{.callout-note}
## 选择依据

概念关系 + 数据关系
:::

```{r corrplot}
#| label: corrplot
#| fig-align: center
#| fig-height: 4

df_big5 %>%
  select(8:57) %>%
  cor(use = "complete.obs") %>%
  corrplot(order = "hclust",
           tl.col = 'black',
           tl.cex = .75) 
```



## Horn's Parallel Analysis

:::: {.columns}

::: {.column width="30%"}
已知观测数据集**O**<sub>m&times;n</sub>

1. 创建随机数据集**R**<sub>m&times;n</sub>;  
1. 相关矩阵<sub>**R**</sub> &rarr; &lambda;<sub>**R**</sub>;  
1. 相关矩阵<sub>**Ok**</sub> &rarr; &lambda;<sub>**Ok**</sub> (k为因子数)
1. &lambda;<sub>**Ok**</sub> vs. &lambda;<sub>**R**</sub>

:::{.fragment}
标准：

A. 如果&lambda;<sub>**Ok**</sub> < &lambda;<sub>**R**</sub>, 则 ~~k~~
B. Kaiser criterion: &lambda; < 1 不可
:::

:::

::: {.column width="70%"}
```{r parallelAnalysis}
#| label: parallelAnalysis
#| fig-align: center
#| fig-height: 6
#| results: hide

df_big5 %>%
  select(8:57) %>%
  fa.parallel(fa = "fa")
```
:::

::::



## 因子提取(Factor Extraction)

:::{.nonincremental}
+ Minimum residual (OLS)
+ Principal axes
+ Alpha factoring
+ Weighted least squares
+ Minimum rank
+ [Maximum likelihood (ML, minimum &chi;<sup>2</sup> goodness of fit)]{.red}

:::


## Rotation: 效果优化

![](https://drhuyue.site:10002/sammo3182/figure/lv_rotation.jpg){fig-align="center" height=450}   

:::{.notes}
Rotation:

A pattern of loadings where each item loads strongly on only one of the factors, and much more weakly on the other factors.
:::

* Orthogonal: Varimax, quartimax, bentlerT, geominT, bifactor  
* [Oblique]{.red}: Oblimin, quartimin, simplimax, bentlerQ, geominQ, biquartimin


:::{.notes}
Orthogonal(正交)

优选Oblique，允许factor间correlate
:::


## 合成结果

:::: {.columns}

::: {.column width="45%"}

ML + Oblimin

```{r efa}
#| label: efa

result_efa <- df_big5 %>%
  select(8:57) %>%
  fa(nfactors = 5,
     rotate = "oblimin",
     fm = "ml")

print(result_efa$loadings, cutoff = .296) # the point was picked for the purpose of presentation
```
:::

::: {.column width="55%"}
```{r scores}
#| label: scores
#| fig-align: center
#| fig-height: 8


# Reshape the data to a long format
df_scores_long <- as.data.frame(result_efa$scores) |>
  pivot_longer(cols = everything(), 
               names_to = "Factor", 
               values_to = "Score")

# Create the histograms using ggplot2 and facet by factor
ggplot(df_scores_long, aes(x = Score)) +
  geom_histogram(binwidth = 0.5) +  # Adjust binwidth as needed
  facet_wrap(~ Factor, scales = "free_y", ncol = 1) +  # Facet into 1 column, adjust y scales independently
  labs(x = "Score", y = "Frequency", title = "Histograms of EFA Factors")  # Adjust labels as necessary
```
:::

::::


## 诊断

1. Sum of squared (SS) loading^[特征值]: 1
1. Communality/Uniqueness
1. Root means square of residuals(RMSR): 0.05
1. Tucker-Lewis Indes (TLI): 0.9
1. Reliability test (Crobach's &alpha;)^[为每个因子分别计算]


```{r efa-full}
#| label: efaFull

summary(result_efa)
```


:::{.notes}
- SS：Kaiser criterion: &lambda; < 1 不可
- Communality:  SS of all the factor loadings for a given variable
- Uniqueness: 1 - Communality
- RMSR < 0.05
- TLI > 0.9
- Crobach's &alpha;: 计算因子分布的variables是不是内部一致，除非发表，不大必要
:::


## 注意事项

- EFA是数据探索方法，[不能用作统计推断]{.red}
- EFA应用需满足以下[*假定*]{.red}:
    1. [*Linearity*]{.red} between the observed and latent
    1. [*Sufficient correlation*]{.red} between the observed and latent
    1. [*Homoscedasticity*]{.red}
    1. [*Multivariate normality*]{.red} among the observed
    1. [*No singularity*]{.blue} among the observed
    1. [*Large N*]{.blue}: 10 ~ 300
    1. [*Factorial Simplicity*]{.blue}
- EFA不是唯一合理的降维方式
    - PCA
    - IRT


:::{.notes}
Red: bias; blue: efficiency

1. **Linearity**: EFA assumes a linear relationship between observed variables and underlying factors. This means that changes in the levels of factors are associated with proportional changes in the observed variables.

2. **Multivariate Normality**: Ideally, the observed variables should follow a multivariate normal distribution. While EFA can be performed on data that do not perfectly adhere to this assumption, deviations from multivariate normality can affect the robustness of the factor analysis, especially when estimating parameters and computing fit indices.

3. **Adequate Sample Size**: There is no strict rule for the minimum sample size in EFA, but a larger sample size can provide more reliable and stable factor solutions. A commonly cited guideline suggests having at least 5 to 10 observations per variable, though some scholars recommend a minimum of 300 cases as a more general benchmark.

4. **No Perfect Multicollinearity or Singularity**: While EFA aims to identify the underlying structure by exploring the correlations between variables, perfect multicollinearity (where one variable is a perfect linear combination of others) should be avoided. Similarly, singularity (where some variables are redundant) should be avoided, as these conditions can distort the analysis.

5. **Sufficient Correlation**: For a factor model to be meaningful, there must be some degree of correlation among the observed variables. If variables are completely uncorrelated, there would be no underlying factors to extract. The Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy and Bartlett's test of sphericity are commonly used to assess this assumption.

6. **Homoscedasticity**: The assumption of homoscedasticity (equal variances) among variables is also important for the generalizability of EFA results. Significant heteroscedasticity can lead to biased estimates of factor loadings and uniqueness.

7. **Factorial Simplicity**: This is more of a desired property than a strict assumption. Factorial simplicity implies that each variable should load strongly on one factor and have minimal loadings on other factors. This assumption facilitates the interpretability of factors.

Violations of these assumptions can lead to inaccurate factor solutions, misleading interpretations, and conclusions. It is therefore important to assess these assumptions before proceeding with EFA. In practice, some of these assumptions can be relaxed or addressed through data transformation, adopting robust estimation methods, or increasing the sample size.
:::

## Principal Component Analysis

:::{.callout-important}
## 根本式

$$C = w_iY_i,$$

:::{style="text-align:center"}
**C**: Components， “因子”

**Y**: Measures
:::

:::

![](https://drhuyue.site:10002/sammo3182/figure/lv_pca.png){fig-align="center" height=300}


:::{.notes}
create one or more index variables (Components) from a larger set of measured variables (Y)
:::

## PCA vs. EFA

![](https://drhuyue.site:10002/sammo3182/figure/lv_pcaVsEfa.PNG){fig-align="center" height=500}


::: {.fragment .large style="text-align:center; margin-top: 1em"}
结果或近似，逻辑大不同
:::


:::{.notes}
PCA: measurement to index   

write all variables in terms of a smaller set of features which allows for a maximum amount of variance to be retained in the data.   


EFA: indices to measurement (of a latent variable)   

find a set of features which allow for understanding as much of the correlations between measured variables as possible. individually.
:::



## 如何选择

* PCA最大程度保留可见变量信息，EFA旨在提取不可变量特征；
* 当Variable之间关系不那么紧密或受同一变量影响，PCA > EFA；
* 当估计潜在变量时，PCA可能夸大可见指标的影响

## Bonus: 非连续变量PCA

```{r pca-category}
#| label: pcaCat
#| fig-cap: "Multiple Correspondence Analysis [MCA, @HussonEtAl2010]"
#| fig-height: 6

data(Arthritis)
arthritis_data <- Arthritis[,c(2,3,5)]

arthritis_mca <- MCA(arthritis_data,
    ncp = 3,
    graph = FALSE)

fviz_mca_biplot(arthritis_mca,
               repel = TRUE)
```


## Bonus: 混合变量的PCA

```{r pca-mixed}
#| label: pcaMixed
#| fig-cap: "Factorial Analysis of Mixed Data [FAMD, @Pages2004]"
#| fig-height: 6

data(wine)
wine_data <- wine[,c(1,2,13,22,24,28,30)]
wine_famd <- FAMD(wine_data, graph=FALSE)

fviz_famd_ind(wine_famd,col.ind = "cos2",
             gradient.cols = gb_cols("black", "gold"),
             repel = TRUE)
```

:::{.notes}
https://www.r-bloggers.com/2022/11/pca-for-categorical-variables-in-r/

- Factorial Analysis of Mixed Data
    - cos² statistic (cosine squared): how well a variable, individual, or category is represented by a given factor (dimension)
:::

## 总结

:::: {.columns}
::: {.column width="50%"}

* 潜在变量分析概述
    + *连续性因子分析*
    + 离散型因子分析
    
:::


::: {.column width="50%"}

* EFA
    + 探索性因子分析：通过loading找到潜在变量
    + EFA诊断：Kaiser's criterion
    + EFA vs. PCA: 结果相似，逻辑不同

:::
::::

:::{.fragment}
::: {.r-fit-text}
**待解之题**：观测变量与潜变量之间关系要是[非线性]{.red}的那该怎么办呢？
:::

:::

## 参考文献

::: {#refs}
:::



