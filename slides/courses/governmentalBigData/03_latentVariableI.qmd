---
title: "æ½œå˜é‡åˆ†æï¼ˆåŸºç¡€ï¼‰"
subtitle: "æ”¿åŠ¡å¤§æ•°æ®åº”ç”¨ä¸åˆ†æ (80700673)"
author: "èƒ¡æ‚¦"
institute: "æ¸…åå¤§å­¦ç¤¾ä¼šç§‘å­¦å­¦é™¢" 
bibliography: t_politicalData.bib
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    number-sections: true
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    show-notes: false
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
execute: 
  echo: false
editor_options: 
  chunk_output_type: console
editor: 
  render-on-save: true
---

```{r setup}
#| include = FALSE

library(pacman)

p_load(
  psych,
  lavaan,
  drhutools,
  semPlot,
  qgraph,
  haven,
  here,
  corrplot,
  FactoMineR, 
  vcd, 
  factoextra,
  tidyverse
)

# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18),
  axis.title = element_text(size = 22),
  axis.text = element_text(size = 18)
)
```

## æè¦ {.unnumbered}

:::: {.columns}

::: {.column width="50%"}

1. æ¦‚å¿µä¸åˆ†æé€»è¾‘
    - ä»€ä¹ˆæ˜¯æ½œå˜é‡
    - æ€ä¹ˆåˆ†æ
1. æ¢ç´¢æ€§å› å­åˆ†æ
1. éªŒè¯æ€§å› å­åˆ†æï¼ˆä¸ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼‰*

:::

::: {.column .fragment .nonincremental width="50%"}
**æ“ä½œè¯­è¨€**

* R
    + [`psych`](https://personality-project.org/r/psych/vignettes/intro.pdf)
    + [`FactoMineR`](http://factominer.free.fr/book/)
    + [`laavan`](https://lavaan.ugent.be/)*
:::

::::



# æ½œå˜é‡æ¦‚å¿µä¸åˆ†æé€»è¾‘


## æ½œåœ¨å˜é‡ (Latent variables)

:::{.r-hstack}

![](https://drhuyue.site:10002/sammo3182/figure/lv_angry.jpg){fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/lv_happy.jpg){.fragment fig-align="center" height=600}

:::



## Why bother

:::{.r-stack}
![Liberty leading the people](https://drhuyue.site:10002/sammo3182/figure/lv_liberty.jpg){fig-align="center" height=600}

![æˆ˜ç‹¼II](https://drhuyue.site:10002/sammo3182/figure/lv_patriotism.webp){.fragment fig-align="center" height=600}
:::

:::{.notes}
åŒæ ·è¡Œä¸ºèƒŒåçš„åŠ¨æœºä¸åŒ

- Liberty leading the people: the July Revolution of 1830 that toppled King Charles Xï¼Œæœ€ç»ˆä»¥æŸ¥ç†åä¸–é€€ä½å¯¼è‡´æ³¢æ—ç‹æœç­äº¡ã€å¥¥å°”è‰¯å…¬çˆµè·¯æ˜“-è²åˆ©æ™®ç»§æ‰¿ç‹ä½å‘Šç»ˆï¼Œæ³•å›½äº¦å¼€å§‹äº†å¥¥å°”è‰¯ç‹æœçš„ç»Ÿæ²»ï¼Œä½†å¥¥å°”è‰¯ç‹æœä¹Ÿä¸é•¿å‘½ï¼Œäº¡äº1848å¹´çš„æ³•å›½äºŒæœˆé©å‘½ã€‚
:::

---

> Vaild [latent variable] measurement is the [cornerstone]{.red} of successful scientific inquiry [@CarpiniKeeter1993, p. 1203].


:::: {.columns}

::: {.column width="30%"}
æ¶µç›–æ‰€æœ‰ç¤¾ç§‘å­¦ç§‘

* æŠ½è±¡
* å¤æ‚
* ç»¼åˆ


:::

::: {.column .fragment width="30%"}
æ“ä½œæ€§æŒ‘æˆ˜

1. ä¸å¯è§(Unobservable)
1. å¤šç»´åº¦(Multidimensional)
1. æœ‰æ•ˆæœ(Consequential)

:::

::: {.column width="40%"}
![[@Stimson1991]](https://drhuyue.site:10002/sammo3182/figure/lv_jamesStimson.jpg){.fragment fig-align="center" height=400}
:::

::::

:::{.notes}
å…œé‡Œæœ‰å¤šå°‘é’±ï¼Œä¸å¯è§ï¼Œä½†ç»´åº¦å•ä¸€ï¼Œä¸å®œç”¨æ½œå˜é‡åˆ†æï¼Œæ›´å¥½çš„åŠæ³•æ˜¯ç¿»å…œ

The latent variable per se can't be directly measured. But its consequences in opinions and behaviors can be observed.
:::

## åˆ†æé€»è¾‘

:::{style="text-align:center"}
å…±å›  [&rarr;]{.red .large} ç»“æœ
:::

:::{.fragment}
ğŸŒ° ä¸ªä½“çš„ç¤¾ä¼šèµ„æœ¬ï¼ˆsocial capitalï¼‰
:::

:::{.fragment .nonincremental style="text-align:center; margin-top: 2em"}
æŒ‡æ ‡é—®é¢˜ï¼ˆ1~10ï¼‰ï¼š

1. æ‚¨æ˜¯å¦ä¿¡ä»»èº«è¾¹äººï¼Ÿ
1. æ‚¨åœ¨æ”¿åºœæœºå…³æœ‰æ²¡æœ‰äº²æˆšï¼Ÿ
1. æ‚¨çš„æœ‹å‹æ˜¯å¦å’Œæ‚¨çš„æƒ³æ³•ç»å¸¸ä¸€è‡´ï¼Ÿ
:::

:::{.notes}
ç»“æœå¯è§ï¼Œå…±å› ä¸å¯è§ï¼Œä»¥æœæ¨å› 

Three dimensions of social capital:

1. Trust
1. Norms
1. Networks
:::

## æµ‹é‡ç¤¾ä¼šèµ„æœ¬

:::: {.columns}

::: {.column width="50%"}
æŒ‡æ ‡é—®é¢˜ï¼ˆ1~10ï¼‰ï¼š

1. æ‚¨æ˜¯å¦ä¿¡ä»»èº«è¾¹äººï¼Ÿ
1. æ‚¨åœ¨æ”¿åºœæœºå…³æœ‰æ²¡æœ‰äº²æˆšï¼Ÿ
1. æ‚¨çš„æœ‹å‹æ˜¯å¦å’Œæ‚¨çš„æƒ³æ³•ç»å¸¸ä¸€è‡´ï¼Ÿ


ç´¯åŠ ç»¼åˆæ³•(additive scales)

$$\tilde{X} = (X_1 + X_2 + X_3)/3.$$
:::

::: {.column .fragment width="50%"}
*æ½œåœ¨é—®é¢˜*

1. ç›¸åŒæƒé‡(equal weight)
1. ç»“æœä¸ç¨³(extreme value sensitivity)
1. å¿½ç•¥æåŒ–(polarity ignoring)
:::

::::

:::{.fragment .large style="text-align:center; margin-top: 2em"}
*å¦‚ä½•åšå¾—æ›´å¥½ï¼Ÿ*
:::


## å› å­åˆ†æåŸºæœ¬æ¨¡å‹

:::: {.columns}

::: {.column width="50%"}
**è¿ç»­å› å­æ¨¡å‹**

1. æ¢ç´¢æ€§å› å­åˆ†æ(EFA)
1. éªŒè¯æ€§å› å­åˆ†æ(CFA)
1. ç»“æ„æ–¹ç¨‹æ¨¡å‹(SEM)
:::

::: {.column .fragment .semi-fade-out width="50%"}
**ç¦»æ•£å›åº”æ¨¡å‹**

- é¡¹ç›®ååº”ç†è®º(IRT)
    - é¡¹ç›®ååº”èšåˆä¼°è®¡
:::

::::


# å› å­åˆ†æ


## åŸºæœ¬åŸç†

:::{style="text-align:center"}
å…±å›  [&rarr;]{.red .large} ç»“æœ

&dArr;

æ½œåœ¨å˜é‡ [&rarr;]{.red .large} å¯è§æŒ‡æ ‡(indicators)
:::

:::{.fragment style="text-align:center"}
&dArr;

![Minimum factors for the variances](https://drhuyue.site:10002/sammo3182/figure/lv_jiangwei.jpg){fig-align="center" height=300}
:::

:::{.notes}
determine the minimum number of hypothetical factors or components that account for the variance between variables.
:::


## æ“ä½œæŒ‘æˆ˜

:::: {.columns}

::: {.column width="60%"}

![](https://drhuyue.site:10002/sammo3182/figure/lv_efa.png){fig-align="center" height=600}
:::

::: {.column width="40%"}
- **ç›®æ ‡**: Fewer dimensons
- **å†³ç­–**: 
    - é™åˆ°å‡ ç»´
      - ä¸€ç»´æ˜¯æ˜¯ä¸æ˜¯æœ€ä¼˜é€‰æ‹©
    - å¦‚ä½•é™ç»´
      - å…‹æœâ€œç´¯åŠ æ³•â€ç¼ºé™·
    
::: {.fragment .r-fit-text}
*Exploratory* FA
:::

:::
::::


## æ¢ç´¢å¼å› å­åˆ†æ

:::{.callout-important}
## æ ¹æœ¬å¼ [@Quinn2004]

$$X^* = \Phi\Lambda' + \epsilon,$$

:::{style="text-align:center"}
**$X^*$**: æ½œåœ¨å˜é‡  
**&Phi;**: æŒ‡æ ‡é€‰æ‹©  
**&Lambda;**: å•é¡¹è´¡çŒ®ï¼ˆa.k.a., factor loadingï¼‰  
**&epsilon;** é€‰æ‹©æ€§è¯¯å·®
:::
:::


:::{.fragment style="text-align:center"}
*æ‰§è¡Œæ­¥éª¤*

1. ä¸ªæ•°é€‰æ‹©
1. å› å­æå–
1. Rotation
1. å› å­åˆæˆ
1. ç»“æœæ£€éªŒ
:::



## ğŸŒ°ï¼šäººæ ¼æµ‹è¯•

19,719 å‚ä¸è€…, Big5 personality

![](https://drhuyue.site:10002/sammo3182/figure/lv_big5.jpg){fig-align="center" height=500}

## å®è¯æ•°æ®

```{r data-big5, cache=TRUE}
df_big5 <- read_csv("https://drhuyue.site:10002/sammo3182/data/lv_dataBIG5.csv")
df_big5[df_big5 == 0] <- NA
df_big5
```


:::{.notes}
ç»éªŒå¼€æ”¾æ€§Openness, å°½è´£æ€§Conscientiousness, å¤–å‘å‹Extraversion, äº²å’Œæ€§Agreeableness, æƒ…ç»ªä¸ç¨³å®šå‹Neuroticism

https://quantdev.ssri.psu.edu/tutorials/intro-basic-exploratory-factor-analysis
:::

:::{.fragment style="text-align:center; margin-top: 1em"}

- é™åˆ°å‡ ç»´
- å¦‚ä½•é™ç»´
:::



## ç»´åº¦é€‰æ‹©

:::{.callout-note}
## é€‰æ‹©ä¾æ®

æ¦‚å¿µå…³ç³» + æ•°æ®å…³ç³»
:::

```{r corrplot}
#| label: corrplot
#| fig-align: center
#| fig-height: 4

df_big5 %>%
  select(8:57) %>%
  cor(use = "complete.obs") %>%
  corrplot(order = "hclust",
           tl.col = 'black',
           tl.cex = .75) 
```



## Horn's Parallel Analysis

:::: {.columns}

::: {.column width="30%"}
å·²çŸ¥è§‚æµ‹æ•°æ®é›†**O**<sub>m&times;n</sub>

1. åˆ›å»ºéšæœºæ•°æ®é›†**R**<sub>m&times;n</sub>;  
1. ç›¸å…³çŸ©é˜µ<sub>**R**</sub> &rarr; &lambda;<sub>**R**</sub>;  
1. ç›¸å…³çŸ©é˜µ<sub>**Ok**</sub> &rarr; &lambda;<sub>**Ok**</sub> (kä¸ºå› å­æ•°)
1. &lambda;<sub>**Ok**</sub> vs. &lambda;<sub>**R**</sub>

:::{.fragment}
æ ‡å‡†ï¼š

A. å¦‚æœ&lambda;<sub>**Ok**</sub> < &lambda;<sub>**R**</sub>, åˆ™ ~~k~~
B. Kaiser criterion: &lambda; < 1 ä¸å¯
:::

:::

::: {.column width="70%"}
```{r parallelAnalysis}
#| label: parallelAnalysis
#| fig-align: center
#| fig-height: 6
#| results: hide

df_big5 %>%
  select(8:57) %>%
  fa.parallel(fa = "fa")
```
:::

::::



## å› å­æå–(Factor Extraction)

:::{.nonincremental}
+ Minimum residual (OLS)
+ Principal axes
+ Alpha factoring
+ Weighted least squares
+ Minimum rank
+ [Maximum likelihood (ML, minimum &chi;<sup>2</sup> goodness of fit)]{.red}

:::


## Rotation: æ•ˆæœä¼˜åŒ–

![](https://drhuyue.site:10002/sammo3182/figure/lv_rotation.jpg){fig-align="center" height=450}   

:::{.notes}
Rotation:

A pattern of loadings where each item loads strongly on only one of the factors, and much more weakly on the other factors.
:::

* Orthogonal: Varimax, quartimax, bentlerT, geominT, bifactor  
* [Oblique]{.red}: Oblimin, quartimin, simplimax, bentlerQ, geominQ, biquartimin


:::{.notes}
Orthogonal(æ­£äº¤)

ä¼˜é€‰Obliqueï¼Œå…è®¸factoré—´correlate
:::


## åˆæˆç»“æœ

:::: {.columns}

::: {.column width="45%"}

ML + Oblimin

```{r efa}
#| label: efa

result_efa <- df_big5 %>%
  select(8:57) %>%
  fa(nfactors = 5,
     rotate = "oblimin",
     fm = "ml")

print(result_efa$loadings, cutoff = .296) # the point was picked for the purpose of presentation
```
:::

::: {.column width="55%"}
```{r scores}
#| label: scores
#| fig-align: center
#| fig-height: 8


# Reshape the data to a long format
df_scores_long <- as.data.frame(result_efa$scores) |>
  pivot_longer(cols = everything(), 
               names_to = "Factor", 
               values_to = "Score")

# Create the histograms using ggplot2 and facet by factor
ggplot(df_scores_long, aes(x = Score)) +
  geom_histogram(binwidth = 0.5) +  # Adjust binwidth as needed
  facet_wrap(~ Factor, scales = "free_y", ncol = 1) +  # Facet into 1 column, adjust y scales independently
  labs(x = "Score", y = "Frequency", title = "Histograms of EFA Factors")  # Adjust labels as necessary
```
:::

::::


## è¯Šæ–­

1. Sum of squared (SS) loading^[ç‰¹å¾å€¼]: 1
1. Communality/Uniqueness
1. Root means square of residuals(RMSR): 0.05
1. Tucker-Lewis Indes (TLI): 0.9
1. Reliability test (Crobach's &alpha;)^[ä¸ºæ¯ä¸ªå› å­åˆ†åˆ«è®¡ç®—]


```{r efa-full}
#| label: efaFull

summary(result_efa)
```


:::{.notes}
- SSï¼šKaiser criterion: &lambda; < 1 ä¸å¯
- Communality:  SS of all the factor loadings for a given variable
- Uniqueness: 1 - Communality
- RMSR < 0.05
- TLI > 0.9
- Crobach's &alpha;: è®¡ç®—å› å­åˆ†å¸ƒçš„variablesæ˜¯ä¸æ˜¯å†…éƒ¨ä¸€è‡´ï¼Œé™¤éå‘è¡¨ï¼Œä¸å¤§å¿…è¦
:::


## æ³¨æ„äº‹é¡¹

- EFAæ˜¯æ•°æ®æ¢ç´¢æ–¹æ³•ï¼Œ[ä¸èƒ½ç”¨ä½œç»Ÿè®¡æ¨æ–­]{.red}
- EFAåº”ç”¨éœ€æ»¡è¶³ä»¥ä¸‹[*å‡å®š*]{.red}:
    1. [*Linearity*]{.red} between the observed and latent
    1. [*Sufficient correlation*]{.red} between the observed and latent
    1. [*Homoscedasticity*]{.red}
    1. [*Multivariate normality*]{.red} among the observed
    1. [*No singularity*]{.blue} among the observed
    1. [*Large N*]{.blue}: 10 ~ 300
    1. [*Factorial Simplicity*]{.blue}
- EFAä¸æ˜¯å”¯ä¸€åˆç†çš„é™ç»´æ–¹å¼
    - PCA
    - IRT


:::{.notes}
Red: bias; blue: efficiency

1. **Linearity**: EFA assumes a linear relationship between observed variables and underlying factors. This means that changes in the levels of factors are associated with proportional changes in the observed variables.

2. **Multivariate Normality**: Ideally, the observed variables should follow a multivariate normal distribution. While EFA can be performed on data that do not perfectly adhere to this assumption, deviations from multivariate normality can affect the robustness of the factor analysis, especially when estimating parameters and computing fit indices.

3. **Adequate Sample Size**: There is no strict rule for the minimum sample size in EFA, but a larger sample size can provide more reliable and stable factor solutions. A commonly cited guideline suggests having at least 5 to 10 observations per variable, though some scholars recommend a minimum of 300 cases as a more general benchmark.

4. **No Perfect Multicollinearity or Singularity**: While EFA aims to identify the underlying structure by exploring the correlations between variables, perfect multicollinearity (where one variable is a perfect linear combination of others) should be avoided. Similarly, singularity (where some variables are redundant) should be avoided, as these conditions can distort the analysis.

5. **Sufficient Correlation**: For a factor model to be meaningful, there must be some degree of correlation among the observed variables. If variables are completely uncorrelated, there would be no underlying factors to extract. The Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy and Bartlett's test of sphericity are commonly used to assess this assumption.

6. **Homoscedasticity**: The assumption of homoscedasticity (equal variances) among variables is also important for the generalizability of EFA results. Significant heteroscedasticity can lead to biased estimates of factor loadings and uniqueness.

7. **Factorial Simplicity**: This is more of a desired property than a strict assumption. Factorial simplicity implies that each variable should load strongly on one factor and have minimal loadings on other factors. This assumption facilitates the interpretability of factors.

Violations of these assumptions can lead to inaccurate factor solutions, misleading interpretations, and conclusions. It is therefore important to assess these assumptions before proceeding with EFA. In practice, some of these assumptions can be relaxed or addressed through data transformation, adopting robust estimation methods, or increasing the sample size.
:::

## Principal Component Analysis

:::{.callout-important}
## æ ¹æœ¬å¼

$$C = w_iY_i,$$

:::{style="text-align:center"}
**C**: Componentsï¼Œ â€œå› å­â€

**Y**: Measures
:::

:::

![](https://drhuyue.site:10002/sammo3182/figure/lv_pca.png){fig-align="center" height=300}


:::{.notes}
create one or more index variables (Components) from a larger set of measured variables (Y)
:::

## PCA vs. EFA

![](https://drhuyue.site:10002/sammo3182/figure/lv_pcaVsEfa.PNG){fig-align="center" height=500}


::: {.fragment .large style="text-align:center; margin-top: 1em"}
ç»“æœæˆ–è¿‘ä¼¼ï¼Œé€»è¾‘å¤§ä¸åŒ
:::


:::{.notes}
PCA: measurement to index   

write all variables in terms of a smaller set of features which allows for a maximum amount of variance to be retained in the data.   


EFA: indices to measurement (of a latent variable)   

find a set of features which allow for understanding as much of the correlations between measured variables as possible. individually.
:::



## å¦‚ä½•é€‰æ‹©

* PCAæœ€å¤§ç¨‹åº¦ä¿ç•™å¯è§å˜é‡ä¿¡æ¯ï¼ŒEFAæ—¨åœ¨æå–ä¸å¯å˜é‡ç‰¹å¾ï¼›
* å½“Variableä¹‹é—´å…³ç³»ä¸é‚£ä¹ˆç´§å¯†æˆ–å—åŒä¸€å˜é‡å½±å“ï¼ŒPCA > EFAï¼›
* å½“ä¼°è®¡æ½œåœ¨å˜é‡æ—¶ï¼ŒPCAå¯èƒ½å¤¸å¤§å¯è§æŒ‡æ ‡çš„å½±å“

## Bonus: éè¿ç»­å˜é‡PCA

```{r pca-category}
#| label: pcaCat
#| fig-cap: "Multiple Correspondence Analysis [MCA, @HussonEtAl2010]"
#| fig-height: 6

data(Arthritis)
arthritis_data <- Arthritis[,c(2,3,5)]

arthritis_mca <- MCA(arthritis_data,
    ncp = 3,
    graph = FALSE)

fviz_mca_biplot(arthritis_mca,
               repel = TRUE)
```


## Bonus: æ··åˆå˜é‡çš„PCA

```{r pca-mixed}
#| label: pcaMixed
#| fig-cap: "Factorial Analysis of Mixed Data [FAMD, @Pages2004]"
#| fig-height: 6

data(wine)
wine_data <- wine[,c(1,2,13,22,24,28,30)]
wine_famd <- FAMD(wine_data, graph=FALSE)

fviz_famd_ind(wine_famd,col.ind = "cos2",
             gradient.cols = gb_cols("black", "gold"),
             repel = TRUE)
```

:::{.notes}
https://www.r-bloggers.com/2022/11/pca-for-categorical-variables-in-r/

- Factorial Analysis of Mixed Data
    - cosÂ² statistic (cosine squared): how well a variable, individual, or category is represented by a given factor (dimension)
:::

## æ€»ç»“

:::: {.columns}
::: {.column width="50%"}

* æ½œåœ¨å˜é‡åˆ†ææ¦‚è¿°
    + *è¿ç»­æ€§å› å­åˆ†æ*
    + ç¦»æ•£å‹å› å­åˆ†æ
    
:::


::: {.column width="50%"}

* EFA
    + æ¢ç´¢æ€§å› å­åˆ†æï¼šé€šè¿‡loadingæ‰¾åˆ°æ½œåœ¨å˜é‡
    + EFAè¯Šæ–­ï¼šKaiser's criterion
    + EFA vs. PCA: ç»“æœç›¸ä¼¼ï¼Œé€»è¾‘ä¸åŒ

:::
::::

:::{.fragment}
::: {.r-fit-text}
**å¾…è§£ä¹‹é¢˜**ï¼šè§‚æµ‹å˜é‡ä¸æ½œå˜é‡ä¹‹é—´å…³ç³»è¦æ˜¯[éçº¿æ€§]{.red}çš„é‚£è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ
:::

:::

## å‚è€ƒæ–‡çŒ®

::: {#refs}
:::



