---
title: "æ–‡æœ¬çš„æ•°æ®åˆ†æè¿›é˜¶"
subtitle: "ä¸Šæµ·äº¤é€šå¤§å­¦æš‘æœŸç¤¾ä¼šç§‘å­¦æ–¹æ³•ç­"
author: "èƒ¡æ‚¦"
institute: "æ¸…åå¤§å­¦ç¤¾ä¼šç§‘å­¦å­¦é™¢" 
bibliography: ../camsTextAnalysis/pre_cams.bib
knitr: 
    opts_chunk:
      echo: false
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  drhutools, 
  icons,
  gridExtra,
  knitr, # dependency
  stringr, 
  tidytext, 
  tidyverse,
  lubridate,
  quanteda,
  quanteda.textstats,
  quanteda.textplots,
  quanteda.corpora,
  text2vec,
  LSX,
  seededlda,
  newsmap,
  keyATM,
  stm,
  tinytable,
  patchwork
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```



## ä¸ªäººç®€ä»‹{.Small}

:::: {.columns}

::: {.column width="60%"}
*ä¸ªäººç»å†*

- æ”¿æ²»å­¦åšå£«[ï¼ˆUniversity of Iowa)]{.small}
  - ä¿¡æ¯å­¦[ï¼ˆGraduated Certificate in Informatics)]{.small}
- æ¸…åå¤§å­¦è®¡ç®—ç¤¾ä¼šç§‘å­¦å¹³å°[(å‰¯ä¸»ä»»)]{.small}
  - æ¸…åæ•°æ®ä¸æ²»ç†ä¸­å¿ƒ[(å‰¯ä¸»ä»»)]{.small}
  - è®¡ç®—ç¤¾ä¼šç§‘å­¦ç¼–ç¨‹è¯­è¨€è¯ä¹¦é¡¹ç›®[ï¼ˆè´Ÿè´£äººï¼‰]{.small}
  - Learning R with Dr. Hu & Friends å·¥ä½œåŠ[ï¼ˆåˆ›å§‹äººï¼‰]{.small}

:::{.fragment}
*ç ”ç©¶å…´è¶£ï¼šè®¤çŸ¥ã€è¡Œä¸ºä¸ç°ä»£æ€§*

- **æ–¹æ³•è·¯å¾„ï¼šè®¡ç®—æ”¿æ²»å­¦**
  - å®éªŒå®¤å’Œè°ƒæŸ¥å®éªŒ
  - æ½œå˜é‡åˆ†æã€ç½‘ç»œåˆ†æã€ç©ºé—´åˆ†æ
  - æ–‡æœ¬å¤§æ•°æ®åˆ†æã€æ•°æ®å¯è§†åŒ–
:::

:::

::: {.column .fragment width="40%"}
*ç ”ç©¶é¢†åŸŸï¼šæ¯”è¾ƒæ”¿æ²»ã€å›½å®¶æ²»ç†*

- **W. å¿ƒç†å­¦**
  - æ”¿æ²»[è®¤çŸ¥]{.red}æ²»ç†
  - è¡Œä¸ºå…¬å…±[æ”¿ç­–]{.red}
  - æ”¿æ²»ä¼ æ’­

- **W. ç»æµå­¦**
  - ç»æµä¸å¹³ç­‰[æ„ŸçŸ¥]{.red}
  - å…¬å…±è®¾æ–½ã€æœåŠ¡å‡ç­‰åŒ–

- **W. è¯­è¨€å­¦**
  - æƒåŠ›èƒŒä¹¦çš„[è¯­è¨€æ•ˆæœ]{.red}ä¸æœºåˆ¶
  - è¯­è¨€æ”¿ç­–çš„æ²»ç†åŠŸèƒ½

:::

::::

## å¤ä¹ 

> @King2015: [The big-data approach is] the [end]{.red} of the quantitative-qualitative divide.

:::{.notes}
King talked about this issue in many places including Shanghai Jiaotong University
:::

:::: {.columns}

::: {.column .fragment width="50%"}
ä½ åº”è¯¥å·²ç»çŸ¥é“â€¦â€¦

- ä½ èƒ½ç”¨æ–‡æœ¬æ•°æ®åšä»€ä¹ˆ
    - ä½ åˆ†æçš„æ˜¯æ–‡å­—è¿˜æ˜¯è¯­è¨€ï¼Ÿ
    - Close reading or distant readingï¼Ÿ
    - æ–‡æœ¬/éŸ³é¢‘/è§†é¢‘åˆ†æçš„ç†è®ºåŸºç¡€æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•è·å–æ•°æ®
    - æ–‡æœ¬æ•°æ®çš„è·å–æ¸ é“æœ‰å“ªäº›ï¼Ÿ
    - ç½‘ç»œçˆ¬å–ä¸æ­£åˆ™è¡¨è¾¾å¼

:::

::: {.column .fragment width="50%"}

- å¦‚ä½•å¤„ç†æ•°æ®
    - å¦‚ä½•ç»“æ„åŒ–æ–‡æœ¬æ•°æ®ï¼Ÿ
    - æ–‡æœ¬é¢„å¤„ç†çš„æ­¥éª¤æœ‰å“ªäº›
    - Tokenizationçš„ä¸¤ç§å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•åˆ†ææ•°æ®
    - è¯é¢‘èƒ½åˆ†æå‡ºä»€ä¹ˆï¼Ÿ
    - å¦‚ä½•é‰´åˆ«å…³é”®è¯ï¼Ÿ
    - è¯çš„ç›¸ä¼¼åº¦ï¼Ÿ
    - ä¸»é¢˜æ¨¡å‹åœ¨å¹²ä»€ä¹ˆï¼Ÿ
        - Bag of Words (BOW)?

:::

::::

## æè¦

å­¦å®Œæœ¬è¯¾ï¼Œä½ å°†äº†è§£:

:::{ style="text-align:center"}

- å¦‚ä½•æ•æ‰â€œè‰è›‡ç°çº¿â€ï¼š*è¯æ±‡å±‚çº§*ä¿¡æ¯æ±‡å…¥
- å¦‚ä½•æå‡ä¸»é¢˜æ¨¡å‹è´¨é‡ï¼š*æ¦‚å¿µå±‚çº§*ä¿¡æ¯æ±‡å…¥
- è¯åµŒå…¥å’ŒLLMå¹²äº†ä»€ä¹ˆï¼š*è¯­ä¹‰å±‚çº§*ä¿¡æ¯æ±‡å…¥

[æœ¬è®²çš„æ ¸å¿ƒè®®é¢˜ï¼š[çªç ´è¯åŸºé™åˆ¶ï¼Œçº³å…¥æœ‰ç”¨ä¿¡æ¯]{.red}]{.fragment .large}
:::

:::{.fragment .callout-warning .incremental}
- å­¦ä¹ æœ¬è¯¾å†…å®¹ï¼Œä½ ä¸éœ€è¦ç¼–ç¨‹çŸ¥è¯†ğŸ˜±
- åº”ç”¨æœ¬è¯¾å†…å®¹ï¼Œä½ éœ€è¦ä¸€ç§ç¼–ç¨‹çŸ¥è¯†ğŸ˜œ
    - [å¦‚æœä½ æƒ³å­¦â€¦â€¦](https://www.drhuyue.site/course/method-series/04-r-workshop/)
:::

# é—®é¢˜æºå¤´

## è®©è®¡ç®—æœºè¯»æ‡‚äººè¨€çš„ä»£ä»·

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_bagOfWords.png){fig-align="center" height=400}

![Document-Term Matrix (DTM)](https://drhuyue.site:10002/sammo3182/figure/theory_bagOfWords2.jpg){.fragment fig-align="center" height=600}
:::


:::{.notes}
In linguistics, the opposite of natural language is artificial/constructed language (conlangs),  like Klingon in "Star Trek," Dothraki in "Game of Thrones"
:::

## ä¸¢å¤±äº†ä»€ä¹ˆ

:::{.fragment .large style="text-align:center; margin-top: 2em"}
- è¯çš„é‡è¦æ€§
- è¯­åº/ä½ç½®
- è™šè¯
- è¯­æ³•
- Meta data
:::


# è¯æ±‡å±‚çº§ä¿¡æ¯æ±‡å…¥

## â€œé¶å‘â€åˆ†æ

- å…³é”®æ€§(Keyness)ï¼šè¯†åˆ«åœ¨ç›®æ ‡è¯­æ–™åº“ä¸­æ¯”åœ¨å‚ç…§è¯­æ–™åº“ä¸­**ç»Ÿè®¡ä¸Šæ›´é¢‘ç¹**å‡ºç°çš„è¯è¯­çš„åº¦é‡æ–¹æ³• [@Gabrielatos2018]ã€‚

ç¤ºä¾‹1ï¼šå¯¹æ¯”ã€Šå«æŠ¥ã€‹æ–°é—»2016å¹´ä¸2012â€”2015å¹´ä¹‹é—´çš„æ–°é—»

```{r keyness, eval=FALSE}
dfmat_news <- tokens(corp_news, remove_punct = TRUE) |> 
  dfm()
 
tstat_key <- textstat_keyness(dfmat_news,
                              target = lubridate::year(dfmat_news$date) >= 2016)

saveRDS(tstat_key, file = here::here("slides", "courses", "governmentalBigData", "data", "text_keyness.rds"))
```

```{r keyness-out}
readRDS(url("https://drhuyue.site:10002/sammo3182/data/text_keyness.rds", open = "rb")) %>% textplot_keyness(n = 10)
```


## Keyness ç¤ºä¾‹2

åœ¨2012å¹´åˆ°2016å¹´çš„6,000ç¯‡ã€Šå«æŠ¥ã€‹æ–°é—»æ–‡ç« ä¸­ä¸æ¬§ç›Ÿï¼ˆ"EU", "europ*", "european union"ï¼‰ç›¸å…³çš„è¯æ±‡

```{r relevantKey,cache=TRUE}
#è¯»å…¥æ•°æ®
corp_news <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/data_corpus_guardian.rds", open = "rb"))
#æ ‡è®°åŒ–
toks_news_guardian <- tokens(corp_news, remove_punct = TRUE)

eu <- c("EU", "europ*", "european union")

toks_inside <- tokens_keep(toks_news_guardian, pattern = eu, window = 10) |> 
  tokens_remove(pattern = eu) # remove the keywords
toks_outside <- tokens_remove(toks_news_guardian, pattern = eu, window = 10)

dfmat_inside <- dfm(toks_inside)
dfmat_outside <- dfm(toks_outside)

tstat_key_inside <-
  textstat_keyness(rbind(dfmat_inside, dfmat_outside),
                   target = seq_len(ndoc(dfmat_inside)))

textplot_keyness(tstat_key_inside, n = 10)
```

## å°ç»“

:::{style="text-align:center; margin-top: 2em"}
- Weighing &larr; ä»[è¯é¢‘]{.red}æ”«å–ä¿¡æ¯
- N-gram &larr; ä»[é‚»å±…]{.red}æ”«å–ä¿¡æ¯
- Collocation &larr; ä»[å…±ç°]{.red}æ”«å–ä¿¡æ¯
- Keyness &larr; ä»[å…³é”®è¯å®šä½]{.red}æ”«å–ä¿¡æ¯
- Functional words &larr; ä»[ç¤¾ä¼šå¿ƒç†]{.red}æ”«å–ä¿¡æ¯
:::


# æ¦‚å¿µå±‚çº§ä¿¡æ¯æ±‡å…¥

## ä¸»é¢˜æ¨¡å‹èƒ½å¹²ä»€ä¹ˆ

{{< video https://drhuyue.site:10002/sammo3182/video/theory_topicModeling.webm title="What happened in topic modeling" height=600 loading="eager" allowfullscreen>}}

:::{.notes}
åŸºäºè¯é¢‘ä¸å…±çº¿çš„unsupervisedé™ç»´ï¼Œæ˜¯ä¸€ç§frequency-basedçš„é™ç»´
:::

## ä¸»é¢˜æ¨¡å‹ç¼ºä»€ä¹ˆ

> â€œä»–çš„è„¸çªç„¶è¢«é­”æ–çš„å…‰ç…§äº®äº†ã€‚è¿™æ˜¯ä¸€å¼ å› ç—›è‹¦ã€ææƒ§å’Œæ„¤æ€’è€Œå˜å¾—ç”ŸåŠ¨çš„è„¸ã€‚çº¢è‰²çš„çœ¼ç›å‘é‚£ä¸ªçœ‹ä¸è§çš„ç”·å­©ç«™ç€çš„åœ°æ–¹å°„å»ï¼Œä»–çš„éšå½¢æ–—ç¯·é®ä½äº†ä»–ã€‚ä»–çš„å£°éŸ³ï¼Œå½“ä»–å‘å‡ºå£°éŸ³æ—¶ï¼Œå°±åƒä¸€ä¸ªå†¬å¤©çš„å¤œæ™šä¸€æ ·å†·ã€‚ä»–è¯´ï¼Œâ€œæˆ‘å›æ¥äº†ï¼Œæ¯”ä»¥å‰æ›´å¼ºå¤§äº†ã€‚â€

:::{.notes}
å“ˆåˆ©æ³¢ç‰¹ä¸ç«ç„°æ¯
:::

:::{.large .fragment style="text-align:center"}
- ä¸»é¢˜ä¹‹é—´çš„è”ç³»
- ç¯‡ç« ä¹‹é—´çš„è”ç³»
- å†…å®¹èƒŒæ™¯çŸ¥è¯†ï¼ˆå¤–éƒ¨ä¿¡æ¯ï¼‰
:::

:::{.large .fragment style="text-align:center"}
&darr;    
STM/SeedLDA/keyATM
:::

## Keyword-Assisted Topic Models [keyATM, @EshimaEtAl2023]

:::{.r-stack}
- é’ˆå¯¹æ¦‚å¿µæµ‹é‡è€Œè®¾è®¡ï¼Œè€Œéæ¢ç´¢ä¸»é¢˜
- åŸºäºå…³é”®è¯ï¼ˆç§å­è¯ï¼‰ ï¼ˆç±»ä¼¼äºseedLDAï¼‰
- å…è®¸æ²¡æœ‰å…³é”®è¯çš„ä¸»é¢˜ ï¼ˆä¸åŒäºseedLDAï¼‰
- å¯¹è¯é¢‘åŠ æƒé˜²æ­¢â€œè¯é¢‘ä¸»å¯¼â€ç°è±¡ï¼ˆç±»ä¼¼äº weightedLDAï¼‰
- å…è®¸æ–‡æ¡£å‘é‡ã€å…ƒä¿¡æ¯çš„åŠ¨æ€å˜åŒ– ï¼ˆç±»ä¼¼äºSTMï¼‰
- è´å¶æ–¯æ–¹æ³•
- å½“ç§å­è¯[è´¨é‡é«˜]{.red}æ—¶ï¼Œæ€§èƒ½ä¼˜äºåŠ æƒLDAå’ŒSTM

:::{.fragment}
```{r atm-input}

#æ•°æ®
dfmat_inaug <- tokens(data_corpus_inaugural) |>
  dfm() |> 
  dfm_remove(stopwords("en"))

keyATM_docs <- keyATM_read(texts = dfmat_inaug)

# Keywords ####

keywords_inaug <- list(
  Government     = c("laws", "law", "executive"),
  Congress       = c("congress", "party"),
  Peace          = c("peace", "world", "freedom"),
  Constitution   = c("constitution", "rights"),
  ForeignAffairs = c("foreign", "war")
)

visualize_keywords(docs = keyATM_docs, keywords = keywords_inaug)

```
:::

:::


## Model Fit

```{r atm-fit, eval=FALSE}
# Fit ####

atm_inaug <- keyATM(
  docs = keyATM_docs,    # text input
  no_keyword_topics = 5,              # number of topics without keywords
  keywords = keywords_inaug,       # keywords
  model = "base",         # select the model
  options = list(seed = 313)
)

# Variable ####

vars_selected <- docvars(data_corpus_inaugural) |>
  dplyr::mutate(
    cent20 = ifelse(Year <= 1899, 0, 1),
    biparty = dplyr::case_when(
      Party == "Democratic" ~ "Democratic",
      Party == "Republican" ~ "Republican",
      TRUE ~ "Other"
    ) |> factor(levels = c("Other", "Democratic", "Republican"))
  )  |>
  dplyr::select(biparty, cent20)

# Model fit ####

atm_inaugX <- keyATM(
  docs = keyATM_docs,
  no_keyword_topics = 5,
  keywords = keywords_inaug,
  model = "covariates",
  model_settings = list(
    covariates_data    = vars_selected,
    covariates_formula = ~ biparty + cent20
  ),
  options = list(seed = 313)
)

# Model covariate

vars_period <- docvars(data_corpus_inaugural) |>
  mutate(period = (Year - 1780) %/% 10 + 1)

max_time_index <- max(vars_period$period)

# Model dynamic

atm_inaugD <- keyATM(
  docs = keyATM_docs,
  no_keyword_topics = 3,
  keywords = keywords_inaug,
  model = "dynamic",
  model_settings = list(
    time_index = vars_period$period,
    num_states = min(5, max_time_index)  # Ensure num_states doesn't exceed max_time_index
  ),
  options = list(seed = 313, 
                 store_theta = TRUE,
                 thinning = 5)
)

save(atm_inaug, atm_inaugX, atm_inaugD, file = here::here("slides", "courses", "governmentalBigData", "data", "text_atm.rda"))
```

```{r atm-fitness}
load(url("https://drhuyue.site:10002/sammo3182/data/text_atm.rda", open = "rb"))

# Model fit ####

plot_modelfit(atm_inaug)
```

## Base Model Result

```{r atm-result}
# Interpret ####

plot_topicprop(atm_inaug, show_topic = 1:10)
```

## Covariate Effect

```{r atmCov}
# Interpretation ####

## Binary
strata_period <- by_strata_DocTopic(atm_inaugX,
                                    by_var = "cent20",
                                    labels = c("18_19c", "20_21c"))

### Together view
plot(strata_period, var_name = "cent20", by = "covariate")
```

## Topic Dynamics

```{r atmDyn}
plot_timetrend(atm_inaugD, 
               time_index_label = docvars(data_corpus_inaugural)$Year, 
               xlab = "Year", 
               ci = 0.95)
```


## å°ç»“

:::{style="text-align:center; margin-top: 1em"}
:::{.semi-fade-out}
- Keyness &larr; ä»[å…³é”®è¯å®šä½]{.red}æ”«å–ä¿¡æ¯
:::

:::{.fragment}
- keyATM &larr; çº³å…¥ç ”ç©¶[æ„å›¾]{.red}
:::

:::


# è¯­ä¹‰å±‚çº§ä¿¡æ¯æ±‡å…¥

:::{.notes}
- è¯­æ³•ï¼šGrammar
- è¯­ä¹‰: Semantic
- è¯­ç”¨: Pragmatic
:::

## ç»™è¯ä¹‰å»ºæ¨¡ï¼šè¯åµŒå…¥(Word embedding)

> Words' meanings depend not just on immediate neighbors

![](https://drhuyue.site:10002/sammo3182/figure/theory_wordEmbedding.png){fig-align="center" height=550}

:::{.notes}
The term "embedding" comes from the neural network literature, in which an "embedding layer" is an input function that efficiently compresses high-dimensional data down to a low-dimensional dense representation for input to subsequent neural network layers.

- The embedding model GloVe ("Global Vectors") by @PenningtonEtAl2014 is explicitly designed to construct word vectors encoding local co-occurrence.
- An equally influential word embedding model is Word2Vec [@BengioEtAl2000], which treats each instance of a word and its context as a separate prediction problem that word vectors are chosen to solve.

LSA, NMF, and LDA can also be viewed as producing word embeddings. In particular, the (V Ã— K) matrix B from (2) contains a series of row vectors corresponding to each term in the vocabulary (see also Levy and Goldberg 2014). Those vectors contain information about word co-occurrence at the document level, rather than within a local context.
:::

## èƒ½åšä»€ä¹ˆ

- çº³å…¥è¯è¯­æ„ä¹‰çš„è¡¨è¾¾ 
    - å“ªä¸ªæœ€æ¥è¿‘`paris - france + germany`ï¼ˆæŸæ—ï¼‰ï¼Ÿ
    - å“ªä¸ªæœ€æ¥è¿‘`berlin - germany + uk + england`ï¼ˆä¼¦æ•¦ï¼‰ï¼Ÿ

:::{.fragment}
- ä¸åˆ†ç±»å’Œèšç±»ç›¸æ¯”: Document-feature matrix &rarr; context-feature matrix (FCM)
    - åˆ†ç±»ï¼šä¸å…³æ³¨å•ä¸ªè¯è¯­é—´çš„å…³ç³»ï¼Œè€Œæ˜¯å…³æ³¨è¯è¯­åœ¨æ–‡æ¡£ä¸­çš„åˆ†å¸ƒã€‚
    - èšç±»ï¼šæä¾›å¯¹æ–‡æ¡£é›†åˆå†…å®¹çš„æ›´é«˜çº§ï¼Œè€Œä¸æ˜¯ä¸“æ³¨äºå•ä¸ªè¯è¯­çš„å«ä¹‰ã€‚
    
:::{.notes}
**context-feature matrix (FCM)** æ˜¯`quantada`ä¸­å¦ä¸€ä¸ªé‡è¦çš„çŸ©é˜µå½¢å¼ï¼Œå®ƒç”¨æ¥æµ‹é‡åœ¨ç”¨æˆ·å®šä¹‰çš„ä¸Šä¸‹æ–‡è¯­å¢ƒä¸­ï¼Œä¸€äº›ç‰¹å¾çš„å…±ç°æ€§ï¼Œä¹Ÿå°±æ˜¯åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æ•æ‰è¯æ±‡ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚
å…·ä½“æ¥è¯´ï¼ŒFCM ä½¿ç”¨äº†ä¸€ä¸ªä¸Šä¸‹æ–‡çª—å£ï¼Œå¯¹æ¯ä¸ªè¯æ±‡å»ºæ¨¡æ—¶è€ƒè™‘å…¶å‘¨å›´çš„ä¸Šä¸‹æ–‡ã€‚
è¿™ä½¿å¾— FCM èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¯æ±‡ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå› ä¸ºå®ƒä¸ä»…è€ƒè™‘äº†è¯æ±‡æœ¬èº«çš„ä¿¡æ¯ï¼Œè¿˜è€ƒè™‘äº†å®ƒä»¬åœ¨è¯­å¢ƒä¸­çš„ä½¿ç”¨æƒ…å†µã€‚
:::
:::

:::{.fragment}
- åº”ç”¨
    - è¯æ¡è¡¨å¾ï¼ˆterm representationï¼‰
    - æƒ…æ„Ÿåˆ†æï¼ˆsentiment analysisï¼‰
:::


## åº”ç”¨ä¸¾ä¾‹ï¼šLatent Semantic Scaling [@Watanabe2021]

- ä¸“é—¨ç”¨äºåŒºåˆ†[å¯¹ç«‹çš„ç«‹åœº]{.red}
- åŸºäºè¯åµŒå…¥æŠ€æœ¯çš„ï¼Œåœ¨æ„å»ºæ¨¡å‹ä¹‹å‰å°†æ–‡æ¡£å’Œç‰¹å¾è½¬æ¢ä¸ºé«˜ç»´åº¦å‘é‡ç©ºé—´
    - GloVe
    - Singular Value Decomposition (SVD)

![](https://drhuyue.site:10002/sammo3182/figure/text_svd.png){.fragment fig-align="center" height=400}

:::{.notes}
SVD is a math tool that helps us break down a big matrix (which you can think of like a giant table of numbers) into smaller parts. 
SVD, a matrix is decomposed into three other matrices: U, Î£, and V . The columns of U and V are called left and right singular vectors, respectively.

Comparison in Semantic Scaling:

- SVD: Better for general-purpose dimensionality reduction and identifying broad patterns in text. It's more of a tool for finding overall structure and relationships, but it may miss finer semantic distinctions.
- GloVe: Superior for scaling semantics in a more nuanced way, capturing both local (nearby words) and global (overall context) word meanings. This makes GloVe more effective for specific tasks like word similarity and analogy.
:::

## åº”ç”¨ï¼šåˆ¤æ–­ã€Šå«æŠ¥ã€‹æ–°é—»çš„æƒ…æ„Ÿèµ°å‘

```{r lss, eval=FALSE}
# Preprocessing ####
# tokenize text corpus and remove various features
corp_sent <- corpus_reshape(corp_news, to =  "sentences")
toks_sent <- corp_sent |>
  tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_url = TRUE
  ) |>
  tokens_remove(stopwords("en", source = "marimo")) |>
  tokens_remove(c("*-time", "*-timeUpdated", "GMT", "BST", "*.com"))  



# create a document feature matrix from the tokens object
dfmat_sent <- toks_sent |> 
    dfm() |> 
    dfm_remove(pattern = "") |> 
    dfm_trim(min_termfreq = 5)

# Seed words ####

seed <- as.seedwords(data_dictionary_sentiment)


# Analyze ####

# identify context words
eco <- char_context(toks_sent, pattern = "econom*", p = 0.05)

# run LSS model (SVD)
tmod_lss_svd <- textmodel_lss(
  dfmat_sent,
  seeds = seed,
  terms = eco,
  k = 300,
  # the number of singular values requested to the SVD engine
  group_data = TRUE,
  # apply `dfm_group()` to `x` to group the sentences into the original documents, effectively reversing the segmentation by `corpus_reshape()`
  include_data = TRUE # save a grouped DFM in the LSS object as `lss$data`
)

fcm_sent <-fcm(dfmat_sent, tri = TRUE)

tmod_lss_glove <- textmodel_lss(
  fcm_sent,
  seeds = seed,
  terms = eco,
  engine = "rsparse" # using gloVe
)

save(tmod_lss_svd, tmod_lss_glove, dfmat_sent, file = here::here("slides", "courses", "governmentalBigData", "data", "text_embedding.rda"))

```

:::{.r-stack}
```{r lss-result-svd}
load(url("https://drhuyue.site:10002/sammo3182/data/text_embedding.rda", open = "rb"))

textplot_terms(tmod_lss_svd, highlighted = data_dictionary_LSD2015["negative"]) # many of the words (but not all of them) have negative meanings in the corpus.
```

:::{.fragment}
```{r lss-result-glove}
textplot_terms(tmod_lss_glove, highlighted = data_dictionary_LSD2015["negative"])
```
:::

:::

:::{.notes}
é«˜äº®çš„ä¸ºè¯å…¸ä¸­çš„è¯æ±‡, æ³¨æ„SVDç»“æœï¼Œæœ‰ä¸€äº›è¯è½åœ¨äº†0å³é¢ï¼Œè€Œgloveéƒ½è½åœ¨äº†0å·¦è¾¹
:::

## è¶‹åŠ¿åˆ†æ

```{r glovePrediction}
# Prediction ####

dat_svd <- docvars(tmod_lss_svd$data)
dat_svd$lss <- predict(tmod_lss_svd)

dfmat_doc <- dfm_group(dfmat_sent) # current glove version does not have the include_data argument
dat_glove <- docvars(dfmat_doc) 
dat_glove$lss <- predict(tmod_lss_glove, newdata = dfmat_doc)


# Draw the polarity score
smo_svd <- smooth_lss(dat_svd, lss_var = "lss", data_var = "date")

plot_svd <- ggplot(smo_svd, aes(x = date, y = fit)) + 
    geom_line() +
    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +
    labs(title = "Sentiment in Guardian (SVD)", x = "Year", y = "Sentiment")

smo_glove <- smooth_lss(dat_glove, lss_var = "lss", data_var = "date")

plot_glove <- ggplot(smo_glove, aes(x = date, y = fit)) + 
    geom_line() +
    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +
    labs(title = "Sentiment in Guardian (GloVe)", x = "Year", y = "Sentiment")
    
plot_svd / plot_glove
```

:::{.notes}
2016å¹´ä»¥åæ³¢åŠ¨æ¯”è¾ƒå¤§
:::

## æ€»ç»“

> æœ¬è®²çš„æ ¸å¿ƒè®®é¢˜ï¼š[çªç ´è¯åŸºé™åˆ¶ï¼Œæ‰¾å›æœ‰ç”¨ä¿¡æ¯]{.red}

:::: {.columns}

::: {.column width="50%"}
**è¯æ±‡å±‚çº§çš„ä¿¡æ¯æ±‡å…¥**

:::{.fragment}
- Keyness &larr; ä»[å…³é”®è¯å®šä½]{.red}æ”«å–ä¿¡æ¯
:::


**æ¦‚å¿µå±‚çº§çš„ä¿¡æ¯æ±‡å…¥**

:::{.fragment}
- keyATM &larr; çº³å…¥ç ”ç©¶[æ„å›¾]{.red}
:::

:::

::: {.column width="50%"}
**è¯­ä¹‰å±‚çº§çš„ä¿¡æ¯æ±‡å…¥**

:::{.fragment}
- Word embedding &larr; è¯æ±‡ä¹‹é—´çš„[è¯­ä¹‰]{.red}å…³è”
:::

:::

::::



# æ„Ÿè°¢å€¾å¬ï¼Œæ¬¢è¿äº¤æµ {background="#43464B"}

:::{style="text-align: right; margin-top: 1em"}  

[`r feather_icons("github")`&nbsp; sammo3182](https://github.com/sammo3182)

[`r feather_icons("mail")`&nbsp; yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

[`r feather_icons("globe")`&nbsp; https://www.drhuyue.site](https://www.drhuyue.site)

![](https://user-images.githubusercontent.com/6463211/232207708-b0e64eee-7fb3-45a4-9779-ec52397f786c.png){height=250}
:::

## å‚è€ƒæ–‡çŒ®

::: {#refs}
:::
