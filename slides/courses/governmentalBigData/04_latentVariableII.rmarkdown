---
title: "æ½œå˜é‡åˆ†æï¼ˆè¿›é˜¶ç¯‡ï¼‰"
subtitle: "æ”¿åŠ¡å¤§æ•°æ®åº”ç”¨ä¸åˆ†æ (80700673)"
author: "èƒ¡æ‚¦"
institute: "æ¸…åå¤§å­¦" 
knitr: 
    opts_chunk: 
      echo: false
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: true
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include = FALSE

library(pacman)

p_load(ltm,
       mirt,
       TeachingSampling,
       lme4,
       ggalt,
       DCPO,
       latex2exp,
       knitr,
       kableExtra,
       drhutools,
       here,
       tidyverse)

# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)
```



## æ¦‚è¦

:::{style="text-align:center
"}
**å¾…è§£ä¹‹é¢˜**

è§‚æµ‹å˜é‡ä¸æ½œå˜é‡ä¹‹é—´å…³ç³»è¦[éçº¿æ€§]{.red}
:::

:::: {.columns}

::: {.column .fragment .nonincremental width="50%"}
### æ–¹æ³•

1. é¡¹ç›®ååº”ç†è®º(IRT)
1. é¡¹ç›®ååº”èšåˆä¼°è®¡
:::

::: {.column .fragment .nonincremental width="50%"}
### æ“ä½œè¯­è¨€

* R^[ç°å­˜å¤„ç†IRTçš„R packageså·²è¶…è¿‡[50](https://www.tandfonline.com/doi/full/10.1080/15366367.2019.1586404?src=recsys)ä¸ªã€‚]
    + [`mirt`](https://github.com/philchalmers/mirt/wiki)
    + [`DCPO`](https://github.com/fsolt/DCPO)
:::

::::

# é¡¹ç›®ååº”ç†è®º

## è¶…è¶Šå› å­åˆ†æ
**å› å­åˆ†æå¼Šç«¯**ï¼š

:::{.nonicremental}
1. å‡å®šæ½œåœ¨å˜é‡æ˜¯[è¿ç»­]{.red}çš„ï¼›
1. å¯¹äºæŒ‡æ ‡[ä¸åŒºåˆ†]{.red}å˜é‡ç±»å‹ï¼›
1. éš¾ä»¥æ•æ‰[ç¾¤ç»„]{.red}å·®å¼‚
1. EFAæ— æ³•å›Šæ‹¬[æŒ‡æ ‡é—´]{.red}å…³ç³»;
1. CFAé¢ä¸´â€œ[ç®€ç•¥]{.red}ç†è®ºvsæµ‹é‡è´¨é‡â€çš„çŸ›ç›¾
:::

:::{.fragment style="text-align:center"}
&dArr;

![ç¦»æ•£å›åº”æ¨¡å‹](https://drhuyue.site:10002/sammo3182/figure/lv_irt.webp){fig-align="center" height=200}
:::

:::{.notes}
CFAç†è®ºé€šå¸¸ç®€ç•¥ï¼Œåªæ¶‰åŠä¸€éƒ¨åˆ†indicesï¼Œä½†å®é™…å¯èƒ½å¾ˆå¤æ‚ï¼›å½“å›Šæ‹¬æ›´å¤šindicesæµ‹é‡è´¨é‡ä¼šé«˜ï¼Œä½†ä¸ç¬¦åˆç†è®ºã€‚
:::


## ç±»å‹åˆ†æ

![åŸºäºå•ä¸€æ½œåœ¨å˜é‡åˆ’åˆ†â€œä¸‰å…­ä¹ç­‰" (Queen's Gambit)](https://drhuyue.site:10002/sammo3182/figure/lv_game.jpg){fig-align="center" height=200}

:::: {.columns}

::: {.column .fragment width="50%"}
![Frederic M. Lord](https://drhuyue.site:10002/sammo3182/figure/lv_lord.png){fig-align="center" height=300}

:::{.notes}
Frederic M. Lord, psychometrician, first worked for the Carnegie Foundation in 1944. By 1950, he began working for the Educational Testing Service (ETS). 
Lord's research shaped the SAT, GRE, GMAT, LSAT and the TOEFL. 
Lord was called the "Father of Modern Testing" by the National Council on Measurement in Education.
:::

:::

::: {.column .fragment width="50%"}
*Item Response Theory *(IRT)

1. å¤©ç”Ÿä¸º[äºŒå…ƒ]{.navy}æŒ‡æ ‡è®¾è®¡ï¼ˆè¡ç”Ÿé€‚åº”å®šåºå˜é‡å’Œè¿ç»­å˜é‡ï¼‰ï¼›
1. æ˜“ä¸Bayesian inferenceç»“åˆï¼Œè§£å†³[æ½œåœ¨å˜é‡scale]{.navy}ä¸ç¡®å®šé—®é¢˜ï¼›
1. æ˜“ä¸è·¨ç¾¤ç»„ä¼°è®¡ç»“åˆï¼Œå®ç°æŒ‡æ ‡[è·¨ç»„å¯æ¯”]{.navy}
:::

::::



## Basic IRT

:::: {.columns}

::: {.column width="60%"}
- è°ƒæŸ¥å±‚æ¬¡ï¼šä¸ªä½“
- Item: "ä¸€é“é¢˜"
- Response: "å¯¹ä¸€é“é¢˜ï¼ˆçš„é€‰é¡¹ï¼‰"
- æŒ‡æ ‡ç§ç±»
    1. Yes/No
    2. å¯ä»¥è½¬åŒ–ä¸ºäºŒå…ƒçš„é—®é¢˜
    3. å®šåºé—®é¢˜ï¼ˆe.g., Liker scale questionsï¼‰
:::

::: {.column .fragment .nonincremental width="40%"}
[**Assumptions**]{.red}

1. Monotonicity
1. Unidimensionality
1. Local independence
1. Parameter invariance
:::

::::

## Monotonicity

![Item characteristic curve, éšæ½œåœ¨å˜é‡å¢åŠ ï¼Œè·å¾—1çš„å¯èƒ½æ€§ä¹Ÿéšä¹‹å¢åŠ ](https://drhuyue.site:10002/sammo3182/figure/irt_icc.png){fig-align="center" height=500}


:::{.notes}
éšç€èƒ½åŠ›çš„æé«˜ï¼Œå›ç­”æ­£ç¡®ç­”æ¡ˆçš„æœºä¼šä¹Ÿå°±è¶Šé«˜
:::


## Unidimensionality

:::: {.columns}

::: {.column width="50%"}
+ èšåˆçš„é¡¹ç›®å‡æŒ‡å‘åŒä¸€ä¸ªæ½œåœ¨å˜é‡^[ç›´åˆ°å¼•å…¥multidimensional IRT]
+ åŸºäºç†è®º
:::

::: {.column .fragment width="50%"}
![](https://drhuyue.site:10002/sammo3182/figure/lv_angry.jpg){fig-align="center" height=500}
:::

::::

## Parameter Invarance

![](https://drhuyue.site:10002/sammo3182/figure/lv_gre.jpg){fig-align="center" height=350}

- é¡¹ç›®ç‰¹ç‚¹ï¼ˆparameters of itemsï¼Œæ¯”å¦‚éš¾åº¦ã€æ¢¯åº¦ç­‰ï¼‰åœ¨[é¡¹ç›®é—´]{.red}ä¸å˜
- é¡¹ç›®ç‰¹ç‚¹åœ¨[å“åº”äººç¾¤é—´]{.red}ä¸å˜^[é€šè¿‡åŸºäºWald and likelihood-ratio approachæ¥æ£€æµ‹Differential item functioning (DIF).]
    + å½“è¿›è¡ŒMultiple Group IRTæ—¶å°¤å…¶å¯èƒ½è¢«è¿å

## Local Independency

:::{.nonincremental}

$$P(Y_{ip}, Y_{iq}|\theta_i) = P(Y_{ip}|\theta_i)P(Y_{iq}|\theta_i),$$

:::{style="text-align:center"}
- p, q: Items
- i: Respondent
- Y<sub>ip</sub>: iå¯¹äºé¡¹ç›®pçš„çš„ååº”
- &theta;<sub>i</sub>: ååº”è€…içš„æ½œåœ¨å˜é‡
:::

:::{.fragment}
å¯¹äºä¸åŒé¡¹ç›®çš„ååº”ä¹‹é—´ï¼Œå…³è”æ€§[åª]{.red}æ¥è‡ªå…±åŒçš„æ½œåœ¨å˜é‡ã€‚
:::


:::

:::{.notes}
æ¢è¨€ä¹‹ï¼Œæ§åˆ¶æ½œåœ¨å˜é‡å½±å“åï¼Œé—®é¢˜é—´å“åº”ç›¸äº’ç‹¬ç«‹
:::




## IRTæ¨¡å‹å‘å±•

:::: {.columns}

::: {.column width="30%"}
Rasch Model (1PL)  

:::{.notes}
Rasch /resh/
:::
:::

::: {.column .fragment width="70%"}
&rarr; Two-Parameter Logistic Model (2PL)     
&rarr; Three-Parameter Logistic Model (3PL)     
&rarr; Four-Parameter Logistic Model (4PL)
:::

::::

:::{.fragment style="text-align:center; margin-top: 2em"}
&darr;

Multidimensional IRT        
Ordinal IRT       
Group IRT       
:::


## Rasch Model

+ y<sub>iq</sub>&isin;{0,1}: ååº”ï¼Œ`i`å¯¹é—®é¢˜`q`çš„å›ç­”
+ &theta;<sub>i</sub>&isin;{-&infin;, +&infin;}: ååº”èƒ½åŠ›ï¼Œ(Unbounded latent trait)
+ &sigma;<sub>q</sub>: Difficultyï¼Œé—®é¢˜éš¾æ˜“ç¨‹åº¦ï¼Œé€šå¸¸æ˜¾ç¤ºä¸ºz scores

:::{.fragment .nonincremental}
ğŸŒ° ä¸åŒéš¾åº¦çš„é¡¹ç›®ï¼š
    
+ å½“é¢ä¸´é‡å¤§å…¬å…±å«ç”Ÿå¨èƒæ—¶ï¼Œæ”¿åºœæ˜¯å¦åº”è¯¥åŠæ—¶å“åº”ï¼Œé‡‡å–æœæ–­æªæ–½ï¼Ÿ
+ æ”¿åºœæ˜¯å¦å¯ä»¥ç‰ºç‰²å°‘æ•°æ°‘ä¼—æƒåˆ©ï¼Œæ¥æ¢å–å¤§å¤šæ•°ç¤¾ä¼šæˆå‘˜çš„å…¬å…±å«ç”Ÿå®‰å…¨ï¼Ÿ
:::

:::{.fragment}
$$
\text{1PL: }\color{red}{P(Y_{iq} = 1)} = \color{blue}{logist^{-1}(\theta_i - \sigma_q)},
$$
:::

:::{.fragment}
$$
\color{red}{é¡¹ç›®ååº”} = \color{blue}{ååº”ç†è®º}.
$$
:::


## æ“ä½œæ¡ˆä¾‹ (Bock & Lieberman 1970)

:::: {.columns}

::: {.column width="50%"}
![](https://drhuyue.site:10002/sammo3182/figure/lv_lsat.jpg){fig-align="center" height=600}
:::

::: {.column .fragment width="50%"}
Law School Admissions Test    
sec 7, 5ä¸ªyes/noé—®é¢˜


```{r data-verbal}
df_lsat <- expand.table(LSAT7)
df_lsat
```


:::{.notes}
[`mirt` Workshop 1](http://philchalmers.github.io/mirt/extra/mirt-Workshop-2015_Day-1.pdf)
:::
:::

::::


## Difficulty Parameter

:::: {.columns}

::: {.column width="30%"}

```{r rasch-difficulty}
m_lsat <- mirt(df_lsat, model = 1, itemtype = "Rasch", verbose = FALSE)
coef(m_lsat, simplify = TRUE)$item
```

:::

::: {.column width="70%"}

```{r icc}
#| fig-cap: "Item Characteristic Curves"

plot(m_lsat, type = "trace", facet_items = FALSE)
```


:::{.notes}
æ£€æŸ¥å„é¢˜affirmativeçš„éš¾æ˜“ç¨‹åº¦ï¼Œçœ‹é€ä¸ªæ˜¯ä¸æ˜¯å¤§ä½“åŒä¸€ä¸ªè¶‹åŠ¿
:::

:::

::::



## Test Charactersitic Curve


```{r tcc}
plot(m_lsat, type = "infoSE")
```


:::{.notes}
TCCï¼š æ‰€æœ‰ICCä¹‹å’Œï¼Œä½“ç°how reliable, information ç†æƒ³æ˜¯å½¢æˆä¸€ä¸ªé’Ÿå½¢,é¡¶å°–å¤„ä»£è¡¨å¹³å‡æ°´å¹³ï¼Œæ®æ­¤å¯¹æ¯”ä¸ªäºº&theta;å¯ä»¥åˆ¤æ–­è¿™äººæ˜¯å¦æ˜¯outlier
SE(&theta;) = (test)<sup>-1/2</sup>
:::


## 1PL &rarr; 2PL {auto-animate=true}

Raschå±€é™ï¼šMeasurement error

$$
\begin{align}
\text{Rasch: } P(Y_{iq} = 1) =& logist^{-1}(\theta_i - \sigma_q),\\
\text{2PL: } P(Y_{iq} = 1) =& logist^{-1}(\color{red}{\kappa_q}\theta_i - \sigma_q),
\end{align}
$$


&kappa;<sub>q</sub>: Discrimination (Parameter of dispersion)

å¦ä¸€ç§å¸¸è§å†™æ³•

$$Pr(y_{iq} = 1) = logist^{-1}[\frac{\theta_i - {\color{red}{\beta_q}}}{\color{red}{\alpha_q}}]$$

- &alpha;<sub>q</sub>: Dispersion

:::{.notes}
Dispersion: magnitude of the measurement error 

äººä»¬å¯¹åŒä¸€ä¸ªé¢˜ç†è§£ä¸åŒï¼Œå›ç­”å‡ºaffirmativeç­”æ¡ˆå¯èƒ½æ€§ä¹Ÿä¸åŒã€‚

Discrimination: how well a question can differentiate people from below to above;

Rule of thumb above 1 meaning a good question in terms of examination
:::


## Difficulty vs. Dispersion (Statistically) {auto-animate=true}

$$Pr(y_{iq} = 1) = logist^{-1}[\frac{\theta_i - {\color{blue}{\beta_q}}}{\color{red}{\alpha_q}}]$$


```{r irt-illustration}
#| fig-align: center

tibble(
  theta = rep(seq(-3, 3, length.out = 100), 6),
  beta = rep(c(-2, 0, 2, 0, 0, 0), each = 100),
  alpha = rep(c(1, 1, 1, .25, 1, 2), each = 100),
  pr_y = plogis((theta - beta) / alpha),
  line_no = rep(1:6, each = 100),
  plot_facet = rep(
    c("Varying Question Difficulty", "Varying Question Dispersion"),
    each = 300
  )
) %>%
  ggplot(aes(theta, pr_y, group = line_no)) +
  geom_line() +
  facet_wrap(~ plot_facet) +
  xlab(TeX("Individual Unbounded Latent Trait $\\theta'_{i}$")) +
  ylab(TeX("Pr(y_{iq} = 1)")) +
  theme_bw() +
  theme(
    strip.background = element_rect(colour = "white", fill = "white"),
    plot.title.position = "plot"
  ) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  geom_text(
    data = tibble(
      theta = -2.25,
      pr_y = .65,
      line_no = 1,
      plot_facet = "Varying Question Difficulty",
    ),
    label = TeX("$\\beta_1 = -2$", output = "character"),
    parse = TRUE
  ) +
  geom_text(
    data = tibble(
      theta = -.6,
      pr_y = .525,
      line_no = 1,
      plot_facet = "Varying Question Difficulty",
    ),
    label = TeX("$\\beta_2 = 0$", output = "character"),
    parse = TRUE
  ) +
  geom_text(
    data = tibble(
      theta = .9,
      pr_y = .4,
      line_no = 1,
      plot_facet = "Varying Question Difficulty",
    ),
    label = TeX("$\\beta_3 = 2$", output = "character"),
    parse = TRUE
  ) +
  geom_text(
    data = tibble(
      theta = 2.5,
      pr_y = 0.025,
      line_no = 1,
      plot_facet = "Varying Question Difficulty",
    ),
    label = TeX("$\\alpha_q = 1$", output = "character"),
    parse = TRUE
  ) +
  geom_text(
    data = tibble(
      theta = -.03,
      pr_y = .96,
      line_no = 1,
      plot_facet = "Varying Question Dispersion",
    ),
    label = TeX("$\\alpha_1 = .25$", output = "character"),
    parse = TRUE
  ) +
  geom_text(
    data = tibble(
      theta = 1.4,
      pr_y = .9,
      line_no = 1,
      plot_facet = "Varying Question Dispersion",
    ),
    label = TeX("$\\alpha_2 = 1$", output = "character"),
    parse = TRUE
  ) +
  geom_text(
    data = tibble(
      theta = 2.1,
      pr_y = .66,
      line_no = 1,
      plot_facet = "Varying Question Dispersion",
    ),
    label = TeX("$\\alpha_3 = 2$", output = "character"),
    parse = TRUE
  ) +
  geom_text(
    data = tibble(
      theta = 2.5,
      pr_y = 0.025,
      line_no = 1,
      plot_facet = "Varying Question Dispersion",
    ),
    label = TeX("$\\beta_q = 0$", output = "character"),
    parse = TRUE
  )
```


:::{.notes}
- &beta;<sub>q</sub>: &sigma;<sub>q</sub> &frasl; &kappa;<sub>q</sub>, threshold("difficulty", æ§åˆ¶location)
- &alpha;<sub>q</sub>: &kappa;<sub>q</sub><sup>-1</sup>, dispersion (æ§åˆ¶æ–œç‡)
:::


## Difficulty vs. Dispersion (Substantively)

:::{.r-hstack}
![Difficulty](https://drhuyue.site:10002/sammo3182/figure/lv_triDimensionalChess.jpg){.fragment fig-align="center" height=500}

![Dispersion](https://drhuyue.site:10002/sammo3182/figure/lv_goes.jpg){.fragment fig-align="center" height=500}
:::

## Estimation

:::: {.columns}

::: {.column width="50%"}
**Rasch**


```{r outcome-1pl}
coef(m_lsat, simplify = TRUE)$item
```


:::

::: {.column .fragment width="50%"}
**2PL**


```{r outcome-2pl}
m_lsat2PL <-  mirt(df_lsat, model = 1, itemtype = "2PL", verbose = FALSE)
coef(m_lsat2PL, simplify = TRUE)$item
```

:::

::::

:::{.fragment}

:::{.callout-tip}

## ä½ çœŸçš„éœ€è¦2PLå—?


```{r llr}
anova(m_lsat, m_lsat2PL) %>%  
  select(AIC, SABIC, HQ, logLik, df, p) %>% 
  kable(caption = "Likelihood-Ratio Test")
```


:::

:::


## Further what if

å¦‚æœå‘ç»™äº†åˆä¸­ç”Ÿé«˜ä¸­æ•°å­¦é¢˜ï¼Ÿ&rarr; Three-Parameter Logistic Model (3PL)

:::{.notes}
å…¨å‡­çŒœ &rarr; å¤§é‡ä½&theta;äººç¾¤
:::

:::{.fragment}
$$Pr(y_{iq} = 1) = \color{red}{c_i + (1 - c_i)}logist^{-1}[\frac{(\theta_i - \beta_q)}{\alpha_q}],$$ c<sub>i</sub>ï¼šItem [lower]{.red} asymptote ("guessing")
:::


:::{.notes}
æå¤§å¢åŠ æ¼”ç®—æˆæœ¬&rarr;é€šå¸¸éœ€è¦1000ä»¥ä¸Šè§‚æµ‹ç‚¹
:::

:::{.fragment}
å¦‚æœæœ‰äººä¸careå’‹åŠ &rarr; Four-Parameter Logistic Model (4PL)

$$Pr(y_{iq} = 1) = c_i + (\color{red}{d_i} - c_i)logist^{-1}[\frac{(\theta_i - \beta_q)}{\alpha_q}], $$ d<sub>i</sub>ï¼šItem [upper]{.red} asymptote ("carelessness"), d < 1
:::


:::{.notes}
é‰´äº3PLå·²ç»éœ€è¦1000-ishè§‚æµ‹ç‚¹â€¦â€¦
:::

## IRT Diagnosis

:::{.large .nonincremental style="text-align:center; margin-top: 2em"}
+ æµ‹è¯•å±‚ï¼šGlobal fit
+ é¡¹ç›®å±‚ï¼šItem fit & residual
+ ååº”è€…å±‚ï¼šPersonal fit
:::



## Global Fit<sup>1</sup>

:::: {.columns}

::: {.column width="40%"}
$G^2 = 2[\sum_l^s r_lln(\frac{r_l}{N\tilde{P}_l})],$^[RMSEA, SRMSR, CFI, TLIå¯¹äºIRTåŒæ ·ä½¿ç”¨.]

N: å‚ä¸äººæ•°  
l: å¯èƒ½çš„ååº”  
r: åšå‡ºç‰¹å®šååº”çš„äººæ•°
:::

::: {.column width="60%"}
å½“æ•°æ®è¿‡äºç¨€ç–æ—¶(item > 10)ï¼ŒM2, M2* 


```{r diagnostics-overall}
M2(m_lsat)
```


:::{.notes}
REDO: @Maydeu-Olivares2013

N is the number of subjects, L is number of possible response patterns, $P_ l$ is the estimated probability of observing response pattern l, and $r_ l$ is the number of subjects who have response pattern l

&chi; sig   
Tucker-lewis index, 1 ideal, < .95 poor   
Comparative fit index, the same   
Root mean square error of approximation, 0 perfect, <.05 good, [.05, .08] ok, > .1 poor    
Standardized root mean square residual <.08 acceptable
:::

:::

::::


## Item Diagnostics

:::: {.columns}

::: {.column width="50%"}
*Covariation-based residuals*


```{r diagnostics-residual}
residuals(m_lsat)
```


- å¤šç”¨äºæ£€éªŒmultidimensionality

:::{.notes}
çœ‹item residualçš„åå˜ç¨‹åº¦ï¼Œunidimnesionality èšåˆçš„é¡¹ç›®å‡æŒ‡å‘åŒä¸€ä¸ªæ½œåœ¨å˜é‡ï¼Œ ä¸åº”æœ‰å…³è”
:::
:::

::: {.column width="50%"}
*Single item/person fit*


```{r item-fit}
# Item 
itemfit(m_lsat, fit_stats = "infit")
```


:::{.fragment}

```{r person-fit}
# Person
personfit(m_lsat)
```

:::


:::{.notes}
infit/outfit, close to 1 is good

Z<sub>h</sub> > 0 better
:::

:::

::::


## å¦‚æœå‡ºç°é—®é¢˜

1. é€šè¿‡S-&chi;<sup>2</sup>ã€local dependencyç­‰æ£€æŸ¥è§‚æµ‹å’Œä¼°è®¡æ•°å€¼å·®åˆ«
1. å¢åŠ æ¨¡å‹å¤æ‚ç¨‹åº¦, æ¯”å¦‚2PL &rarr; 3PL
1. å¦‚æœæœ€åˆç”¨binaryï¼Œå°è¯•polytomousæˆ–è€…nominal response models
1. å°è¯•non-parametric smoothing techniques


# å‘å±•çš„é¡¹ç›®ååº”ç†è®º

## å‘å±•æ–¹å‘

:::{.fragment .nonincremental}
- ä¸ªä½“å±‚çº§
    - ä¸€ç»´åˆ°å¤šç»´
    - äºŒåˆ†åˆ°å¤šç±»
    - å•å±‚åˆ°å¤šå±‚
:::

:::{.fragment .nonincremental}
- ç¾¤ä½“å±‚çº§
    - æ›´å‡†ç¡®çš„ã€å¯æ¯”è¾ƒçš„ç¾¤ä½“å·®å¼‚
:::


## ä¸€ç»´åˆ°å¤šç»´

Multidimentional IRT (MIRT, Phil Chalmers, 2015)

$$Pr(y_{iq} = 1) = logist^{-1}[\frac{\boldsymbol{\theta_i} - \beta_q}{\boldsymbol{\alpha_q}}]$$

**&theta;<sub>i</sub>**å’Œ**&alpha;<sub>q</sub>**ä¸å†æ˜¯å•ä¸€å€¼ï¼Œè€Œæ˜¯ä¸€ä¸ªçŸ©é˜µã€‚


## äºŒåˆ†åˆ°å¤šç±»

:::: {.columns}

::: {.column width="65%"}
Logit &rarr; Cumulative logit

$$P(Y_{iq} = 1) \rightarrow Pr(\frac{Y_{iq}\leq c}{Y_{iq}>c}).$$


```{r twoDimension}
m_lsat2D <- mirt(df_lsat, model = 2, verbose = FALSE)
plot(m_lsat2D, type = "score")
```

:::

::: {.column .fragment width="35%"}
*ä¸‰ç§ä¸»è¦ç±»å‹*

1. (Modified) Graded Response Model
    + ç”¨äºscoring rubricsï¼Œæ¯”å¦‚ Likert
1. (Generalized) Partial Credit Modelï¼ŒRating Scale Model
    + ç”¨äºå¯è½¬åŒ–ä¸ºå®šåºçš„åˆ†ç±»å˜é‡
1. Nominal Response Model
    + ç”¨äºæ— åºåˆ†ç±»å˜é‡

:::{.notes}
https://stats.stackexchange.com/questions/402440/how-should-we-select-between-various-item-response-theory-models-e-g-rsm-grm

:::
:::

::::

## å•å±‚åˆ°å¤šå±‚

![](https://drhuyue.site:10002/sammo3182/figure/countryBias.png){fig-align="center" height=300}

Multilevel Mixture IRT with Item Bias Effects (Stegmueller 2011)

åœ¨ä¼°æµ‹&alpha;<sub>q</sub>æ—¶åŠ å…¥random effect.

:::{.notes}
Daniel Stegmueller, Duke U, poli sci
:::



## è¶…è¶Šä¸ªä½“

Why bother? Individual fallacy

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/lv_maleFeminist.jpg){fig-align="center" .fragment height=500}

![](https://drhuyue.site:10002/sammo3182/figure/lv_incomeGap.jpg){.fragment fig-align="center" height=500}
:::


:::{.notes}
Ecological fallacy (Simpson's paradox)

![](https://drhuyue.site:10002/sammo3182/figure/ea_yuleSimpson.gif){fig-align="center" height=600}

å†æ¯”å¦‚ï¼Œæ°‘ä¸»ã€ä¸å¹³ç­‰ã€æ”¿æ²»æ–‡åŒ–â€¦â€¦
:::



## èšåˆå±‚çº§ä¸Šçš„å°è¯•ï¼šCaughey & Warshaw (2015)

**Dynamic Group-level IRTï¼ˆDGIRT)**


:::: {.columns}

::: {.column .fragment width="50%"}

*èšåˆï¼ˆGroup-levelï¼‰*ï¼š

$\eta_{ktq} = logit^{-1}(\frac{\color{red}{\bar{\theta}_{kt}}- \beta_q}{\sqrt{\alpha^2_q + \color{red}{(1.7\sigma_{kt})^2}}}),$ 

$\bar{\theta}_k$ å’Œ &sigma;<sub>kt</sub> æ˜¯æ½œåœ¨å˜é‡åœ¨ç»„kæ—¶é—´tçš„å‡å€¼å’ŒSDã€‚

:::{.notes}

1. åœ¨ç¾¤ç»„å±‚é¢ä¼°æµ‹IRTï¼›^[ä¸ªä½“ï¼š$p_{iq} = logist^{-1}[\frac{\theta_i - {\beta_q}}{\alpha_q}].$]
1. åœ¨ä¼°æµ‹IRTè¿‡ç¨‹ä¸­åŠ å…¥ç¾¤ç»„çº§åˆ«å˜é‡ï¼›
1. å°†æ—¶é—´å˜é‡èå…¥IRTä¼°æµ‹ï¼›
1. ç”¨[MrP](#sec-mrp)ç»™ä¼°æµ‹è¿›è¡Œæƒé‡ã€‚


1.7: sd of probit is (&pi;/3)<sup>1/2</sup> for logit, while Long 1997 found it is more close to 1.7 in actual estimations.

Mislevy, Robert J. 1983. â€œItem Response Models for Grouped Data.â€ Journal of Educational Statistics 8(4): 271â€“88.

&eta;: eta 
:::

:::

::: {.column .fragment width="50%"}

*å›Šæ‹¬æ—¶é—´ä¸ç©ºé—´ (Dynamic)*

$$
\begin{align}
\bar{\theta}_k\sim& N(\xi_t + \boldsymbol{x'_k\gamma}, \sigma^2_{\bar{\theta}}), \\
\xi_t \sim& N(\xi_{t-1}, \sigma^2_{\gamma}), \\
\gamma_{pt} \sim& N(\gamma_{p,t-1}, \delta_t + \boldsymbol{z'_p.\eta_t}, \sigma^2_{\gamma})
\end{align}
$$ 

:::{.notes}
&xi;<sub>t</sub>: xi

x ä¸ºç¾¤ç»„çº§å˜é‡  

t-1, dynamic linear model  

**z'<sub>p.</sub>&eta;<sub>t</sub>**: geography-level attributes, &eta;æ˜¯coefficients  

n<sup>*</sup><sub>kqt</sub>åŸºäºMrP
:::
:::

::::


:::{.fragment style="text-align:center"}

*æ•ˆæœ*ï¼š

+ å›Šæ‹¬è¯¸å¤šå› ç´ 
+ å¯ä»¥éƒ¨åˆ†å¹³è¡¡æ ·æœ¬ä»£è¡¨æ€§é—®é¢˜
+ å¼ºå¤§ï¼Œä½†[å¤æ‚]{.red}

:::

:::{.notes}
Caughey & Warshawç§°ä¼šè·‘å‡ ä¸ªæ˜ŸæœŸ
:::


## ç®€åŒ–DGIRT (Claassen 2019)

$$
\begin{align}
\eta_{ktq} = logit^{-1}&(\frac{\bar{\theta}'_{kt}- \beta_q}{\sqrt{\alpha^2_q + (1.7\sigma_{kt})^2}}).\\
\downarrow&\\
\eta_{ktq} = logit^{-1}&(\frac{\bar{\theta}'_{kt}- (\beta_q \color{red}{+ \delta_{kq}})}{\alpha_q}).
\end{align}
$$

:::{.notes}
&delta;<sub>kq</sub>: é—®é¢˜çš„difficultyéšå›½å®¶kå˜åŒ–ã€‚
:::

1. åªä½œç”¨äºä»£è¡¨æ€§æ ·æœ¬å’Œ[å›½å®¶]{.red}çº§åˆ«  
1. åªå¯¹äºŒåˆ†å˜é‡è¿›è¡Œåˆ†æï¼›
1. å°†å›½å®¶ä½œç”¨ä»ä¼°æµ‹&theta;å˜ä¸ºä¼°æµ‹difficulty  
1. å‡å®šæœ¬åœ°é¡¹ç›®æ­£æ€åˆ†å¸ƒï¼ˆå¿½ç•¥æåŒ–ç­‰ç°è±¡ğŸ˜±ï¼‰

:::{.fragment .large style="text-align:center"}
*è¿‡çŠ¹ä¸åŠï¼Ÿ*
:::


## èšåˆIRTæœ€æ–°å°è¯•ï¼šDCPO

:::: {.columns}

::: {.column width="50%"}
![](https://drhuyue.site:10002/sammo3182/figure/fsolt.jpeg){fig-align="center" height=400}
:::

::: {.column width="50%"}
[D]{.red}ynamic [C]{.red}omparative [P]{.red}ublic [O]{.red}pinion

:::{.fragment}
å¤æ‚ç¨‹åº¦ï¼š

Claasseen 2019 < DCPO < DGIRT
:::

:::

::::

## æ“ä½œ

1. æ”¶é›†surveyæ•°æ®ï¼Œæ˜ç¡®ä¸æ„Ÿå…´è¶£çš„å˜é‡ç›¸å…³çš„æŒ‡æ ‡é—®é¢˜ï¼ˆæ‰‹åŠ¨ï¼‰
1. é€šè¿‡`DCPOtools`å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ˆåŠè‡ªåŠ¨ï¼‰
1. é€šè¿‡`DCPO`è¿›è¡Œæ•°æ®åˆ†æï¼ˆè‡ªåŠ¨ï¼‰
1. é€šè¿‡`shinystan`è¯Šæ–­convergenceï¼ˆè‡ªåŠ¨ï¼‰

:::{.fragment}
:::{.callout-important}
## åƒä¸‡æ³¨æ„ï¼

å¦‚åŒå…¶ä»–æ–¹æ³•ä¸€æ ·ï¼ŒèšåˆIRTä¹Ÿéœ€è¦å¯¹ç»“æœä¸æ•°æ®æ‹Ÿåˆç¨‹åº¦è¿›è¡Œè¯Šæ–­ã€‚
DCPOç­‰èšåˆIRTæ–¹æ³•ä½¿ç”¨è´å¶æ–¯ç»Ÿè®¡æ¡†æ¶ï¼Œç»“æœéœ€è¦å¯¹[reccurence, stationarity, aperodicityç­‰](@sec-bayesian)è¿›è¡Œæ£€éªŒã€‚
:::
:::



## æ•ˆæœä¸é•¿å¤„

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/irtCompare.png){fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/irtFitCompare.png){.fragment fig-align="center"}
:::


:::{.notes}
Bounded: ä½¿ç”¨logitå½’ä¸º0-1
:::


## æ€»ç»“

:::: {.columns}

::: {.column width="50%"}
*ç¦»æ•£å›åº”æ¨¡å‹*

- è§£å†³é—®é¢˜ï¼šModelingéè¿ç»­æ€§æŒ‡æ ‡
- IRT
  - Raschï¼ŒnPL
  - Diagnoses: æ€»ä½“/item/respondent
  - å‘å±•
    - å¤šç»´ã€å¤šç±»ã€å¤šå±‚
- GIRT
  - è§£å†³é—®é¢˜ï¼šIndividual fallacy
  - DGIRT, Claassen model
  - DCPO
:::

::: {.column width="50%"}
*è¿ç»­å› å­æ¨¡å‹*

- è§£å†³é—®é¢˜ï¼šæ½œåœ¨å˜é‡æè¿°ä¸å…³ç³»æ£€éªŒ
- æ¢ç´¢æ€§å› å­åˆ†æ(EFA)
    - æ½œåœ¨å› å­æ¢ç´¢
- éªŒè¯æ€§å› å­åˆ†æ(CFA)
    - ç»Ÿè®¡æ¨æ–­
- ç»“æ„æ–¹ç¨‹æ¨¡å‹(SEM)
    - æ½œåœ¨å˜é‡ä¸å¤–ç”Ÿå˜é‡å…³ç³»
    
:::{.fragment}
:::{.callout-tip}
## ä½¿ç”¨å»ºè®®

- æ²¡æœ‰é«˜ä½ä¹‹åˆ†
- ç¡®æœ‰éš¾æ˜“ä¹‹åˆ«
- æŒ‡æ ‡é‡çº²æˆ–ä¸ºçº¿ç´¢
:::
:::

:::

::::




# é™„å½•ï¼šç¾¤ç»„åŠ æƒå¹³å‡è®¡ç®— {#sec-mrp .appendix}

## Individual &rarr; Aggregated

$$Y_{kq} = \frac{\sum Y_{ikq}}{n}.$$

:::{.fragment style="text-align:center"}
ä¸å¦¥ä¹‹å¤„ï¼Ÿ

1. å¦‚æœç¾¤ç»„è¿‡å°ï¼Œå…¶å¹³å‡å€¼çš„ä»£è¡¨æ„ä¹‰ä¸å¤§
2. ä¸åŒçš„æŒ‡æ ‡å¯¹äºæ½œåœ¨å˜é‡è´¡çŒ®ä¸ä¸€æ ·
:::

:::{.fragment style="text-align:center"}
[&darr;]{.large}

ç»è¿‡ç¾¤ç»„ä¿¡æ¯ï¼ˆåœ°ç†ã€äººå£ï¼‰åŠ æƒçš„å¹³å‡å€¼    

Multilevel Regression and Post-stratification (MrP)
:::

:::{.notes}
Gelman, Andrew, and Thomas C. Little. 1997. â€œPoststratification Into Many Categories Using Hierarchical Logistic Regression.â€ Survey Methods 23: 127--135.
:::

## Get the mean right

$$\theta_h = \frac{\sum_{j \in h} N_j \mu_j }{\sum_{j \in h} n_j},$$

:::{style="text-align:center"}
N: æ€»ä½“ï¼ˆæ¥è‡ªæ™®æŸ¥ï¼‰  
n: æ ·æœ¬ï¼ˆæ¥è‡ªsampleï¼‰
:::


1. å°†æ€»ä½“ï¼ˆpopulationï¼‰æŒ‰ç¾¤ç»„ï¼ˆstrataï¼Œå¦‚å›½å®¶ã€åœ°åŒºï¼‰[åˆ‡åˆ†]{.red}ï¼›
1. ä¼°æµ‹å¯¹è±¡ä¸ºæ ¸å¿ƒå˜é‡åœ¨[æ¯ä¸ªç¾¤ç»„]{.red}ä¸­çš„å¹³å‡å€¼/æ¯”ä¾‹ï¼Œ &theta;<sub>h</sub> (h &isin; {1, H});
1. å·²çŸ¥å„ç¾¤ç»„ä»¥äººå£å˜é‡jï¼ˆå¦‚è€å¹´ç”·æ€§ã€é’å¹´å¥³æ€§ç­‰ï¼‰åˆ’åˆ†ï¼Œç¡®å®š[ç¾¤ç»„äººå£]{.red}ï¼ˆN<sub>j</sub>ï¼‰æˆ–å æ€»äººå£æ¯”ï¼›
1. é€šè¿‡[multilevel model]{.red}è¿›è¡Œä¼°ç®—å„ç»„æ€»ä½“å¹³å‡å€¼&mu;<sub>j</sub>ã€‚



## ä¸€ä¸ªç»æµå­¦ğŸŒ°

æ•°æ®ï¼šæŸå¹´æŸå¸‚äº”åŒºåŸŸ2396å®¶äº§ä¸šå…¬å¸çš„è´¢æ”¿ä¿¡æ¯  
ç›®æ ‡ï¼šä¼°æµ‹æ¯ä¸ªåŒºåŸŸçš„äº§ä¸šå¹³å‡æ”¶å…¥ï¼ˆè®°ä¸º&theta;<sub>1~5</sub>ï¼‰

:::: {.columns}

::: {.column width="70%"}
å…¬å¸è§„æ¨¡å’ŒåŒºåŸŸåˆ†å¸ƒ


```{r descriptive-level}
data(Lucy)
table(Lucy$Level, Lucy$Zone) %>% 
  kable()
```

:::

::: {.column .fragment width="30%"}
æ€»ä½“å¹³å‡å€¼ï¼ˆçœŸå€¼ï¼‰


```{r trueMean}
tb_true <- group_by(Lucy, Zone) %>% 
  summarise(income = mean(Income) )
tb_true %>% 
  kable(digits = 2)
```

:::

::::


:::{.notes}
https://www.r-bloggers.com/gelmans-mrp-in-r-what-is-this-all-about/
:::

## æ ·æœ¬

æˆ‘ä»¬éšæœºé€‰å–æ•°æ®ä¸­1000ä¸ªäº§ä¸šå…¬å¸ä½œä¸ºæ ·æœ¬ï¼š


```{r rawVsTrue}
SLucy <- sample_n(Lucy, size = 1000)
Np <- table(Lucy$Level, Lucy$Zone)

tb_compare <- group_by(SLucy, Zone) %>% 
  summarise(income = mean(Income)) %>% 
  left_join(tb_true, by = c("Zone")) %>% 
  mutate(incomeTrue = 0,
         rawDiff = income.x - income.y,)

ggplot(tb_compare, aes(x = incomeTrue, xend = rawDiff, y = Zone)) +
  geom_dumbbell(
    size = 4,
    color = gb_cols("light grey"),
    colour_x = gb_cols("gold"),
    colour_xend = gb_cols("black"),
  ) +
  xlab("æ ·æœ¬å¹³å‡å€¼ä¸çœŸå€¼å·®å¼‚") +
  ylab("åŒºåŸŸ")
```



## è®¡ç®—

**Step I: Mr**

\begin{align}
æ”¶å…¥ = \beta_{0z}& + \beta_{1Z=z}å…¬å¸è§„æ¨¡_{iz} + \epsilon_{iz}, \\
\beta_{0z}& = \gamma_{00} + \gamma_{01}åŒºåŸŸ_z + u_{0z}.
\end{align}

Output: Post-strata means ($\mu_z$)


```{r mr}
# Step 1: <<MR>> - Multilevel regression
M1 <- lmer(Income ~ Level + (1 | Zone), data = SLucy)
SLucy$Pred <- predict(M1)

# Summary
sum <- group_by(SLucy, Zone, Level) %>% 
  summarise(mean2 = mean(Pred))
Mupred <- matrix(sum$mean2, ncol = 5, nrow = 3)

rownames(Mupred) <- levels(SLucy$Level)
colnames(Mupred) <- levels(SLucy$Zone)

Mupred %>% kable(digits = 2)
```


**Step II: P** $\frac{N_z \times \mu_z}{n_z}$


```{r p}
colSums(Np * Mupred) / count(Lucy, Zone)$n
```



## çŸ«æ­£æ•ˆæœ

:::: {.columns}

::: {.column width="50%"}

```{r mrpVsraw}
tb_compare$mrpDiff <- colSums(Np * Mupred) / count(Lucy, Zone)$n - tb_compare$income.y

tb_compare %>%
  pivot_longer(rawDiff:mrpDiff, names_to = "methods", values_to = "diff") %>%
  ggplot(aes(x = incomeTrue, xend = diff, y = Zone, color = methods)) +
  geom_dumbbell(
    size = 4,
    colour_x = gb_cols("gold")
  ) +
  scale_color_viridis_d(alpha = 0.5, end = 0.7) +
  xlab("æ ·æœ¬å¹³å‡å€¼ä¸çœŸå€¼å·®å¼‚") +
  ylab("åŒºåŸŸ")
```

:::

::: {.column .fragment width="50%"}
MrP æ²¡æœ‰è§£å†³çš„é—®é¢˜

+ ç­”é¢˜éš¾åº¦çš„åœ°åŒºå·®å¼‚
+ é¢˜ç›®çš„scale
+ Measurement error

:::

::::


# é™„å½•ï¼šè´å¶æ–¯ç»“æœè¯Šæ–­ {#sec-bayesian}

## è´å¶æ–¯åˆ†æå‚æ•°æ£€éªŒ

- æœ€å¸¸è§çš„Bayesian inferenceæ–¹æ³•ï¼šMarkov Chain Monte Carlo (MCMC)
- æ•°æ®æ‹Ÿåˆâ€œåº•çº¿â€ï¼šConvergence
- å½“Chainçš„posterioråœç•™åœ¨ä¸€ä¸ª[ç›¸å¯¹ç¨³å®š]{.red}çš„åŒºåŸŸå†…([ergodic]{.red} chain)
    - $\lim_{n\to \infty}p^n(\theta_i, \theta_j) = \pi(\theta_j), \forall \theta_i, \theta_j.$
- ç‰¹å¾ï¼š
    + Reccurent
        + Homogeneous/Closed: At step m if the trasition probabilities at this step do not depend on m; for State A, B, p(A, B) = 0
        + Irreducible: If every reached point/point collection can be reached from every other reached point/point collection; p(&theta;<sub>i</sub>, &theta;<sub>j</sub>)&ne; 0, &forall; &theta;<sub>i</sub>, &theta;<sub>j</sub>
    + Stationary: no autocorrelation
    + Aperodic: even with a long time there's no identical cycle of chain values repeating

:::{.notes}
+ Homogeneity: at step m the transition probability at this step do not depend on
+ Ergodicityï¼šéå†æ€§
:::


## Convergedæ—¶ä»€ä¹ˆæ ·ï¼šä¸€ä¸ªğŸŒ°

:::: {.columns}

::: {.column width="35%"}
>ä¸‹é›ªå•¦å¤©æ™´å•¦   
ä¸‹é›ªåˆ«å¿˜ç©¿æ£‰è¢„    
ä¸‹é›ªå•¦å¤©æ™´å•¦    
å¤©æ™´åˆ«å¿˜æˆ´è‰å¸½    
å¸¦è‰å¸½~~~  
---ã€Šå¿ƒä¸­çš„å¤ªé˜³ã€‹

ä»Šæ™´ï¼Œæ˜80%ä¹Ÿæ™´ï¼›  
ä»Šé›ªï¼Œæ˜60%ä¹Ÿé›ªã€‚

|      |                     | æ˜å¤©                |                     |
|------|---------------------|---------------------|---------------------|
|      |                     | &theta;<sub>1</sub> | &theta;<sub>2</sub> |
| ä»Šå¤© | &theta;<sub>1</sub> | 0.8                 | 0.2                 |
|      | &theta;<sub>2</sub> | 0.6                 | 0.4                 |

:::{.notes}
åˆ˜æ¬¢ï¼šã€Šå¿ƒä¸­çš„å¤ªé˜³ã€‹, ã€Šé›ªåŸã€‹ä¸»é¢˜æ›²ï¼Œ1988å¹´ï¼Œå€ªèä¸»æ¼”

è¯¥æ›²é‡‡ç”¨å†ç°ä¸‰æ®µä½“ç»“æ„å½¢å¼å†™æˆï¼Œå¤–åŠ ä¸€ä¸ªé«˜äº¢ã€æ‚ æ‰¬çš„å°¾å£°ã€‚Aæ®µé‡‡ç”¨æ‚ æ‰¬è‡ªç”±çš„æ•£æ¿èŠ‚å¥ï¼Œå…·æœ‰æ€å¿–æ²‰å¯‚çš„æ„å¢ƒã€‚â€œå¤§å®è¯â€å¼çš„æ­Œè¯ï¼Œé¥±å«ç€å¯¹è‡ªç„¶ã€ç¤¾ä¼šã€äººç”Ÿçš„ä¸è§£ã€ç–‘æƒ‘ä¸è¯˜é—®ã€‚Bæ®µå¼€å§‹è½¬ä¸ºå……æ»¡æ´»åŠ›çš„å¿«æ¿é€Ÿåº¦ï¼Œæ­Œè¯å¼€å§‹ç”±è¯˜é—®è½¬å˜ä¸ºå¯¹äººé—´å¸¸æ€çš„è‚¯å®šã€‚A'æ®µçš„æ—‹å¾‹ä¸å˜ï¼Œåªæ˜¯èŠ‚å¾‹å˜å¾—å¯Œäºæ€¥è¿«æ„Ÿã€åŠ¨åŠ›æ„Ÿï¼Œåœ¨è¿™ç§åŠ¨åŠ›çš„é©±åŠ¨ä¸‹ï¼Œç»§ç»­å¼€å§‹è¯˜é—®ã€‚é«˜äº¢ã€æ‚ æ‰¬çš„å°¾å£°ï¼Œä»¥åå¤å’å”±çš„â€œå•Šå¤ªé˜³â€ç»“æŸï¼Œå¯“æ„ä½œè€…å¯¹äººé—´çœŸç†çš„æ— é™å‘å¾€ä¸åšå®šè¿½æ±‚


example of converged
:::

:::

::: {.column .fragment width="65%"}
èµ·å§‹ç‚¹: [0.5 0.5]

$S_1 = [0.5\; 0.5]\begin{bmatrix}0.8 & 0.2\\ 0.6 & 0.4 \end{bmatrix}=[0.7\; 0.3];$
$S_2 = [0.7\; 0.3]\begin{bmatrix}0.8 & 0.2\\ 0.6 & 0.4 \end{bmatrix}=[0.74\; 0.26];$
$S_3 = [0.74\; 0.26]\begin{bmatrix}0.8 & 0.2\\ 0.6 & 0.4 \end{bmatrix}=[0.748\; 0.252];$
$S_4 = [0.748\; 0.252]\begin{bmatrix}0.8 & 0.2\\ 0.6 & 0.4 \end{bmatrix}=[0.749\; 0.250].$

:::{.fragment}
å†å¾€ä¸‹ç®—ï¼Œä¼šå‘ç°æ¦‚ç‡å˜åŒ–è¶Šæ¥è¶Šå°ï¼Œè¶‹äºç¨³å®šï¼ˆcongvergedï¼‰
:::


:::

::::

## æ£€éªŒConvergence

ç›®å‰æ²¡æœ‰ç»Ÿè®¡åŠæ³•èƒ½å¤Ÿè¯æ˜ä¸€ä¸ªMarkov Chainå·²ç»converged

1. åœ¨ç»™å®šæ—¶é—´å†…ï¼Œæ— æ³•ä¿è¯Markov chainèƒ½å¤Ÿ[è¾¾åˆ°]{.red}ç›®æ ‡åˆ†å¸ƒ;
1. æ— æ³•é¢„å…ˆç¡®å®šä¸€æ¡Chainèƒ½å¤Ÿ[éå†]{.red}ç›®æ ‡åˆ†å¸ƒçš„æ‰€æœ‰åŒºåŸŸ;
1. åªèƒ½è¯Šæ–­ä¸€æ¡Chainæ˜¯å¦[æœª]{.red}converged (è¯Šæ–­å·¥å…·ï¼šGeweke's G, Gelman-Rubin)

:::{.fragment}
å¢åŠ Convergenceå¯èƒ½çš„æ–¹æ³•

1. è¶³å¤Ÿçš„Burn-in rounds (è¯Šæ–­å·¥å…·ï¼šRaftery-Lewis)
1. å°½å¯èƒ½æ’é™¤autocorrelationï¼ˆè¯Šæ–­å·¥å…·ï¼šHeidelberger-Welchï¼‰
:::


## Geweke's G

æ¯”è¾ƒparametersåœ¨Chainæ—©æœŸå’Œæ™šæœŸä¸¤ä¸ª[ä¸é‡å ]{.red}çš„çª—å£å†…çš„å‡å€¼;ç”¨è¯­æ£€éªŒrecurrenceç‰¹å¾

$$G = \frac{\bar{\theta_1}- \bar{\theta_2}}{\sqrt{\frac{s_1}{n_1} + \frac{s_2}{n_2}}}.$$


:::{.notes}
A fancy difference of means

converge åˆ™æ˜¾ç¤ºä¸æ˜¾è‘—å·®å¼‚ï¼Œå¢åŠ burn-in
:::


## Gelman & Rubin 1992

$$\hat{R}= \sqrt{\frac{\hat{var(\theta)}}{W}}$$

1. è·‘å¤šæ¡chainsï¼ˆ5~10ï¼‰ï¼Œæ¯æ¡é•¿2n
1. å¯¹æ¯ä¸€ä¸ªæ„Ÿå…´è¶£çš„parameterè®¡ç®—
    1. Within chain variance(W)
    1. Between chain variance(B)
1. è®¡ç®—æ€»ä½“varianceï¼š var(&theta;) = (1 - 1/n)W + (1/n)B
1. è®¡ç®—Scale reduction (äº¦ç§°shrink factor)


:::{.fragment}
:::{.callout-tip}
- Rè¶‹è¿‘äº1è¡¨ç¤ºchains operating on same distribution
    - < 1.1æˆ–1.2æ˜¯å¯ä»¥æ¥å—çš„
::: 
:::
  

## Burn-In

ç»™ä¸è¶³å¤Ÿçš„burning inä»¥åˆ°è¾¾ç›®æ ‡åˆ†å¸ƒï¼›â€œç‚¸æ¯›çš„æ¯›æ¯›è™«â€ï¼ˆFuzzy Caterpillarï¼‰

![](https://drhuyue.site:10002/sammo3182/figure/fuzzyCaterpillar.png){fig-align="center" height=500}


## æ£€éªŒBurn-inæ˜¯å¦è¶³å¤Ÿï¼šRaftery & Lewis (1991, 1996)

+ åˆ†åˆ«è¯„ä»·æ¯ä¸€ä¸ªChainçš„æ¯ä¸€ä¸ªå˜é‡
    1. æ ¹æ®Chainé—´çš„ç›¸å…³æ€§ï¼Œå¹¶æ®æ­¤æä¾›ä¸€ä¸ªè¿­ä»£æ•°ï¼ˆiteration numberï¼‰
    1. æ£€éªŒautocorrelation inflation
    
+ è¾“å‡º
    + Burn-inï¼šä¸€ä½æ•°æˆ–ä¸¤ä½æ•°ä¸ºä½³
    + Totalï¼šå»ºè®®çš„burn-inæ•°ï¼Œæœªè€ƒè™‘cross-chainï¼Œå› æ­¤çœŸæ­£burninè¦ä¹˜ä¸Šchainæ•°
    + Dependence Factor


## Autocorrelation

ç‰¹å¾ï¼š

1. Chainé—´é«˜ç›¸å…³æ€§
1. å•ä¸€parameteré«˜ç›¸å…³æ€§


:::{.notes}
1. Chainé—´é«˜ç›¸å…³æ€§ï¼šSlow convergence
1. å•ä¸€parameterçš„é«˜ç›¸å…³ï¼šIndividual nonconvergence
:::

:::{.r-hstack}
![Autocorrelated](https://drhuyue.site:10002/sammo3182/figure/isAutocorrelation.png){.fragment fig-align="center" height=300}

![Not autocorrelated](https://drhuyue.site:10002/sammo3182/figure/noAutocorrelation.png){.fragment fig-align="center" height=300}
:::



## Heidelberger and Welch

ç”¨äºæ£€éªŒStationarity

1. ç¡®å®šä¸€ä¸ªè¿­ä»£æ•°N, ä»¥åŠå‡†ç¡®æ€§ï¼ˆ&epsilon;ï¼‰å’Œæ˜¾è‘—æ€§æ•°å‡†(&alpha;)
1. è¿è¡Œæ•´ä¸ªchain
1. æ–½ç”¨Cram&eacute;r-von Mises Testï¼ŒNull: Chainæ˜¯stationary
1. å¦‚æ£€éªŒæœªé€šè¿‡åˆ™ä¾æ¬¡ç•¥å»10%ã€20%,ä¹ƒè‡³50%çš„è¿­ä»£ï¼Œå†æ¬¡æ£€æµ‹ï¼›
1. å¦‚ç»“æœè¡¨ç¤ºéƒ¨åˆ†æ•°æ®ä¸æ˜¯stationaryï¼Œåˆ™å¯¹è¯¥éƒ¨åˆ†æ•°æ®è¿›è¡Œhalfwidthæ£€éªŒ
1. å¦‚æœhalfwidth ratio < &epsilon;, åˆ™é€šè¿‡æ£€éªŒ


## Thinning

Thinning å¹¶[ä¸ä¼š]{.red}æé«˜Chainçš„è¿ç®—é€Ÿåº¦ã€å¸®åŠ©convergenceæˆ–å¢å¼ºä¼°æµ‹è´¨é‡

+ æ¯ä¸ªChainè®°å½•å¤šå°‘samples
    + Chainå°†ä»…è®°å½•ç¬¬kä¸ªå€¼ï¼Œè¶Šé«˜ä¸¢å¤±çš„ä¿¡æ¯è¶Šå¤š
    + ké€šå¸¸å–å€¼ï¼š4ï¼Œ5ï¼Œ10
+ ä»…ç”¨äºé™ä½autocorrelation

:::{.fragment .nonincremental style="text-align:right"}
*ä½•æ—¶è°ƒThin*

1. è¿­ä»£ä¸­autocorrelationå¤ªé«˜
1. Chainçš„convergenceå¤ªä½
1. å¹¶è¡Œè¿ç®—
1. æ¨¡å‹ç»´åº¦è¿‡é«˜
:::
