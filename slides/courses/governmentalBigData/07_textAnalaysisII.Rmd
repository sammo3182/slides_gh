---
title: "è®¡ç®—æœºè¾…åŠ©æ–‡æœ¬åˆ†æÂ·æ“ä½œ"
subtitle: "Learning Text Analysis in Practice with Dr. Hu"
author: "èƒ¡æ‚¦"
institute: "æ¸…åå¤§å­¦æ”¿æ²»å­¦ç³»"
output:
  xaringan::moon_reader:
    self_contained: FALSE
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - style_ui.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: 16:9
---

```{r setup, include=FALSE}
gc()

knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)


htmltools::tagList(rmarkdown::html_dependency_font_awesome())

if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  flextable, gridExtra,
  knitr, # dependency
  tidyverse,
  broom
) # data wrangling # data wrangling

xaringanExtra::use_xaringan_extra(c("tile_view", # O
                                    "broadcast", 
                                    "panelset",
                                    "tachyons"))

# Theme setup
theme_set(theme_minimal())

# Functions preload
set.seed(313)

#Sys.setlocale(, "Chinese") # Leting the computer recognize Chinese
```


## Overview

.pull-left[
### æ–‡å­—åˆ†æ

+ è¯é¢‘åˆ†æ
+ ä¿¡æ¯æå–
+ æƒ…æ„Ÿæ ‡è®°
+ è¯­ä¹‰ç½‘ç»œ

]

.pull-right[
### æ–‡æœ¬åˆ†æ

+ æ–‡æœ¬åˆ†ç±»
+ æ–‡æœ¬èšç±»
+ è¯­ä¹‰è”ç³»*

]

--

### æŠ€æœ¯æ¥æº ï¼ˆRï¼‰

[`rvest`](https://rvest.tidyverse.org/)   
[`jiebaR`](https://qinwenfeng.com/jiebaR/)/[`TopWORDS`](http://www.stat.tsinghua.edu.cn/kdeng/r-package/)    
[`tidytext`](https://www.tidytextmining.com/)   
[`quanteda`](https://quanteda.io/index.html) series

---

class: inverse, bottom

# æ–‡å­—åˆ†æ

---

## ä½•ä¸ºæ–‡å­—åˆ†æ

+ å•ä½ï¼šè¯ã€å¥ã€æ®µã€ç« 

--

+ ç›®çš„å’Œæ–¹æ³•ï¼š
    + å†™ä½œç‰¹å¾ &larr; è¯é¢‘åˆ†æ
    + ç”¨è¯­ç‰¹å¾ &larr; ä¿¡æ¯æå–
    + æƒ…ç»ªç‰¹å¾ &larr; æƒ…æ„Ÿåˆ†æ
    + æ¦‚å¿µå…³ç³» &larr; è¯­ä¹‰ç½‘ç»œ

--

+ åˆ†ææ¡ˆä¾‹

.center[<img src="images/text_xiSpeech.jpg" height = 220 />]

---

## æ•°æ®è·å–

From html

1. æ‰“å¼€æ¥æºç½‘ç«™[æ–°åç½‘](http://www.xinhuanet.com/politics/2021-07/15/c_1127658385.htm)
1. `SelectorGadget` é€‰æ‹©å†…å®¹éƒ¨åˆ†
1. ä½¿ç”¨scraperè¿›è¡ŒæŠ“å–ï¼ˆä»¥`rvest`ä¸ºä¾‹)

```{r scrape-xiSpeech}
library(tidyverse) # data cleaning and management
library(rvest)

link_speech <- "http://www.xinhuanet.com/politics/2021-07/15/c_1127658385.htm"

tx_xi <- read_html(link_speech) %>% 
  html_nodes("p") %>%
    html_text

head(tx_xi)
```

```{r output-csv, include=FALSE, eval=FALSE}
#Saving csv for later Baidu API
tx_xi <- strsplit(tx_xi, "\\s")

names(tx_xi) <- "paragraph"

tx_xi <- as.data.frame(tx_xi) %>% 
  dplyr::filter(paragraph != "") %>% 
  dplyr::filter(!grepl('/æ‘„|æ–°åç¤¾è®°è€…', paragraph))

write.csv(tx_xi, "tx_xi.csv", fileEncoding = "GBK")
```

---

## ç®€å•æ¸…ç†

1. å»æ‰å‰äº”è¡Œæ— å…³å†…å®¹
1. å°†æ‰€æœ‰æ®µåˆå¹¶

```{r truncate-speech}
paragraph_xi <- tx_xi[6:length(tx_xi)]
tx_xi <- paragraph_xi %>% paste(collapse = "")
substr(tx_xi, 1, 50)

# Saving the data to local
# writeLines(tx_xi, file("data/tx_xi.txt", encoding = "UTF-8")) 
```

---

From text

```{r input-txt}
tx_xi <- readLines("data/tx_xi.txt", encoding = "UTF-8")
substr(tx_xi, 1, 50)
```

From pdf

```{r input-pdf}
library(pdftools)
# library(tesseract) # Loading this package if you need ocr

pdf_xi <- pdf_text("data/tx_xi.pdf")
pdf_xi[1]
```

???

https://alexluscombe.ca/blog/getting-your-.pdfs-into-r/

---

## æ–‡å­—å¤„ç†

Tokenizing + Segmenting

```{r seg-raw}
library(jiebaR)

cutter <- worker() # segment engine
segment(tx_xi, cutter)[1:20]
```

--

.pull-left[
### Tokenizing
1. å»æ‰ç©ºæ ¼ã€æ ‡ç‚¹ã€ç‰¹æ®Šç¬¦å·
1. è‡ªå®šä¹‰åœè¯ï¼ˆä¸æƒ³è®¡å…¥ç»Ÿè®¡ä¹‹è¯)
1. æ ‡è®°è¯æ€§
1. Stemming*
]

.pull-right[
### Segmenting

Languages w.o. inter-word spaces

1. é€‰æ‹©åˆ†è¯å™¨ï¼ˆRwordseg, jiebaR, etc.ï¼‰
1. è‡ªå®šä¹‰ä¸“æœ‰ä¹‹è¯
]

---

```{r seg-customized}
library(stringr)

tx_xi <- str_remove_all(tx_xi, "\\s|\\t|\\n|\\r|\\v|\\f")

# Saving the stopwords to a file so as to be accessed by jiebaR functions
stopwords::stopwords("zh", source = "stopwords-iso") %>% 
  writeLines(file("data/stopwords.txt", encoding = "UTF-8")) # make sure saving stuff with utf-8

cutter <- worker(type = "tag", # tagging
                 stop_word = "data/stopwords.txt") 

new_user_word(cutter, "åŒå¿—ä»¬", "r") # customize segmentation
new_user_word(cutter, "æœ‹å‹ä»¬", "r") 
```

---

```{r result-seg}
seg_xi <- segment(tx_xi, cutter)
seg_xi[1:50]
```

```{r id-paragraph, include=FALSE}
paragraph_xi <- str_remove_all(paragraph_xi, "\\s|\\t|\\n|\\r|\\v|\\f") %>% 
  .[nchar(.) != 0]

paragraph_xi <- paragraph_xi[nchar(paragraph_xi) != 0]

seg_xiPara <- map(paragraph_xi, ~ segment(., cutter))
df_xiPara <- map2_df(seg_xiPara, 1:length(seg_xiPara), ~tibble(id = .y, words = .x))
```

---

## è¯é¢‘åˆ†æ

Do you remember the BOW? 

```{r arrange-freq}
freq_xi <- freq(seg_xi)
arrange(freq_xi, desc(freq))
```

???

BOW: Bag of Words assumption

---

.pull-left[
```{r plot-bar, echo=FALSE}
library(ggplot2)

mutate(freq_xi, char = reorder(char, freq)) %>% 
  filter(freq >= 20) %>% 
  ggplot(aes(char, freq)) +
  geom_col() +
  xlab(NULL) +
  ylab("è¯é¢‘ç»Ÿè®¡") + 
  theme(axis.text = element_text(size = 20)) +
  coord_flip()
```
]

.pull-right[
```{r plot-cloud, echo=FALSE}
library(wordcloud2)

wordcloud2(freq_xi)
```
]
---

## å…³é”®è¯åˆ†æ

Keyword æ˜¯ä¸ªâ€œå‘â€ï¼Œè¿˜æ˜¯ä¸ªâ€œæ·±å‘â€ğŸ˜«

1. Frequency (?)
1. Co-occurence/Collocation (?)
1. Term frequencyâ€“inverse document frequency (TF-IDF, ?)

......


???

TF-IDF: Compare (multiply) term frequency in a document with the inverse document frequency (how rare or common that word is in the entire data set)

The higher the score is, the more relevant the word is to the document.

--

```{r id-keywords}
vector_keywords(seg_xi, worker(type = "keywords", topn = 5))
```

Make sense?

---

## ä¿¡æ¯æå–

æ–‡æœ¬ä¸­æåˆ°äº†å¤šå°‘æ¬¡â€œäººæ°‘â€ï¼Ÿ

```{r check-people}
freq_xi %>% 
  filter(grepl("äººæ°‘", char))
```

---

å›´ç»•â€œäººæ°‘â€éƒ½è¯´äº†ä»€ä¹ˆ (n-gram)

.pull-left[
```{r bigram-people}
ngram_xi <- NLP::ngrams(seg_xi, n = 2) 

vec_cond <- map_dbl(ngram_xi, ~ grepl("äººæ°‘", .) %>% 
      sum) %>% as.logical()

ngram_people <- ngram_xi[vec_cond] %>% 
  map(~.[!grepl("äººæ°‘", .)]) %>% 
  unlist %>% 
  freq 
```
]

.pull-right[
```{r bar-people, echo=FALSE}
mutate(ngram_people, char = reorder(char, freq)) %>% 
  filter(freq > 2) %>% 
  ggplot(aes(char, freq)) +
  geom_col() +
  xlab(NULL) +
  ylab("å’Œâ€œäººæ°‘â€å‡ºç°è¯æ±‡") + 
  theme(axis.text = element_text(size = 20)) +
  coord_flip()
```
]

---

## æƒ…æ„Ÿåˆ†æ

Sentiment analysis (or opinion mining)

.center[<img src="images/text_sentiment.jpg" height = 300 />]

???

åœ¨â€œè´Ÿ-ä¸­-æ­£â€è°±ç³»ä¸Šç•Œå®šæ–‡æœ¬çš„.red[å±æ€§]æˆ–.red[ä½ç½®]

--

.center[
å®ç°æ–¹æ³•

1. è‡ªå®šä¹‰è¯å…¸
1. è°ƒç”¨API
1. è‡ªå®šä¹‰æœºå™¨å­¦ä¹ 
]
---

## åŸºäºè‡ªå®šä¹‰è¯å…¸çš„åˆ†æ

æ¡ˆä¾‹ï¼šç®€Â·å¥¥æ–¯æ±€çš„é•¿ç¯‡å°è¯´ä¸­çš„æƒ…æ„Ÿèµ°å‘

```{r data-austen, include=FALSE}
library(janeaustenr)
library(tidytext)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>% # set the threshold
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r plot-sentimentAusten, echo=FALSE, out.width="100%", fig.height=3, fig.align='center'}
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  xlab("") +
  facet_wrap(~book, ncol = 3, scales = "free_x")
```

???

https://www.tidytextmining.com/sentiment.html

ã€Šç†æ™ºä¸æƒ…æ„Ÿã€‹ï¼ˆ1811ï¼‰
ã€Šå‚²æ…¢ä¸åè§ã€‹ï¼ˆ1813ï¼‰
ã€Šæ›¼æ–¯è²å°”å¾·åº„å›­ã€‹ï¼ˆ1814ï¼‰
ã€ŠåŸƒç›ã€‹ï¼ˆ1815ï¼‰
ã€Šè¯ºæ¡‘è§‰å¯ºã€‹ï¼ˆ1818ï¼Œå»ä¸–åå‡ºç‰ˆï¼‰
ã€ŠåŠå¯¼ã€‹ï¼ˆ1818ï¼Œå»ä¸–åå‡ºç‰ˆï¼‰

---

å¸¸è§ä¸­æ–‡æƒ…æ„Ÿè¯å…¸ï¼š

+ çŸ¥ç½‘ï¼ˆHowNetï¼‰æƒ…æ„Ÿè¯å…¸
+ å°æ¹¾å¤§å­¦ï¼ˆNTSUSDï¼‰ç®€ä½“ä¸­æ–‡æƒ…æ„Ÿææ€§è¯å…¸
+ å¤§è¿ç†å·¥å¤§å­¦æƒ…æ„Ÿè¯æ±‡æœ¬ä½“

---

.pull-left[
## è°ƒç”¨API

ä¸­æ–‡æƒ…æ„Ÿåˆ†æAPIï¼š

+ [ç™¾åº¦](https://ai.baidu.com/tech/nlp_apply/sentiment_classify)
+ [è®¯é£](https://www.xfyun.cn/doc/nlp/emotion-analysis/API.html)
+ [è…¾è®¯äº‘](https://wiki.open.qq.com/wiki/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90API)
+ [åä¸ºäº‘](https://support.huaweicloud.com/api-nlp/nlp_03_0015.html)
]

.pull-right[
## æœºå™¨å­¦ä¹ 

1. è®­ç»ƒé›† &rarr; é¢„æµ‹é›†
1. æ·±åº¦å­¦ä¹ 
]

--

.center[

ç™¾åº¦APIåˆ†æè®²è¯ï¼ˆä»¥æ®µä¸ºå•ä½ï¼‰

```{python sentiment-Baidu, echo = FALSE, eval = FALSE}
tx_xi = pd.read_csv("/Users/sunyufei/Downloads/tx_xi.csv", encoding = "GBK")

from aip import AipContentCensor
import pandas as pd
from aip import AipNlp

# set API & read data
APP_ID = '24047356'
API_KEY = 'GDGSN58wcscFOBH8gv9lolcu'
SECRET_KEY = 'GFBfGF7SYebN5bp7DEnN0LkOSnMh2aj8'
client = AipNlp(APP_ID, API_KEY, SECRET_KEY)
    
# sentiment

n = 0
tx_xi["sentScore"] = ""

while n < len(tx_xi["paragraph"]):
  try:
    result = client.sentimentClassify(text = tx_xi["paragraph"][n])
    tx_xi["sentScore"][n] = result["items"][0]["positive_prob"]
    print(n)
    print(tx_xi["sentScore"][n])
    n = n + 1
  except:
    print("ERROR")
    n = n + 1

tx_xi.to_csv("data/sentScore.csv", columns=["paragraph", "sentScore"])
```

```{r sentScore-speech, echo=FALSE}
df_sentScore <- read.csv("data/sentScore.csv", encoding = "UTF-8")
rbind(
  df_sentScore %>% arrange(desc(sentScore)) %>% .[1,],
  df_sentScore %>% arrange(sentScore) %>% .[1,]
) %>%
  select(-X) %>% 
  kable
```
]
---

## è¯­ä¹‰ç½‘ç»œ

åŸºäºè¯è¯­å…³è”æ€§ï¼ˆCorrelation, collocation, etc.ï¼‰æ­å»ºçš„ç¤¾äº¤ç½‘ç»œ

```{r build-semanticNetwork}
# count words co-occurring within sections
df_xi_pairs <- df_xiPara %>% 
  group_by(words) %>% 
  filter(n() > 10) %>% 
  widyr::pairwise_cor(words, id, sort = TRUE)
```

---

```{r plot-semanticNetwork, echo=FALSE, out.width="100%", fig.height=4, fig.align='center'}
library(ggraph)

a <- arrow(angle = 30, length = unit(0.1, "inches"), ends = "last", type = "open")

df_xi_pairs %>% 
  filter(correlation > 0.4) %>%
  igraph::graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(color = "red", width = correlation), arrow = a) + geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

---

class: inverse, bottom

# æ–‡æœ¬åˆ†æ

---

.pull-left[
## æ–‡å­—åˆ†æ

+ å•ä½ï¼šè¯ã€å¥ã€æ®µã€ç« 

+ ç›®çš„å’Œæ–¹æ³•ï¼š
    + å†™ä½œç‰¹å¾ &larr; è¯é¢‘åˆ†æ
    + ç”¨è¯­ç‰¹å¾ &larr; ä¿¡æ¯æå–
    + æƒ…ç»ªç‰¹å¾ &larr; æƒ…æ„Ÿåˆ†æ
    + æ¦‚å¿µå…³ç³» &larr; è¯­ä¹‰ç½‘ç»œ
]

.pull-right[
## æ–‡æœ¬åˆ†æ
+ å•ä½ï¼šç¯‡ã€éƒ¨ã€æœˆã€å¹´

+ ç›®çš„å’Œæ–¹æ³•ï¼š
    + é£æ ¼åˆ¤æ–­ &larr; æ–‡æœ¬åˆ†ç±»
    + ç‰¹å¾æå– &larr; æ–‡æœ¬èšç±»
    + è¯­ä¹‰è”ç³» &larr; è¯­ä¹‰å…³ç³»
]

---

## æ–‡æœ¬åˆ†æåŸºæœ¬æ€è·¯

.pull-left[
æ–¹æ³•ï¼š

1. æœ‰ç›‘ç£å¼(Supervised)
1. åŠç›‘ç£å¼(Semi-supervised)
1. æ— ç›‘ç£æ˜¯(Unsupervised)
]

.pull-right[
ææ–™ï¼š

1. è®­ç»ƒé›†(training set)
1. é¢„æµ‹é›†(testing set)

]

---

## æ–‡æœ¬åˆ†ç±»ï¼ˆClassification/Scaling)

å·²çŸ¥ç±»åˆ« &rarr; é‡åŒ–ç‰¹æ€§ &rarr; â€œè£…ç®±/æ ‡è®°â€

1. å†…å®¹æ¯”è¾ƒ
1. ä½œè€…é¢„æµ‹
1. æ–‡ä½“åˆ’åˆ†

--

å½¢å¼ï¼š

1. æœ‰ç›‘ç£å¼åˆ†ç±»
1. åŠç›‘ç£å¼åˆ†ç±»

--

æ€è·¯ï¼š

å­¦ä¹ è®­ç»ƒé›† &rarr; ç¡®å®šè¾¨è¯†è§„å¾‹ &rarr; ä½¿ç”¨æ£€éªŒé›†éªŒè¯è§„å¾‹æœ‰æ•ˆæ€§      
&rArr; 
ä½¿ç”¨è§„å¾‹åˆ†ç±»é¢„æµ‹é›†

---

class: center, middle

<img src="images/text_trainingTest.png" height = 500 />

---

## æ–‡æœ¬åˆ†ç±»

\1. è®­ç»ƒé›†å¯¼å…¥

```{r input-gutenberg}
library(tidytext)
library(gutenbergr)

titles <- c("The War of the Worlds","Pride and Prejudice")

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title") %>%
  mutate(document = row_number())

books
```

---

\2. è®­ç»ƒé›†æ¸…ç†ï¼ˆsegment, token, stemmingç­‰)

+ åˆ’åˆ†æ¯”è¾ƒå•ä½ï¼ˆè¡Œã€æ®µã€ç« ã€æœ¬ç­‰)
+ ä¿ç•™å¸¸ç”¨è¯

```{r str-2books}
tidy_books <- books %>%
  unnest_tokens(word, text) %>%
  group_by(word) %>%
  filter(n() > 10) %>% # keep words occurring over 10 times
  ungroup()

tidy_books
```

---

\3. ç‰¹å¾æå–ï¼ˆé«˜é¢‘è¯ç­‰ï¼‰

```{r classification-supervised, eval=FALSE}
tidy_books %>%
  count(title, word, sort = TRUE) %>%
  anti_join(get_stopwords()) %>%
  group_by(title) %>%
  top_n(20) %>%
  ungroup() %>%
  ggplot(aes(reorder_within(word, n, title), n,
    fill = title
  )) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~title, scales = "free") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(
    x = NULL, y = "Word count",
    title = "Most frequent words after removing stop words",
    subtitle = "Words like 'said' occupy similar ranks but other words are quite different"
  )
```

---

```{r output-classification, echo=FALSE, fig.align='center', fig.height=4, out.width="100%"}
tidy_books %>%
  count(title, word, sort = TRUE) %>%
  anti_join(get_stopwords()) %>%
  group_by(title) %>%
  top_n(20) %>%
  ungroup() %>%
  ggplot(aes(reorder_within(word, n, title), n,
    fill = title
  )) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~title, scales = "free") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(
    x = NULL, y = "Word count",
    title = "Most frequent words after removing stop words",
    subtitle = "Words like 'said' occupy similar ranks but other words are quite different"
  )
```

---

\4. ï¼ˆä¾æ®ç‰¹å¾ï¼‰åˆ¤æ–­é¢„æµ‹é›†

ï¼ˆä¸€æ®µæ–‡å­—è¿™äº›è¯ç”¨å¾—è¶Šå¤šï¼Œè¶Šå¯èƒ½æ˜¯æŸä¸ªä½œè€…çš„ï¼Œthat's it ğŸ˜‹)

???
https://juliasilge.com/blog/tidy-text-classification/

One can perform other types of comparison, e.g., with tf-idf


---

class: middle

.pull-left[
### æ–‡æœ¬åˆ†ç±»Â·æœ‰ç›‘ç£å¼

1. è®­ç»ƒé›†å¯¼å…¥
1. è®­ç»ƒé›†æ¸…ç†
1. äººå·¥ç‰¹å¾æå–
1. æ£€éªŒé›†è¯„ä¼°*
1. åˆ¤æ–­é¢„æµ‹é›†
]

.pull-right[
### æ–‡æœ¬åˆ†ç±»Â·åŠç›‘ç£å¼

1. è®­ç»ƒé›†å¯¼å…¥
1. è®­ç»ƒé›†æ¸…ç†
1. .red[æœºå™¨]ç‰¹å¾æå–
1. æ£€éªŒé›†.red[è¯„ä¼°]
1. åˆ¤æ–­é¢„æµ‹é›†
]

---

## æœºå™¨å­¦ä¹ è¾…åŠ©ç‰¹å¾æå–

è®©æˆ‘ä»¬æ¥é€ ä¸€ä¸ªè®­ç»ƒé›† vis-&agrave;-vis é¢„æµ‹é›†

```{r classification-semisupervised}
library(rsample)

books_split <- books %>%
  select(document) %>%
  initial_split() # prop = 3/4
train_data <- training(books_split)
test_data <- testing(books_split)

```

---

## æœºå™¨å­¦ä¹ ç‰¹å¾æå–æ“ä½œæ­¥éª¤

\1. å»ºæ„sparse matrix

```{r sparseMatrix}
sparse_words <- tidy_books %>%
  count(document, word) %>%
  inner_join(train_data) %>%
  cast_sparse(document, word, n)

dim(sparse_words)

# Create a response variable to associate each of the rownames() of the sparse matrix 

word_rownames <- as.integer(rownames(sparse_words))

books_joined <- data_frame(document = word_rownames) %>%
  left_join(books %>%
    select(document, title))
```

(å¯ä»¥å¹¶å…¥meta dataä½œä¸ºæ–°features)

???

12,028 training observations and 1652 features

---

\2. é€šè¿‡æœºå™¨å­¦ä¹ ç”Ÿæˆåˆ†ç±»å™¨

åŸºäºLASSO regularization 

å…¶ä»–åˆ†ç±»å™¨: Naive Bayes, Wordscores, Wordfishï¼ˆä¹Ÿå¯ä½¿ç”¨SVMã€æ·±åº¦ç¥ç»ç½‘ç»œç­‰ï¼‰

--

```{r train-lasso}
library(glmnet)
library(doParallel)
registerDoParallel(cores = 8)

is_jane <- books_joined$title == "Pride and Prejudice"
model <- cv.glmnet(sparse_words, is_jane,
  family = "binomial",
  parallel = TRUE, 
  keep = TRUE
)
```

---

```{r plot-lasso, echo = FALSE, fig.align='center', fig.height=4, out.width="100%"}
plot(model)
```

---

\3. .red[Validation! Validation! Validation!]

ç»“æœæ˜¯ä¸æ˜¯make sense?

å¯¹åˆ†ç±»å½±å“æœ€å¤§çš„coefficientsæœ‰å“ªäº›?   
ï¼ˆæ‹©å–ç¬¬ä¸€ä¸ªæ ‡å‡†å·®çš„ç³»æ•°/è¯ï¼‰

```{r evaluate-largeLambda}
library(broom)

coefs <- model$glmnet.fit %>%
  tidy() %>%
  filter(lambda == model$lambda.1se)
```

---

## æåˆ°Maritiansçš„ä¸å¤ªå¯èƒ½æ˜¯å¥¥æ–¯æ±€çš„ä½œå“ğŸ¥±

```{r plot-largeLambda, echo=FALSE, fig.align='center', fig.height=4, out.width="100%"}
coefs %>%
  group_by(estimate > 0) %>%
  top_n(10, abs(estimate)) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  coord_flip() +
  labs(
    x = NULL,
    title = "æœ€å¤§å½±å“è¯æ±‡"
  )
```

---

\4. æ£€éªŒé›†è¯„ä¼°

```{r evaluate-coef}
intercept <- coefs %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)

classifications <- tidy_books %>%
  inner_join(test_data) %>%
  inner_join(coefs, by = c("word" = "term")) %>%
  group_by(document) %>%
  summarize(score = sum(estimate)) %>%
  mutate(probability = plogis(intercept + score))

classifications
```

???

å¹¶å…¥æ£€éªŒé›†ï¼Œå³ä¾§æ˜¯å¯èƒ½æ€§æ¦‚ç‡

---

## ä½•ä¸ºæœ‰æ•ˆ

æ³•1ï¼šReceiver Operating Characteristic (ROC) Curve


.pull-left[
$$TPR = \frac{TP}{TP + FN}$$
åˆ¤æ–­æ˜¯å¥¥æ–¯æ±€å†™çš„/å®é™…æ˜¯å¥¥æ–¯æ±€å†™çš„    


$$FPR = \frac{FP}{FP + TN}$$

åˆ¤æ–­éå¥¥æ–¯æ±€å†™çš„/å®é™…éå¥¥æ–¯æ±€å†™çš„
]

--

.pull-right[<img src="images/text_ROC.png" height = 500 />]

---

```{r evaluate-roc, echo=FALSE, fig.align='center', fig.height=4, out.width="100%"}
library(yardstick)

comment_classes <- classifications %>%
  left_join(books %>%
    select(title, document), by = "document") %>%
  mutate(title = as.factor(title))

comment_classes %>%
  roc_curve(title, probability) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(
    color = "midnightblue",
    size = 1.5
  ) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  ) +
  labs(
    title = "ROC Curve"
  )
```


ä½¿ç”¨AUCä¹Ÿå¯ä»¥

???

Area under the ROC Curve: AUC measures the entire two-dimensional area underneath the entire ROC curve from (0,0) to (1,1).

---

æ³•2ï¼šConfusion Matrix

ROCçš„ç²—æš´2 &times; 2 ç‰ˆï¼ˆä»¥50%å¯èƒ½æ€§ä¸ºç•Œï¼‰

```{r evaluate-CM, echo=FALSE, fig.align='center', fig.height=3, out.width="100%"}
comment_classes %>%
  mutate(
    prediction = case_when(
      probability > 0.5 ~ "Pride and Prejudice",
      TRUE ~ "The War of the Worlds"
    ),
    prediction = as.factor(prediction)
  ) %>%
  conf_mat(title, prediction) %>% 
  autoplot
```

---

## å“ªäº›é”™äº†ï¼ŸType I & Type II Errors

.center[<img src="../analysisOfPoliticalData/images/errorType.png" height = 550 />]

---

ä¸æ˜¯ä½†é”™è®¤ä¸ºå¥¥æ–¯æ±€å†™çš„ (Type I, prob > 0.5)

```{r evaluate-typeI}
comment_classes %>%
  mutate(
    prediction = case_when(
      probability > 0.5 ~ "Pride and Prejudice",
      TRUE ~ "The War of the Worlds"
    ),
    prediction = as.factor(prediction)
  ) %>%
  conf_mat(title, prediction) %>% 
  autoplot
```

---

æ˜¯ä½†æœªè¢«è®¤ä¸ºå¥¥æ–¯æ±€å†™çš„ (Type II, prob < 0.3)

```{r evaluate-typeII}
comment_classes %>%
  filter(
    probability < .3,
    title == "Pride and Prejudice"
  ) %>%
  sample_n(10) %>%
  inner_join(books %>%
    select(document, text)) %>%
  select(probability, text)
```

---

æƒ³çŸ¥é“æ›´å¤šï¼Ÿ

Emil Hvitfeldt & Julia Silge [Supervised Machine Learning for Text Analysis in R](https://smltar.com/)

.center[<img src="images/text_machineLearningBook.tif" height = 500 />]

---

## Classification vs. Scaling

.pull-left[
### Classification: è£…ç®±

+ ä½œè€…
+ é£æ ¼ã€æµæ´¾
+ æ—¶æœŸ

<img src="images/text_visClassification.gif" height = 350 />
]

--

.pull-right[
### Scalingï¼šæ ‡ç­¾ï¼Œlatent traits

+ Ideology
+ Policy standpoint
+ Soft facts vs. hard facts   
......

<img src="images/text_visScaling.jpeg" height = 350 />

]

???

https://lse-me314.github.io/lecturenotes/ME314_day10.pdf

---

.pull-left[
### æ–‡æœ¬åˆ†ç±»ï¼ˆClassification)

.red[å·²çŸ¥]åˆ†ç±» &rarr; é‡åŒ–ä¸ªæ€§ &rarr; â€œè£…ç®±â€

1. å†…å®¹æ¯”è¾ƒ
1. ä½œè€…é¢„æµ‹
1. æ–‡ä½“åˆ’åˆ†

å½¢å¼ï¼š

1. æœ‰ç›‘ç£å¼åˆ†ç±»
1. åŠç›‘ç£å¼åˆ†ç±»
]

.pull-right[
### æ–‡æœ¬èšç±» (Clustering)

.red[æœªçŸ¥]å·®å¼‚ &rarr; é‡åŒ–å…±æ€§ &rarr; â€œåˆ†åŒ…â€

1. ç‰©ä»¥ç±»èš
1. æ¦‚å¿µæå–

å½¢å¼ï¼š

åŠç›‘ç£å¼èšç±»    
æ— ç›‘ç£å¼èšç±»

1. å•æ ‡ç­¾èšç±»
1. æ··åˆæ ‡ç­¾èšç±»
]

---

## å•æ ‡ç­¾èšç±»ï¼ˆSingle-Membership Clustering)

é‡åŒ–å…±æ€§ &rarr; ç›¸ä¼¼å½’ç»„ &rarr; æœ€å¤§åŒ–.blue[ç»„å†…]ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–.blue[ç»„é—´]ç›¸ä¼¼åº¦

--

### å½’ç»„é€»è¾‘

Hierarchical/Agglomerative

èµ·ç‚¹ï¼šå„ç‚¹å„è‡ªä¸€ç»„    
è¿‡ç¨‹ï¼šä¸æ–­åˆå¹¶è·ç¦»æœ€è¿‘ç»„

Point Assignment

èµ·ç‚¹ï¼šéšæœºå°†ç‚¹åˆ†ç»„    
è¿‡ç¨‹ï¼šä¸æ–­æ”¹å˜åˆ†ç»„æ–¹å¼ä½¿ç»„å†…è·ç¦»æœ€å°

---

## è·ç¦»

ä¸¤ä¸ªæ•°æ®ç‚¹çš„ç›¸å¼‚ç¨‹åº¦

--

### ç©ºé—´è·ç¦»æµ‹ç®—æ³•

.pull-left[


Euclidean Distance

<img src="images/text_euclidean.png" height = 350 />


]

--

.pull-right[

Manhattan Distance
<img src="images/text_manhattan.png" height = 350 />

]

???

https://www.datanovia.com/en/lessons/clustering-distance-measures/
https://shairozsohail.medium.com/a-comprehensive-introduction-to-clustering-methods-1e1e4f95b501

---

### ç›¸ä¼¼æ€§è·ç¦»æµ‹ç®—æ³•

.center[
Pearson metrics

Spearman metrics

Kendall metrics

Cosine metrics
]

--

.center[æ¬¢è¿æ¥ 70700173	æ²»ç†æŠ€æœ¯ä¸“é¢˜ï¼šæ”¿æ²»æ•°æ®åˆ†æ ğŸ‰]

---

## Hierarchical Clustering

èšç±»ç¾å›½æ€»ç»Ÿå›½æƒ…å’¨æ–‡

```{r euclidean}
library(quanteda)
library(quanteda.corpora)
library(quanteda.textstats)
pres_dfm <- tokens(corpus_subset(data_corpus_sotu, Date > "1980-01-01"), remove_punct = TRUE) %>%
  tokens_wordstem("en") %>%
  tokens_remove(stopwords("en")) %>%
  dfm() %>%
  dfm_trim(min_termfreq = 5, min_docfreq = 3)

# hierarchical clustering - get distances on normalized dfm
pres_dist_mat <- dfm_weight(pres_dfm, scheme = "prop") %>%
    textstat_dist(method = "euclidean") %>% 
    as.dist()

# hiarchical clustering the distance object
pres_cluster <- hclust(pres_dist_mat)
```


???

https://books.psychstat.org/textmining/cluster-analysis.html

---

```{r plot-hiearchical, echo=FALSE, fig.align='center', fig.height=4, out.width="100%"}
# label with document names
pres_cluster$labels <- docnames(pres_dfm)

# plot as a dendrogram
plot(pres_cluster, xlab = "", sub = "", 
     main = "Euclidean Distance on Normalized Token Frequency")
```

---

## Point Assignment

K-mean

.center[<img src="images/text_kmean.gif" height = 500 />]


---

```{r kmean}
k <- 4

pres_kmean <- stats::kmeans(pres_dist_mat, 
                            centers = k,
                            nstart = 10) # number of iterations
```

```{r plot-kmean, echo=FALSE, fig.align='center', fig.height=3, out.width="100%"}
factoextra::fviz_cluster(pres_kmean, data = pres_dist_mat,
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_minimal())
```


---

Get the K right

æ£€éªŒ1--20ä¸ªmeanä¹‹åŒºåˆ«

```{r knum, echo=FALSE, fig.align='center', fig.height=4, out.width="100%"}
k <- 20
varper <- NULL
for (i in 1:k) {
    kfit <- kmeans(pres_dist_mat, i)
    varper <- c(varper, kfit$betweenss/kfit$totss)
}

plot(1:k, varper, xlab = "# of clusters", ylab = "explained variance")
```

---

```{r plot-kmean6, echo=FALSE, fig.height=7, out.width="50%", results='hold'}
k <- 6
pres_kmean6 <- stats::kmeans(pres_dist_mat, 
                            centers = k,
                            nstart = 10) # number of iterations

factoextra::fviz_cluster(pres_kmean6, data = pres_dist_mat,
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_minimal())

factoextra::fviz_cluster(pres_kmean, data = pres_dist_mat,
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_minimal())
```

---

## æ··åˆæ ‡ç­¾èšç±»ï¼ˆMixed-Membership Clustering)

.pull-left[

ä¸»é¢˜æ¨¡å‹(Topic modeling)

ä¸»é¢˜: æ¦‚å¿µ

Decisions:

1. ä¸»é¢˜æ•°é‡
1. èµ·å§‹ç‚¹
1. ä¸»é¢˜.red[å«ä¹‰]
]

.pull-right[
<video width="500" height="500" controls>
<source src="images/text_topicModeling.webm" type="video/webm">
</video>
]

---

## ä¸»é¢˜æ¨¡å‹ä¸¾ä¾‹

What does "democracy" mean in Chinese political language?

--

.center[<img src="images/text_theoryMap.png" height = 500 />]

---

Structural Equation Model

1. Identifying concept and conceptual relations
1. Meta data

--

Data: 

People's Daily (1949--2003)

--

Decisions:

1. How many topics?
1. Model robustness?
1. Meanings of the topics?

---

## How Many Topics?

Data driven: Testing 20--140 topics

Criteria:

1. Leveraging held-out likelihood and semantic coherence
1. Residual dropped more

--

&rArr; 40 topics


---

## Model Robustness

### Influences beyond the text per se

Controls:

1. Time dependency
1. Period
1. Holidays and two Sessions ("Liang Hui")

--

### Avoiding initialization sensitivity


Drop low likelihood values after 20 E-M iterations;     
Only the highest 20% were converged and the one with highest semantic coherence and exclusivity were used.

---

## Model Interpretation

Five vital concepts: Democracy, liberal values, regime feature, national priority (economy, security, stability)

Keywords &harr; FREX score words

---

## Conceptual Network

Topic Correlations across years

.pull-left[
### 1991

<img src="images/text_democracy1991.png" height = 400 />
]
.pull-right[
### 1997

<img src="images/text_democracy1997.png" height = 400 />
]

---

## What Can Explain Democracy Within and Between Discourses

.center[<img src="images/text_democracyCoef.png" height = 500 />]

---

## Bonus: è¯­ä¹‰å…³ç³»

vector("paris")âˆ’vector("france")+vector("germany")

.center[<img src="images/text_wordEmbedding.png" height = 500 />]

???

https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4

é«˜çº¬åº¦è¯æ±‡é™ç»´è¡¨è¾¾ç›¸è¿‘çš„å‘é‡(å…³ç³»)

é™ç»´æ–¹å¼ï¼š

PCA/LSA

Word2Vec

GloVe: Leverage LSA and skip-gram model of Mikolov (Word2Vec)

https://www.cnblogs.com/mantch/p/11403771.html

LSAï¼ˆLatent Semantic Analysisï¼‰æ˜¯ä¸€ç§æ¯”è¾ƒæ—©çš„count-basedçš„è¯å‘é‡è¡¨å¾å·¥å…·ï¼Œå®ƒä¹Ÿæ˜¯åŸºäºco-occurance matrixçš„ï¼Œåªä¸è¿‡é‡‡ç”¨äº†åŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰çš„çŸ©é˜µåˆ†è§£æŠ€æœ¯å¯¹å¤§çŸ©é˜µè¿›è¡Œé™ç»´ï¼Œè€Œæˆ‘ä»¬çŸ¥é“SVDçš„å¤æ‚åº¦æ˜¯å¾ˆé«˜çš„ï¼Œæ‰€ä»¥å®ƒçš„è®¡ç®—ä»£ä»·æ¯”è¾ƒå¤§ã€‚è¿˜æœ‰ä¸€ç‚¹æ˜¯å®ƒå¯¹æ‰€æœ‰å•è¯çš„ç»Ÿè®¡æƒé‡éƒ½æ˜¯ä¸€è‡´çš„ã€‚è€Œè¿™äº›ç¼ºç‚¹åœ¨GloVeä¸­è¢«ä¸€ä¸€å…‹æœäº†ã€‚

è€Œword2vecæœ€å¤§çš„ç¼ºç‚¹åˆ™æ˜¯æ²¡æœ‰å……åˆ†åˆ©ç”¨æ‰€æœ‰çš„è¯­æ–™ï¼Œæ‰€ä»¥GloVeå…¶å®æ˜¯æŠŠä¸¤è€…çš„ä¼˜ç‚¹ç»“åˆäº†èµ·æ¥ã€‚ä»è¿™ç¯‡è®ºæ–‡ç»™å‡ºçš„å®éªŒç»“æœæ¥çœ‹ï¼ŒGloVeçš„æ€§èƒ½æ˜¯è¿œè¶…LSAå’Œword2vecçš„ï¼Œä½†ç½‘ä¸Šä¹Ÿæœ‰äººè¯´GloVeå’Œword2vecå®é™…è¡¨ç°å…¶å®å·®ä¸å¤šã€‚

https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4

---

## Take-Home Points

.pull-left[
### æ–‡å­—åˆ†æ

+ å•ä½ï¼šè¯ã€å¥ã€æ®µã€ç« 
+ ç›®çš„å’Œæ–¹æ³•ï¼š
    + å†™ä½œç‰¹å¾ &larr; è¯é¢‘åˆ†æ
        + **é«˜é¢‘è¯ã€å…³é”®è¯**
    + ç”¨è¯­ç‰¹å¾ &larr; ä¿¡æ¯æå–
        + **ç‰¹å¾è¯ã€N-gram**
    + æƒ…ç»ªç‰¹å¾ &larr; æƒ…æ„Ÿåˆ†æ
        + **è¯å…¸ã€è°ƒç”¨APIã€æœºå™¨å­¦ä¹ **
    + æ¦‚å¿µå…³ç³» &larr; è¯­ä¹‰ç½‘ç»œ
        + **å…±ç°ã€ç›¸å…³æ€§**

]

.pull-right[
### æ–‡æœ¬åˆ†æ

+ å•ä½ï¼šç¯‡ã€éƒ¨ã€æœˆã€å¹´

+ ç›®çš„å’Œæ–¹æ³•ï¼š
    + é£æ ¼åˆ¤æ–­ &larr; æ–‡æœ¬åˆ†ç±»
        + **æœ‰ç›‘ç£ã€æœºå™¨å­¦ä¹ **
    + ç‰¹å¾æå– &larr; æ–‡æœ¬èšç±»
        + **å•æ ‡ç­¾ï¼ˆhierarchical, point assignmentï¼‰**
        + **æ··åˆæ ‡ç­¾ï¼ˆä¸»é¢˜æ¨¡å‹ï¼‰**
    + è¯­ä¹‰è”ç³» &larr; è¯­ä¹‰å…³ç³»
        + **Word embedding (word2vec, GloVe)**
]

---

class: inverse, center, middle

# Thank you!

<i class="fa fa-envelope fa-lg"></i>&nbsp; [yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

<i class="fa fa-globe fa-lg"></i>&nbsp; https://sammo3182.github.io/

<i class="fab fa-github fa-lg"></i>&nbsp; [sammo3182](https://github.com/sammo3182)