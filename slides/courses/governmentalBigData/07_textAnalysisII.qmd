---
title: "æ–‡æœ¬çš„æ•°æ®åˆ†æè¿›é˜¶"
subtitle: "æ”¿åŠ¡å¤§æ•°æ®åº”ç”¨ä¸åˆ†æ (80700673)"
author: "èƒ¡æ‚¦"
institute: "æ¸…åå¤§å­¦ç¤¾ä¼šç§‘å­¦å­¦é™¢" 
bibliography: ../camsTextAnalysis/pre_cams.bib
knitr: 
    opts_chunk:
      echo: false
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  drhutools, 
  icons,
  gridExtra,
  knitr, # dependency
  stringr, 
  tidytext, 
  tidyverse,
  lubridate,
  quanteda,
  quanteda.textstats,
  quanteda.textplots,
  quanteda.corpora,
  text2vec,
  LSX,
  seededlda,
  newsmap,
  keyATM,
  stm,
  tinytable
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

# æ€»ç»Ÿå°±èŒæ•°æ®è¯­æ–™åº“ï¼ˆCorpus of presidential inaugural dataï¼‰

dfmat_inaug <- tokens(data_corpus_inaugural) |>
  dfm() |> 
  dfm_remove(stopwords("en"))

```



## ä¸ªäººç®€ä»‹{.Small}

:::: {.columns}

::: {.column width="60%"}
*ä¸ªäººç»å†*

- æ”¿æ²»å­¦åšå£«[ï¼ˆUniversity of Iowa)]{.small}
  - ä¿¡æ¯å­¦[ï¼ˆGraduated Certificate in Informatics)]{.small}
- æ¸…åå¤§å­¦è®¡ç®—ç¤¾ä¼šç§‘å­¦å¹³å°[(å‰¯ä¸»ä»»)]{.small}
  - æ¸…åæ•°æ®ä¸æ²»ç†ä¸­å¿ƒ[(å‰¯ä¸»ä»»)]{.small}
  - è®¡ç®—ç¤¾ä¼šç§‘å­¦ç¼–ç¨‹è¯­è¨€è¯ä¹¦é¡¹ç›®[ï¼ˆè´Ÿè´£äººï¼‰]{.small}
  - Learning R with Dr. Hu & Friends å·¥ä½œåŠ[ï¼ˆåˆ›å§‹äººï¼‰]{.small}

:::{.fragment}
*ç ”ç©¶å…´è¶£ï¼šè®¤çŸ¥ã€è¡Œä¸ºä¸ç°ä»£æ€§*

- **æ–¹æ³•è·¯å¾„ï¼šè®¡ç®—æ”¿æ²»å­¦**
  - å®éªŒå®¤å’Œè°ƒæŸ¥å®éªŒ
  - æ½œå˜é‡åˆ†æã€ç½‘ç»œåˆ†æã€ç©ºé—´åˆ†æ
  - æ–‡æœ¬å¤§æ•°æ®åˆ†æã€æ•°æ®å¯è§†åŒ–
:::

:::

::: {.column .fragment width="40%"}
*ç ”ç©¶é¢†åŸŸï¼šæ¯”è¾ƒæ”¿æ²»ã€å›½å®¶æ²»ç†*

- **W. å¿ƒç†å­¦**
  - æ”¿æ²»[è®¤çŸ¥]{.red}æ²»ç†
  - è¡Œä¸ºå…¬å…±[æ”¿ç­–]{.red}
  - æ”¿æ²»ä¼ æ’­

- **W. ç»æµå­¦**
  - ç»æµä¸å¹³ç­‰[æ„ŸçŸ¥]{.red}
  - å…¬å…±è®¾æ–½ã€æœåŠ¡å‡ç­‰åŒ–

- **W. è¯­è¨€å­¦**
  - æƒåŠ›èƒŒä¹¦çš„[è¯­è¨€æ•ˆæœ]{.red}ä¸æœºåˆ¶
  - è¯­è¨€æ”¿ç­–çš„æ²»ç†åŠŸèƒ½

:::

::::

## å¤ä¹ 

> @King2015: [The big-data approach is] the [end]{.red} of the quantitative-qualitative divide.

:::{.notes}
King talked about this issue in many places including Shanghai Jiaotong University
:::

:::: {.columns}

::: {.column .fragment width="50%"}
ä½ åº”è¯¥å·²ç»çŸ¥é“â€¦â€¦

- ä½ èƒ½ç”¨æ–‡æœ¬æ•°æ®åšä»€ä¹ˆ
    - ä½ åˆ†æçš„æ˜¯æ–‡å­—è¿˜æ˜¯è¯­è¨€ï¼Ÿ
    - Close reading or distant readingï¼Ÿ
    - æ–‡æœ¬/éŸ³é¢‘/è§†é¢‘åˆ†æçš„ç†è®ºåŸºç¡€æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•è·å–æ•°æ®
    - æ–‡æœ¬æ•°æ®çš„è·å–æ¸ é“æœ‰å“ªäº›ï¼Ÿ
    - ç½‘ç»œçˆ¬å–ä¸æ­£åˆ™è¡¨è¾¾å¼

:::

::: {.column .fragment width="50%"}

- å¦‚ä½•å¤„ç†æ•°æ®
    - å¦‚ä½•ç»“æ„åŒ–æ–‡æœ¬æ•°æ®ï¼Ÿ
    - æ–‡æœ¬é¢„å¤„ç†çš„æ­¥éª¤æœ‰å“ªäº›
    - Tokenizationçš„ä¸¤ç§å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•åˆ†ææ•°æ®
    - é€šè¯é¢‘å“èƒ½åˆ†æå‡ºä»€ä¹ˆï¼Ÿ
    - å¦‚ä½•é‰´åˆ«å…³é”®è¯ï¼Ÿ
    - ä»€ä¹ˆæ˜¯ç›¸ä¼¼åº¦åˆ†æï¼Ÿ
    - ä¸»é¢˜æ¨¡å‹åœ¨å¹²ä»€ä¹ˆï¼Ÿ
        - Bag of Words (BOW)?

:::

::::

## æè¦

å­¦å®Œæœ¬è¯¾ï¼Œä½ å°†äº†è§£:

:::{ style="text-align:center; margin-top: 2em"}

- å¦‚ä½•åœ¨*è¯æ±‡å±‚çº§*çº³å…¥æ›´å¤šä¿¡æ¯
- å¦‚ä½•å¢åŠ *ä¸»é¢˜æ¨¡å‹*çš„è´¨é‡
- å¦‚ä½•å°†*è¯­ä¹‰ä¿¡æ¯*çº³å…¥è€ƒé‡

[æœ¬è®²çš„æ ¸å¿ƒè®®é¢˜ï¼š[çªç ´è¯åŸºé™åˆ¶ï¼Œæ‰¾å›æœ‰ç”¨ä¿¡æ¯]{.red}]{.fragment .large}
:::

:::{.fragment .callout-warning .incremental}
- å­¦ä¹ æœ¬è¯¾å†…å®¹ä½ ä¸éœ€è¦ç¼–ç¨‹çŸ¥è¯†ğŸ˜±
- åº”ç”¨æœ¬è¯¾å†…å®¹ä½ éœ€è¦ä¸€ç§ç¼–ç¨‹çŸ¥è¯†ğŸ˜œ
    - [å¦‚æœä½ æƒ³å­¦â€¦â€¦](https://www.drhuyue.site/course/method-series/04-r-workshop/)
:::

# é—®é¢˜æºå¤´

## è®©è®¡ç®—æœºè¯»æ‡‚äººè¨€çš„ä»£ä»·

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_bagOfWords.png){fig-align="center" height=400}

![Document-Term Matrix (DTM)](https://drhuyue.site:10002/sammo3182/figure/theory_bagOfWords2.jpg){.fragment fig-align="center" height=600}
:::


:::{.notes}
In linguistics, the opposite of natural language is artificial/constructed language (conlangs),  like Klingon in "Star Trek," Dothraki in "Game of Thrones"
:::

## ä¸¢å¤±äº†ä»€ä¹ˆ

:::{.fragment .large style="text-align:center; margin-top: 2em"}
- è¯çš„é‡è¦æ€§
- è¯­åº/ä½ç½®
- è™šè¯
- è¯­æ³•
- Meta data
:::



# è¯æ±‡å±‚çº§ä¿¡æ¯æ±‡å…¥

## åŠ æƒ

DTMçš„é—®é¢˜ï¼š

1. æœªå°†è¯çš„é‡è¦æ€§çº³å…¥è€ƒé‡
2. è¿‡åº¦ä½“ç°å¸¸è§è¯
3. è½»è§†å°‘è§è¯

:::{.fragment}
ä¸¾ä¾‹ï¼šTerm frequency-inverse document frequency (TF-IDF)

:::: {.columns}

::: {.column width="50%"}
$$\displaystyle \mathrm {tf} (t,d)={\frac {f_{t,d}}{\sum _{t'\in d}{f_{t',d}}}},$$ where $f_{t,d}$ is the number of times that term t occurs in document d. 
:::

::: {.column width="50%"}
$$\displaystyle \mathrm {idf} (t,D)=\log {\frac {N}{|\{d:d\in D{\text{ and }}t\in d\}|}},$$ 

- $N$: total number of documents; D.
- $|\{d\in D:t\in d\}|$ : number of documents where the term $t$ appears.

:::

::::

:::

:::{.notes}
TFï¼š Importance of a term in a document
IDFï¼š Frequency a term appear across documents
:::


## åŠ æƒå¸¦æ¥ä»€ä¹ˆ

- å¥½å¤„ï¼š
  1. è¯†åˆ«é‡è¦è¯æ±‡
  2. å‡å°‘å¸¸è§è¯æ±‡çš„æƒé‡
  3. æé«˜æœºå™¨å­¦ä¹ æ€§èƒ½ (why?)

:::{.fragment}
- å‰¯ä½œç”¨ï¼š
  1. å‡å®šè¯æ±‡ç‹¬ç«‹ï¼ˆä¸DTMç›¸åŒï¼‰
  2. å¯¹åœ¨è¯­æ–™åº“ä¸­éå¸¸ç½•è§ä½†åœ¨ç‰¹å®šæ–‡æ¡£ä¸­å‡ºç°è¿‡å‡ æ¬¡çš„ç½•è§è¯æ±‡ç»™äºˆé«˜æƒé‡
  3. å¯¹è¯­æ–™åº“çš„å¤§å°å’Œå¤šæ ·æ€§æ•æ„Ÿ
:::


## N-gramåˆ†æ

- Markov Model of Order N
    + Unigram: æ¸…å å¤§å­¦ ç¤¾ä¼š ç§‘å­¦ å­¦é™¢
    + Bigram: æ¸…åå¤§å­¦ å¤§å­¦ç¤¾ä¼š ç¤¾ä¼šç§‘å­¦ ç§‘å­¦ å­¦é™¢
    + Trigramï¼šæ¸…åå¤§å­¦ç¤¾ä¼š å¤§å­¦ç¤¾ä¼šç§‘å­¦ ç¤¾ä¼šç§‘å­¦å­¦é™¢

![](https://drhuyue.site:10002/sammo3182/figure/text_ngram.png){.fragment fig-align="center" height=400}

## æ­é…åˆ†æ

- è¿ç»­æ­é…ï¼ˆContiguous collocationsï¼‰ï¼šåœ¨æ–‡æœ¬ä¸­ç›´æ¥ç›¸é‚»å‡ºç°ã€‚
- æ­ç¤ºè¯­è¨€ä½¿ç”¨ä¸­çš„æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼ä»æŸ¥çœ‹å•ä¸ªå•è¯æ—¶å¹¶ä¸ç«‹å³æ˜æ˜¾ã€‚

ç¤ºä¾‹æ•°æ®ï¼š2012å¹´åˆ°2016å¹´çš„6,000ç¯‡ã€Šå«æŠ¥ã€‹æ–°é—»æ–‡ç« 

```{r guardianData, cache=TRUE}
# 2016å¹´å«æŠ¥æ–°é—»è¯­æ–™åº“ï¼ˆCorpus of Guardian news in 2016 from those from 2012--2015ï¼‰

corp_news <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/data_corpus_guardian.rds", open = "rb"))

toks_news_guardian <- tokens(corp_news, remove_punct = TRUE)

corp_news
```

## Collocation ç¤ºä¾‹

- æœ€å¸¸è§çš„è¯å¯¹ï¼ˆpairs of wordsï¼‰
- æœ€å¸¸è§çš„ä¸‰è¯æ¨¡å¼ï¼ˆthree-word patternsï¼‰

```{r collocation, message=FALSE}
toks_select <- tokens_select(
  toks_news_guardian,
  pattern = "^[A-Z]",
  valuetype = "regex",
  case_insensitive = FALSE,
  padding = TRUE
) 

tstat_col_caps <- textstat_collocations(toks_select, min_count = 100)
head(tstat_col_caps, 10)

tstat_col_caps3 <- textstat_collocations(toks_select, min_count = 80, size = 3)
head(tstat_col_caps3, 10)
```

## â€œé¶å‘â€åˆ†æ

- å…³é”®æ€§(Keyness)ï¼šè¯†åˆ«åœ¨ç›®æ ‡è¯­æ–™åº“ä¸­æ¯”åœ¨å‚ç…§è¯­æ–™åº“ä¸­**ç»Ÿè®¡ä¸Šæ›´é¢‘ç¹**å‡ºç°çš„è¯è¯­çš„åº¦é‡æ–¹æ³•ã€‚

ç¤ºä¾‹1ï¼šå¯¹æ¯”ã€Šå«æŠ¥ã€‹æ–°é—»2016å¹´ä¸2012â€”2015å¹´ä¹‹é—´çš„æ–°é—»

```{r keyness, eval=FALSE}
dfmat_news <- tokens(corp_news, remove_punct = TRUE) |> 
  dfm()
 
tstat_key <- textstat_keyness(dfmat_news,
                              target = lubridate::year(dfmat_news$date) >= 2016)

saveRDS(tstat_key, file = here::here("slides", "courses", "governmentalBigData", "data", "text_keyness.rds"))
```

```{r keyness-out}
readRDS(url("https://drhuyue.site:10002/sammo3182/data/text_keyness.rds", open = "rb")) %>% textplot_keyness(n = 10)
```


## Keyness ç¤ºä¾‹2

åœ¨2012å¹´åˆ°2016å¹´çš„6,000ç¯‡ã€Šå«æŠ¥ã€‹æ–°é—»æ–‡ç« ä¸­ä¸æ¬§ç›Ÿï¼ˆ"EU", "europ*", "european union"ï¼‰ç›¸å…³çš„è¯æ±‡

```{r relevantKey}
eu <- c("EU", "europ*", "european union")

toks_inside <- tokens_keep(toks_news_guardian, pattern = eu, window = 10) |> 
  tokens_remove(pattern = eu) # remove the keywords
toks_outside <- tokens_remove(toks_news_guardian, pattern = eu, window = 10)

dfmat_inside <- dfm(toks_inside)
dfmat_outside <- dfm(toks_outside)

tstat_key_inside <-
  textstat_keyness(rbind(dfmat_inside, dfmat_outside),
                   target = seq_len(ndoc(dfmat_inside)))

textplot_keyness(tstat_key_inside, n = 10)
```


## â€œå…¥æœ¨ä¸‰åˆ†â€åˆ†æ

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_pronoun.png){fig-align="center" height=400}

![](https://drhuyue.site:10002/sammo3182/figure/css_liwcTree.png){.fragment fig-align="center" height=600}
:::


## å°ç»“

:::{style="text-align:center; margin-top: 2em"}
- Weighing &larr; ä»[è¯é¢‘]{.red}æ”«å–ä¿¡æ¯
- N-gram &larr; ä»[é‚»å±…]{.red}æ”«å–ä¿¡æ¯
- Collocation &larr; ä»[å…±ç°]{.red}æ”«å–ä¿¡æ¯
- Keyness &larr; ä»[å…³é”®è¯å®šä½]{.red}æ”«å–ä¿¡æ¯
- Functional words &larr; ä»[ç¤¾ä¼šå¿ƒç†]{.red}æ”«å–ä¿¡æ¯
:::


# ä¸»é¢˜æ¨¡å‹æ‰©å±•

## ä¸»é¢˜æ¨¡å‹èƒ½å¹²ä»€ä¹ˆ

{{< video https://drhuyue.site:10002/sammo3182/video/theory_topicModeling.webm title="What happened in topic modeling" height=600 loading="eager" allowfullscreen>}}

:::{.notes}
åŸºäºè¯é¢‘ä¸å…±çº¿çš„unsupervisedé™ç»´ï¼Œæ˜¯ä¸€ç§frequency-basedçš„é™ç»´
:::

## ä¸»é¢˜æ¨¡å‹ç¼ºä»€ä¹ˆ

> â€œä»–çš„è„¸çªç„¶è¢«é­”æ–çš„å…‰ç…§äº®äº†ã€‚è¿™æ˜¯ä¸€å¼ å› ç—›è‹¦ã€ææƒ§å’Œæ„¤æ€’è€Œå˜å¾—ç”ŸåŠ¨çš„è„¸ã€‚çº¢è‰²çš„çœ¼ç›å‘é‚£ä¸ªçœ‹ä¸è§çš„ç”·å­©ç«™ç€çš„åœ°æ–¹å°„å»ï¼Œä»–çš„éšå½¢æ–—ç¯·é®ä½äº†ä»–ã€‚ä»–çš„å£°éŸ³ï¼Œå½“ä»–å‘å‡ºå£°éŸ³æ—¶ï¼Œå°±åƒä¸€ä¸ªå†¬å¤©çš„å¤œæ™šä¸€æ ·å†·ã€‚ä»–è¯´ï¼Œâ€œæˆ‘å›æ¥äº†ï¼Œæ¯”ä»¥å‰æ›´å¼ºå¤§äº†ã€‚â€

:::{.notes}
å“ˆåˆ©æ³¢ç‰¹ä¸ç«ç„°æ¯
:::

:::{.large .fragment style="text-align:center"}
- ä¸»é¢˜ä¹‹é—´çš„è”ç³»
- ç¯‡ç« ä¹‹é—´çš„è”ç³»
- å†…å®¹èƒŒæ™¯çŸ¥è¯†ï¼ˆå¤–éƒ¨ä¿¡æ¯ï¼‰
:::

:::{.large .fragment style="text-align:center"}
&darr;    
STM/SeedLDA/keyATM
:::

## Correlated Topic Model (CTM)

ä¸»é¢˜ä¹‹é—´å½¼æ­¤å…³è”è¢«çº³å…¥è€ƒé‡

:::{.r-vstack}
![LDA](https://drhuyue.site:10002/sammo3182/figure/cluster_ldaDiagram.png){fig-align="center" height=150}

![CTM](https://drhuyue.site:10002/sammo3182/figure/cluster_ctmDiagram.png){fig-align="center" height=150}
:::

 $$\{\mu,\Sigma\}\sim N(\mu,\Sigma).$$

:::{.notes}
CTMæ¨¡å‹ï¼ˆcorrelated topic modelï¼‰çš„è¿›æ­¥ä¹‹å¤„åœ¨äºå“ªé‡Œå‘¢ï¼Ÿ

æœ€åˆäººä»¬åšLDAæ¨¡å‹çš„æ—¶å€™ï¼Œäººä»¬è®¤ä¸ºä¸åŒçš„ä¸»é¢˜ä¹‹é—´æ˜¯ä¸å­˜åœ¨ä»€ä¹ˆè”ç³»çš„ï¼Œä½†è¿™æ˜¯ä¸å¯èƒ½çš„ã€‚
æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä»ã€Šäººæ°‘æ—¥æŠ¥ã€‹æç‚¼å‡ºå…³äºç»æµå‘å±•ã€æ°‘ç”Ÿå·¥ç¨‹ç­‰ä¸»é¢˜ï¼Œä½†æ˜¯æˆ‘ä»¬ç»å¯¹ä¸å¯èƒ½ä»ä¸­æŠ½å–å‡ºå…³äºå¦‡ç§‘å¹¿å‘Šçš„ä¸»é¢˜ï¼Œå› ä¸ºè¿™æ˜¯è¿™ä»½æŠ¥çº¸çš„æ€§è´¨å†³å®šçš„ã€‚
æ‰€ä»¥æˆ‘ä»¬ä»è¯­æ–™ä¸­æŠ½å–å‡ºæ¥çš„ä¸»é¢˜ï¼Œå½¼æ­¤ä¹‹é—´è‚¯å®šæ˜¯å…·æœ‰é«˜åº¦çš„ç›¸å…³æ€§çš„ã€‚

CTMçš„åå­—ä¸­ä¹‹æ‰€ä»¥æœ‰ä¸€ä¸ªcorrelatedï¼Œå°±æ˜¯å› ä¸ºè¿™ä¸ªæ¨¡å‹å¯ä»¥æŠŠæ‰€æœ‰çš„ä¸»é¢˜æ”¾åœ¨ä¸€èµ·ï¼Œæ”¾åœ¨ä¸€ä¸ªç»“æ„ï¼ˆstructureï¼‰é‡Œé¢å»ç†è§£æ¯ä¸€ä¸ªä¸»é¢˜ã€‚
å› æ­¤CTMæ˜¯ä¸€ç§å±‚æ¬¡åŒ–ä¸»é¢˜æ¨¡å‹ï¼Œå®ƒæ˜ç¡®æŠ“å–äº†ä¸»é¢˜é—´çš„æ½œåœ¨ç›¸å…³æ€§ã€‚

ç›¸æ¯”äºLDAï¼ŒCTMæ¨¡å‹å¤šäº†ä¸¤ä¸ªå‚æ•°Î¼ & Î£ï¼šä¸€ä¸ªKç»´çš„å‡å€¼å’Œåæ–¹å·®çŸ©é˜µ $\{\mu,\Sigma\}\sim N(\mu,\Sigma)$ã€‚
:::

## Sparse Additive Generative Model (SAGE)

æ¯ä¸ªä¸»é¢˜éƒ½è¢«èµ‹äºˆä¸€ä¸ªæ¨¡å‹ï¼Œèƒ½å¤Ÿæè¿°ç»™äºˆæ’å®šèƒŒæ™¯åˆ†å¸ƒå¯¹æ•°é¢‘ç‡çš„åå·®ã€‚

![SAGE](https://drhuyue.site:10002/sammo3182/figure/cluster_sageDiagram.png){fig-align="center" height=400}


## Structure Topic Model (STM)

CTM + SAGE + covariants

![STM](https://drhuyue.site:10002/sammo3182/figure/cluster_stmDiagram.png){fig-align="center" height=500}


## æ“ä½œ

ç¤ºä¾‹ï¼šç¾å›½æ€»ç»Ÿå°±èŒæ¼”è¯´æ•°æ®

1. è®¾å®šä¸»é¢˜æ•°ç›®

```{r stm-searchKresult, cache=TRUE}
#å¯¹äºä¸Šè¿°æ¯ä¸€ä¸ªç»“æœï¼Œè®¡ç®—è¿è´¯æ€§å’Œæ’ä»–æ€§
stm_searchK <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/stm_searchK.rds", open = "rb"))

fit_searchK <- map2(stm_searchK, names(stm_searchK), \(result, gName){
  tibble(group = gName,
         exclusivity = exclusivity(result),
         coherence = semanticCoherence(result, dfmat_inaug))
}) |> 
  list_rbind()

#æ±‚å‡ºæ¯ç»„çš„å¹³å‡å€¼
fit_searchK_agg <- group_by(fit_searchK, group) |> 
  summarise(coherence = mean(coherence),
            exclusivity = mean(exclusivity))

#ä½œå›¾
ggplot(fit_searchK_agg, aes(coherence, exclusivity, color = group, size = 3)) +
  geom_point() +
  scale_color_gb(palette = "full")

#é€‰æ‹©ä¸»é¢˜æ•°ä¸º8çš„stmæ¨¡å‹ï¼Œå°†å…¶ä¿å­˜åœ¨stm_selectedä¸­
stm_selected <- stm_searchK$n8
```

## 2. ä¸»é¢˜å½’ç±»

$$Topic \sim Party + s(Year).$$

```{r topicDist, exercise = TRUE, exercise.setup = "stm-searchKresult"}
#æŸ¥çœ‹è¯è¯­åœ¨ä¸»é¢˜ä¸­çš„åˆ†å¸ƒ
labelTopics(stm_selected, topics = c(1:3), n = 5)
```

- Highest Probï¼šé«˜é¢‘è¯
- FREXï¼šä¸»é¢˜é«˜é¢‘è¯
- liftï¼šé€šè¿‡è¯è¯­åœ¨å…¶ä»–ä¸»é¢˜ä¸­çš„é¢‘ç‡ç›¸é™¤æ¥åŠ æƒè¯è¯­
- scoreï¼šå°†è¯è¯­åœ¨ä¸»é¢˜ä¸­çš„å¯¹æ•°é¢‘ç‡é™¤ä»¥è¯è¯­åœ¨å…¶ä»–ä¸»é¢˜ä¸­çš„å¯¹æ•°é¢‘ç‡

:::{.notes}
è¿™è¡Œä»£ç ä¸ºæˆ‘ä»¬è¿”å›äº†æ¯ä¸ªä¸»é¢˜ä¸‹é¢çš„ä¸€äº›è¯è¯­ã€‚

- â€œHighest Probâ€æŒ‡çš„æ˜¯è¿™ä¸ª*è¯­æ–™åº“*ä¸­é¢‘ç‡æœ€é«˜çš„è¯è¯­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‡ºç°äº†é€—å·å’Œå¥å·ï¼Œè¿˜æœ‰â€œamericaâ€ç­‰ã€‚

- FREXçŸ©é˜µï¼ˆFREX matrixï¼‰ä¸­çš„è¯è¯­ä¾ç„¶æ˜¯é«˜é¢‘è¯ï¼Œä½†æ˜¯å®ƒæ˜¯ä»…åœ¨è¿™ä¸ªä¸»é¢˜ä¸­çš„é«˜é¢‘è¯ï¼Œä¹Ÿå°±æ˜¯èƒ½å¤ŸåŒºåˆ†è¿™ä¸€ä¸»é¢˜å’Œå…¶ä»–ä¸»é¢˜ä¹‹é—´ç‰¹æ®Šæ€§çš„é«˜é¢‘è¯ã€‚
å®ƒçš„ç®—æ³•æ˜¯é€šè¿‡è¯è¯­çš„æ•´ä½“é¢‘ç‡åŠå…¶å¯¹ä¸»é¢˜çš„ä¸“æœ‰ç¨‹åº¦æ¥åŠ æƒè¯è¯­ã€‚

- æå‡ï¼ˆliftï¼‰ï¼šä¸FREXç›¸åŒï¼Œä½†æ˜¯ç®—æ³•ä¸ä¸€æ ·ï¼Œå®ƒé€šè¿‡è¯è¯­åœ¨å…¶ä»–ä¸»é¢˜ä¸­çš„é¢‘ç‡ç›¸é™¤æ¥åŠ æƒè¯è¯­ï¼Œå› æ­¤ç»™åœ¨å…¶ä»–ä¸»é¢˜ä¸­è¾ƒå°‘å‡ºç°çš„è¯è¯­èµ‹äºˆæ›´é«˜çš„æƒé‡ã€‚

- èµ‹åˆ†ï¼ˆscoreï¼‰ï¼šä¸FREXç›¸åŒï¼Œä½†æ˜¯ç®—æ³•æ˜¯å°†è¯è¯­åœ¨ä¸»é¢˜ä¸­çš„å¯¹æ•°é¢‘ç‡é™¤ä»¥è¯è¯­åœ¨å…¶ä»–ä¸»é¢˜ä¸­çš„å¯¹æ•°é¢‘ç‡ã€‚

æ‰€ä»¥æˆ‘ä»¬é€šå¸¸å»ºè®®å¤§å®¶ï¼Œå½“ä½ å»å¯¹äºæ¯ä¸ªä¸»é¢˜çš„å«ä¹‰è¿›è¡Œè§£é‡Šçš„æ—¶å€™ï¼Œå¯ä»¥åŸºäºFREXï¼Œliftå’Œscoreæ¥è§£è¯»ï¼Œè€Œä¸æ˜¯åªçœ‹é«˜é¢‘è¯å»è§£é‡Šå®ƒçš„å«ä¹‰ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬è®²çš„è¿™ä¸‰ä¸ªæµ‹é‡è¿™æ˜¯å¸®åŠ©æˆ‘ä»¬å»è§£è¯»ä¸»é¢˜ç”¨çš„ã€‚
:::

## ä¸»é¢˜ç›¸å…³æ€§

ä»¥ä½™å¼¦ç›¸ä¼¼åº¦æ¥è®¡ç®—ç›¸å…³æ€§ï¼Œè¶Šæ¥è¿‘äº1è¡¨ç¤ºä¸¤ä¸ªä¸»é¢˜è¶Šç›¸å…³ï¼Œè¶Šæ¥è¿‘äº0è¡¨ç¤ºä¸¤ä¸ªä¸»é¢˜è¶Šä¸ç›¸å…³ã€‚

```{r topicCorrelation}
#è®¡ç®—STMæ¨¡å‹ä¸­æ‰€æœ‰ä¸»é¢˜ä¹‹é—´çš„ç›¸å…³æ€§
corr_stm8 <- topicCorr(stm_selected)

library(GGally)
library(network)

#è®¡ç®— STM æ¨¡å‹ä¸­ä¸»é¢˜ä¹‹é—´çš„ç›¸å…³æ€§çŸ©é˜µï¼Œå¹¶å–å…¶ç»å¯¹å€¼
net_stm8 <- corr_stm8$cor |> abs() |> as.matrix()
#å°†ç›¸å…³æ€§çŸ©é˜µä¸­çš„å€¼ä¹˜ä»¥10ï¼Œä»¥å¢åŠ å·®å¼‚çš„å¯è§†åŒ–æ•ˆæœã€‚
net_stm8 <- net_stm8 * 10 
#å°†çŸ©é˜µçš„å¯¹è§’çº¿å…ƒç´ è®¾ç½®ä¸º1ï¼Œå› ä¸ºæ¯ä¸ªä¸»é¢˜ä¸è‡ªèº«çš„ç›¸å…³æ€§æ€»æ˜¯1ã€‚
diag(net_stm8) <- 1

#åˆ›å»ºä¸€ä¸ªç½‘ç»œå¯¹è±¡ï¼Œè¡¨ç¤ºä¸»é¢˜ä¹‹é—´çš„ç›¸å…³æ€§
graph_stm8 <- network(net_stm8,
    matrix.type = "adjacency",
    ignore.eval = FALSE,
    names.eval = "weights",
    directed = FALSE
  )

#ç»˜åˆ¶ç½‘ç»œå›¾  
ggnet2(graph_stm8, label = TRUE, edge.size = "weights")
```


## åå˜é‡çš„å½±å“ï¼šå…šæ´¾

```{r covariateParty}
#| fig-height: 6.5

#æ¨¡å‹å»ºæ„
result_stm8 <- estimateEffect(formula = 1:8 ~ Party + s(Year), 
              #è¦åˆ†æçš„stmæ¨¡å‹
               stmobj = stm_selected,
              #å…ƒæ•°æ®æ¥æº
               metadata = docvars(dfmat_inaug))


#ç»˜åˆ¶å…šæ´¾ï¼ˆPartyï¼‰å¯¹ä¸»é¢˜çš„å½±å“çš„æ£®æ—å›¾
plot(result_stm8, 
     covariate = "Party",
     model = stm_selected,
     method = "difference",
     cov.value1 = "Democratic",
     cov.value2 = "Republican",
     xlim = c(-1, 1), 
     xlab = "Democratic <-> Republican")

```

## åå˜é‡çš„å½±å“ï¼šæ—¶é—´

```{r covariateTime}
#ç»˜åˆ¶ä¸»é¢˜3å’Œä¸»é¢˜7åœ¨å¹´ä»½ä¸Šçš„è¿ç»­æ•ˆåº”å›¾
plot(
  result_stm8,
  "Year",
  method = "continuous",
  model = stm_selected,
  topics = c(3, 7)
)
```


## SeedLDA

ä»¥ç§å­è¯ï¼ˆseedsï¼‰å¼•é¢†ä¸»é¢˜åˆ†ç±»ã€‚

ç¤ºä¾‹æ•°æ®ï¼šã€Šå«æŠ¥ã€‹2016

ç§å­è¯ï¼šç»æµã€æ”¿æ²»ã€ç¤¾ä¼šã€å¤–äº¤å’Œå†›äº‹ï¼ŒåŸºäºå¯¹è¯¥ç±»æ–°é—»çš„è®¤çŸ¥è·å–

```{r seeds, exercise = TRUE}
# é¢„å¤„ç† ###
## è¯»å–æ•°æ®
corp_news_2016 <- corpus_subset(corp_news, year(date) == 2016)

#æ ‡è®°åŒ–ä¸æ¸…æ´—
toks_news_guardian <-
  tokens(
    corp_news_2016,
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbol = TRUE
  ) |>
  tokens_remove(pattern = c(stopwords("en"), "*-time", "updated-*", "gmt", "bst"))

#çŸ©é˜µåŒ–ä¸æ¸…æ´—
dfmat_news_guardian <- dfm(toks_news_guardian) |>
  dfm_trim(
    min_termfreq = 0.8,
    termfreq_type = "quantile",
    max_docfreq = 0.1,
    docfreq_type = "prop"
  )

## è¯»å–è¯å…¸æ•°æ®
dict_topic <- dictionary(file = system.file("extdata", "topics.yml", package = "drhurText"))
dict_topic
```

## æ•ˆæœæ¯”è¾ƒï¼šLDA

```{r seedLda, eval=FALSE}
# åˆ†æ ####

#ä½¿ç”¨ldaæ¨¡å‹
tmod_lda <- textmodel_lda(dfmat_news_guardian, k = 5)
#ä½¿ç”¨seeded ldaæ¨¡å‹
tmod_slda <- textmodel_seededlda(dfmat_news_guardian, dictionary = dict_topic)

save(tmod_lda, tmod_slda, file = here::here("slides", "courses", "governmentalBigData", "data", "text_seedLDA.rda"))
```

```{r lda-out}
load(url("https://drhuyue.site:10002/sammo3182/data/text_seedLDA.rda", open = "rb"))

#æ·»åŠ ä¸€ä¸ªæ–°çš„æ–‡æ¡£çº§åˆ«çš„å˜é‡ä»¥å­˜å‚¨seededLDA æ¨¡å‹ä¸ºæ¯ç¯‡æ–‡ç« åˆ†é…çš„ä¸»é¢˜ã€‚
dfmat_news_guardian$topic_seeded <- topics(tmod_slda) 

# æ¯”è¾ƒ

seededlda::terms(tmod_lda, 8) %>% 
  as.data.frame() %>% 
  tt()
```

## æ•ˆæœæ¯”è¾ƒï¼šSeedLDA

```{r seededlda-out}
seededlda::terms(tmod_slda, 8) %>% 
  as.data.frame() |> 
  tt()
```


:::{.notes}
è€Œæ¯”è¾ƒLDAä¸seeded LDAåšå‡ºçš„ä¸»é¢˜åˆ†ç±»ç»“æœï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å‘ç°ï¼Œåœ¨LDAåˆ†æˆçš„ä¸»é¢˜3ä¸‹é¢ï¼Œä¸ä»…æœ‰â€œclimateâ€å’Œ"water"ï¼Œè¿˜æœ‰"apple"å’Œ"education"ï¼Œå®ƒåˆ†ç±»çš„æ•ˆæœå¾ˆæ˜æ˜¾æ˜¯ä¸å¤ªå¥½çš„ã€‚
ä½†æ˜¯åœ¨seeded LDAä¸­ï¼Œæ¯ä¸ªä¸»é¢˜ä¸‹é¢çš„è¯è¯­å¤§ä½“ä¸Šç¬¦åˆæˆ‘ä»¬çš„è®¤çŸ¥ã€‚
:::


## keyATM





# çº³å…¥è¯­ä¹‰ä¿¡æ¯

## ç»™è¯ä¹‰å»ºæ¨¡ï¼šè¯åµŒå…¥(Word embedding)

> Words' meanings depend not just on immediate neighbors

![](https://drhuyue.site:10002/sammo3182/figure/theory_wordEmbedding.png){fig-align="center" height=550}

:::{.notes}
The term "embedding" comes from the neural network literature, in which an "embedding layer" is an input function that efficiently compresses high-dimensional data down to a low-dimensional dense representation for input to subsequent neural network layers.

- The embedding model GloVe ("Global Vectors") by @PenningtonEtAl2014 is explicitly designed to construct word vectors encoding local co-occurrence.
- An equally influential word embedding model is Word2Vec [@BengioEtAl2000], which treats each instance of a word and its context as a separate prediction problem that word vectors are chosen to solve.
:::

## ä¸»é¢˜æ¨¡å‹(Topic modeling)

{{< video https://drhuyue.site:10002/sammo3182/video/theory_topicModeling.webm title="What happened in topic modeling" height=600 loading="eager" allowfullscreen>}}

:::{.notes}
LSA, NMF, and LDA can also be viewed as producing word embeddings. In particular, the (V Ã— K) matrix B from (2) contains a series of row vectors corresponding to each term in the vocabulary (see also Levy and Goldberg 2014). Those vectors contain information about word co-occurrence at the document level, rather than within a local context.
:::

## æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanism)

Word embedding è®¤ä¸ºæ‰€æœ‰è¯å’Œè¯ä¹‹é—´å…³ç³»éƒ½åŒç­‰é‡è¦[ğŸ¤¦â€â™‚ï¸]{.large}

:::{.fragment .fade-in-then-semi-out}
"Attention is all you need" [@VaswaniEtAl2017]

ä»¥ä¸‹æ˜¯å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ³¨æ„åŠ›æœºåˆ¶çš„ç¤ºä¾‹ç¿»è¯‘ï¼š

> ä½œä¸ºåœ¨_____é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„è½¯ä»¶å·¥ç¨‹å¸ˆã€‚    
> ä½œä¸ºåœ¨_____é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„å¤ªé˜³èƒ½å·¥ç¨‹å¸ˆã€‚

åº”è¯¥åœ¨ "_____" å¡«å…¥ä»€ä¹ˆè¯ï¼Ÿä½ æ˜¯å¦‚ä½•å¾—å‡ºè¿™ä¸ªç»“è®ºçš„ï¼Ÿ

:::


:::{.fragment}
ä½œä¸ºåœ¨*ä¿¡æ¯æŠ€æœ¯*é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„[è½¯ä»¶]{.red}å·¥ç¨‹å¸ˆã€‚    
ä½œä¸ºåœ¨*ç»¿è‰²èƒ½æº*é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„[å¤ªé˜³èƒ½]{.red}å·¥ç¨‹å¸ˆã€‚

:::


## è¯åµŒå…¥ &rarr; è¯åºåˆ—ï¼ˆWord Sequenceï¼‰

:::{.callout-note}
## è‡ªæ³¨æ„åŠ›æœºåˆ¶

è¾“å…¥ä¸€ä¸ªåˆå§‹è¯å…ƒåµŒå…¥åºåˆ—ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ–°çš„è¯å…ƒåµŒå…¥åºåˆ—ï¼Œ[ä½¿åˆå§‹åµŒå…¥èƒ½å¤Ÿç›¸äº’ä½œç”¨]{.red}ã€‚
:::

- ç”±å †å çš„æ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œå±‚ç»„æˆçš„å¤§å‹ç¥ç»ç½‘ç»œå¯ä»¥ä½¿ç”¨ä¸“ç”¨å¤„ç†å™¨é«˜æ•ˆå¹¶è¡Œè®­ç»ƒï¼Œå³ [Transformer]{.red}

:::{.fragment}
- å¸¸è§çš„ Transformer æ¨¡å‹
    - BERT
        - RoBERTa, PALM
    - [GPT]{.red .large} ç³»åˆ—
:::



## æ€»ç»“

:::: {.columns}

::: {.column width="50%"}
1. è®¤çŸ¥
    + ä¸°å¯Œèµ„æº
    + æŠ€æœ¯é—¨æ§›
1. åŸåˆ™
    + åœ¨â€œé”™è¯¯â€çš„å‰æä¸‹å¯»æ‰¾ä»·å€¼
:::

::: {.column width="50%"}
3. æ“ä½œ
    - æ‰“æ•£ï¼šé¢„å¤„ç†ä¸ç»“æ„åŒ–
    - èšåˆï¼š
        - è¯é¢‘
        - ç›¸å…³æ€§/ç›¸ä¼¼åº¦
        - BOWä¹‹ä¸Šï¼ˆè¯­å¢ƒä¸è”ç³»ï¼‰
:::
::::

![Distant reading](https://drhuyue.site:10002/sammo3182/figure/text_indirect_phone.gif){.fragment fig-align="center" height=300}



# æ„Ÿè°¢å€¾å¬ï¼Œæ•¬è¯·æŒ‡æ­£ {background="#43464B"}

:::{style="text-align: right; margin-top: 1em"}  

[`r feather_icons("github")`&nbsp; sammo3182](https://github.com/sammo3182)

[`r feather_icons("mail")`&nbsp; yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

[`r feather_icons("globe")`&nbsp; https://www.drhuyue.site](https://www.drhuyue.site)

![](https://user-images.githubusercontent.com/6463211/232207708-b0e64eee-7fb3-45a4-9779-ec52397f786c.png){height=250}
:::

## å‚è€ƒæ–‡çŒ®

::: {#refs}
:::
