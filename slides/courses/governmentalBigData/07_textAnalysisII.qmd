---
title: "æ–‡æœ¬çš„æ•°æ®åˆ†æè¿›é˜¶"
subtitle: "æ”¿åŠ¡å¤§æ•°æ®åº”ç”¨ä¸åˆ†æ (80700673)"
author: "èƒ¡æ‚¦"
institute: "æ¸…åå¤§å­¦ç¤¾ä¼šç§‘å­¦å­¦é™¢" 
bibliography: ../camsTextAnalysis/pre_cams.bib
knitr: 
    opts_chunk:
      echo: false
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  drhutools, 
  icons,
  gridExtra,
  knitr, # dependency
  stringr, 
  tidytext, 
  tidyverse,
  lubridate,
  quanteda,
  quanteda.textstats,
  quanteda.textplots,
  quanteda.corpora,
  text2vec,
  LSX,
  seededlda,
  newsmap,
  keyATM,
  stm,
  tinytable,
  patchwork
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```



## ä¸ªäººç®€ä»‹{.Small}

:::: {.columns}

::: {.column width="60%"}
*ä¸ªäººç»å†*

- æ”¿æ²»å­¦åšå£«[ï¼ˆUniversity of Iowa)]{.small}
  - ä¿¡æ¯å­¦[ï¼ˆGraduated Certificate in Informatics)]{.small}
- æ¸…åå¤§å­¦è®¡ç®—ç¤¾ä¼šç§‘å­¦å¹³å°[(å‰¯ä¸»ä»»)]{.small}
  - æ¸…åæ•°æ®ä¸æ²»ç†ä¸­å¿ƒ[(å‰¯ä¸»ä»»)]{.small}
  - è®¡ç®—ç¤¾ä¼šç§‘å­¦ç¼–ç¨‹è¯­è¨€è¯ä¹¦é¡¹ç›®[ï¼ˆè´Ÿè´£äººï¼‰]{.small}
  - Learning R with Dr. Hu & Friends å·¥ä½œåŠ[ï¼ˆåˆ›å§‹äººï¼‰]{.small}

:::{.fragment}
*ç ”ç©¶å…´è¶£ï¼šè®¤çŸ¥ã€è¡Œä¸ºä¸ç°ä»£æ€§*

- **æ–¹æ³•è·¯å¾„ï¼šè®¡ç®—æ”¿æ²»å­¦**
  - å®éªŒå®¤å’Œè°ƒæŸ¥å®éªŒ
  - æ½œå˜é‡åˆ†æã€ç½‘ç»œåˆ†æã€ç©ºé—´åˆ†æ
  - æ–‡æœ¬å¤§æ•°æ®åˆ†æã€æ•°æ®å¯è§†åŒ–
:::

:::

::: {.column .fragment width="40%"}
*ç ”ç©¶é¢†åŸŸï¼šæ¯”è¾ƒæ”¿æ²»ã€å›½å®¶æ²»ç†*

- **W. å¿ƒç†å­¦**
  - æ”¿æ²»[è®¤çŸ¥]{.red}æ²»ç†
  - è¡Œä¸ºå…¬å…±[æ”¿ç­–]{.red}
  - æ”¿æ²»ä¼ æ’­

- **W. ç»æµå­¦**
  - ç»æµä¸å¹³ç­‰[æ„ŸçŸ¥]{.red}
  - å…¬å…±è®¾æ–½ã€æœåŠ¡å‡ç­‰åŒ–

- **W. è¯­è¨€å­¦**
  - æƒåŠ›èƒŒä¹¦çš„[è¯­è¨€æ•ˆæœ]{.red}ä¸æœºåˆ¶
  - è¯­è¨€æ”¿ç­–çš„æ²»ç†åŠŸèƒ½

:::

::::

## å¤ä¹ 

> @King2015: [The big-data approach is] the [end]{.red} of the quantitative-qualitative divide.

:::{.notes}
King talked about this issue in many places including Shanghai Jiaotong University
:::

:::: {.columns}

::: {.column .fragment width="50%"}
ä½ åº”è¯¥å·²ç»çŸ¥é“â€¦â€¦

- ä½ èƒ½ç”¨æ–‡æœ¬æ•°æ®åšä»€ä¹ˆ
    - ä½ åˆ†æçš„æ˜¯æ–‡å­—è¿˜æ˜¯è¯­è¨€ï¼Ÿ
    - Close reading or distant readingï¼Ÿ
    - æ–‡æœ¬/éŸ³é¢‘/è§†é¢‘åˆ†æçš„ç†è®ºåŸºç¡€æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•è·å–æ•°æ®
    - æ–‡æœ¬æ•°æ®çš„è·å–æ¸ é“æœ‰å“ªäº›ï¼Ÿ
    - ç½‘ç»œçˆ¬å–ä¸æ­£åˆ™è¡¨è¾¾å¼

:::

::: {.column .fragment width="50%"}

- å¦‚ä½•å¤„ç†æ•°æ®
    - å¦‚ä½•ç»“æ„åŒ–æ–‡æœ¬æ•°æ®ï¼Ÿ
    - æ–‡æœ¬é¢„å¤„ç†çš„æ­¥éª¤æœ‰å“ªäº›
    - Tokenizationçš„ä¸¤ç§å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•åˆ†ææ•°æ®
    - è¯é¢‘èƒ½åˆ†æå‡ºä»€ä¹ˆï¼Ÿ
    - å¦‚ä½•é‰´åˆ«å…³é”®è¯ï¼Ÿ
    - è¯çš„ç›¸ä¼¼åº¦ï¼Ÿ
    - ä¸»é¢˜æ¨¡å‹åœ¨å¹²ä»€ä¹ˆï¼Ÿ
        - Bag of Words (BOW)?

:::

::::

## æè¦

å­¦å®Œæœ¬è¯¾ï¼Œä½ å°†äº†è§£:

:::{ style="text-align:center"}

- å¦‚ä½•æ•æ‰â€œè‰è›‡ç°çº¿â€ï¼š*è¯æ±‡å±‚çº§*ä¿¡æ¯æ±‡å…¥
- å¦‚ä½•æå‡ä¸»é¢˜æ¨¡å‹è´¨é‡ï¼š*æ¦‚å¿µå±‚çº§*ä¿¡æ¯æ±‡å…¥
- è¯åµŒå…¥å’ŒLLMå¹²äº†ä»€ä¹ˆï¼š*è¯­ä¹‰å±‚çº§*ä¿¡æ¯æ±‡å…¥

[æœ¬è®²çš„æ ¸å¿ƒè®®é¢˜ï¼š[çªç ´è¯åŸºé™åˆ¶ï¼Œçº³å…¥æœ‰ç”¨ä¿¡æ¯]{.red}]{.fragment .large}
:::

:::{.fragment .callout-warning .incremental}
- å­¦ä¹ æœ¬è¯¾å†…å®¹ï¼Œä½ ä¸éœ€è¦ç¼–ç¨‹çŸ¥è¯†ğŸ˜±
- åº”ç”¨æœ¬è¯¾å†…å®¹ï¼Œä½ éœ€è¦ä¸€ç§ç¼–ç¨‹çŸ¥è¯†ğŸ˜œ
    - [å¦‚æœä½ æƒ³å­¦â€¦â€¦](https://www.drhuyue.site/course/method-series/04-r-workshop/)
:::

# é—®é¢˜æºå¤´

## è®©è®¡ç®—æœºè¯»æ‡‚äººè¨€çš„ä»£ä»·

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_bagOfWords.png){fig-align="center" height=400}

![Document-Term Matrix (DTM)](https://drhuyue.site:10002/sammo3182/figure/theory_bagOfWords2.jpg){.fragment fig-align="center" height=600}
:::


:::{.notes}
In linguistics, the opposite of natural language is artificial/constructed language (conlangs),  like Klingon in "Star Trek," Dothraki in "Game of Thrones"
:::

## ä¸¢å¤±äº†ä»€ä¹ˆ

:::{.fragment .large style="text-align:center; margin-top: 2em"}
- è¯çš„é‡è¦æ€§
- è¯­åº/ä½ç½®
- è™šè¯
- è¯­æ³•
- Meta data
:::


# è¯æ±‡å±‚çº§ä¿¡æ¯æ±‡å…¥

## åŠ æƒ

DTMçš„é—®é¢˜ï¼š

1. æœªå°†è¯çš„é‡è¦æ€§çº³å…¥è€ƒé‡
2. è¿‡åº¦ä½“ç°å¸¸è§è¯
3. è½»è§†å°‘è§è¯

:::{.fragment}
ä¸¾ä¾‹ï¼šTerm frequency-inverse document frequency (TF-IDF)

:::: {.columns}

::: {.column width="50%"}
$$\displaystyle \mathrm {tf} (t,d)={\frac {f_{t,d}}{\sum _{t'\in d}{f_{t',d}}}},$$ where $f_{t,d}$ is the number of times that term t occurs in document d. 
:::

::: {.column width="50%"}
$$\displaystyle \mathrm {idf} (t,D)=\log {\frac {N}{|\{d:d\in D{\text{ and }}t\in d\}|}},$$ 

- $N$: total number of documents; D.
- $|\{d\in D:t\in d\}|$ : number of documents where the term $t$ appears.

:::

::::

:::

:::{.notes}
TFï¼š Importance of a term in a document
IDFï¼š Frequency a term appear across documents
:::


## åŠ æƒå¸¦æ¥ä»€ä¹ˆ

- å¥½å¤„ï¼š
  1. è¯†åˆ«é‡è¦è¯æ±‡
  2. å‡å°‘å¸¸è§è¯æ±‡çš„æƒé‡
  3. æé«˜æœºå™¨å­¦ä¹ æ€§èƒ½ (why?)

:::{.fragment}
- å‰¯ä½œç”¨ï¼š
  1. å‡å®šè¯æ±‡ç‹¬ç«‹ï¼ˆä¸DTMç›¸åŒï¼‰
  2. å¯¹åœ¨è¯­æ–™åº“ä¸­éå¸¸ç½•è§ä½†åœ¨ç‰¹å®šæ–‡æ¡£ä¸­å‡ºç°è¿‡å‡ æ¬¡çš„ç½•è§è¯æ±‡ç»™äºˆé«˜æƒé‡
  3. å¯¹è¯­æ–™åº“çš„å¤§å°å’Œå¤šæ ·æ€§æ•æ„Ÿ
:::


## N-gramåˆ†æ

- Markov Model of Order N
    + Unigram: æ¸…å å¤§å­¦ ç¤¾ä¼š ç§‘å­¦ å­¦é™¢
    + Bigram: æ¸…åå¤§å­¦ å¤§å­¦ç¤¾ä¼š ç¤¾ä¼šç§‘å­¦ ç§‘å­¦ å­¦é™¢
    + Trigramï¼šæ¸…åå¤§å­¦ç¤¾ä¼š å¤§å­¦ç¤¾ä¼šç§‘å­¦ ç¤¾ä¼šç§‘å­¦å­¦é™¢

![](https://drhuyue.site:10002/sammo3182/figure/text_ngram.png){.fragment fig-align="center" height=400}

## æ­é…åˆ†æ

- è¿ç»­æ­é…ï¼ˆContiguous collocationsï¼‰ï¼šåœ¨æ–‡æœ¬ä¸­ç›´æ¥ç›¸é‚»å‡ºç°ã€‚
- æ­ç¤ºè¯­è¨€ä½¿ç”¨ä¸­çš„æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼ä»æŸ¥çœ‹å•ä¸ªå•è¯æ—¶å¹¶ä¸ç«‹å³æ˜æ˜¾ã€‚

ç¤ºä¾‹æ•°æ®ï¼š2012å¹´åˆ°2016å¹´çš„6,000ç¯‡ã€Šå«æŠ¥ã€‹æ–°é—»æ–‡ç« 

```{r guardianData, cache=TRUE}
# 2016å¹´å«æŠ¥æ–°é—»è¯­æ–™åº“ï¼ˆCorpus of Guardian news in 2016 from those from 2012--2015ï¼‰

corp_news <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/data_corpus_guardian.rds", open = "rb"))

toks_news_guardian <- tokens(corp_news, remove_punct = TRUE)

corp_news
```

## Collocation ç¤ºä¾‹

- æœ€å¸¸è§çš„è¯å¯¹ï¼ˆpairs of wordsï¼‰
- æœ€å¸¸è§çš„ä¸‰è¯æ¨¡å¼ï¼ˆthree-word patternsï¼‰

```{r collocation, message=FALSE}
toks_select <- tokens_select(
  toks_news_guardian,
  pattern = "^[A-Z]",
  valuetype = "regex",
  case_insensitive = FALSE,
  padding = TRUE
) 

tstat_col_caps <- textstat_collocations(toks_select, min_count = 100)
head(tstat_col_caps, 10)

tstat_col_caps3 <- textstat_collocations(toks_select, min_count = 80, size = 3)
head(tstat_col_caps3, 10)
```

## â€œé¶å‘â€åˆ†æ

- å…³é”®æ€§(Keyness)ï¼šè¯†åˆ«åœ¨ç›®æ ‡è¯­æ–™åº“ä¸­æ¯”åœ¨å‚ç…§è¯­æ–™åº“ä¸­**ç»Ÿè®¡ä¸Šæ›´é¢‘ç¹**å‡ºç°çš„è¯è¯­çš„åº¦é‡æ–¹æ³• [@Gabrielatos2018]ã€‚

ç¤ºä¾‹1ï¼šå¯¹æ¯”ã€Šå«æŠ¥ã€‹æ–°é—»2016å¹´ä¸2012â€”2015å¹´ä¹‹é—´çš„æ–°é—»

```{r keyness, eval=FALSE}
dfmat_news <- tokens(corp_news, remove_punct = TRUE) |> 
  dfm()
 
tstat_key <- textstat_keyness(dfmat_news,
                              target = lubridate::year(dfmat_news$date) >= 2016)

saveRDS(tstat_key, file = here::here("slides", "courses", "governmentalBigData", "data", "text_keyness.rds"))
```

```{r keyness-out}
readRDS(url("https://drhuyue.site:10002/sammo3182/data/text_keyness.rds", open = "rb")) %>% textplot_keyness(n = 10)
```


## Keyness ç¤ºä¾‹2

åœ¨2012å¹´åˆ°2016å¹´çš„6,000ç¯‡ã€Šå«æŠ¥ã€‹æ–°é—»æ–‡ç« ä¸­ä¸æ¬§ç›Ÿï¼ˆ"EU", "europ*", "european union"ï¼‰ç›¸å…³çš„è¯æ±‡

```{r relevantKey}
eu <- c("EU", "europ*", "european union")

toks_inside <- tokens_keep(toks_news_guardian, pattern = eu, window = 10) |> 
  tokens_remove(pattern = eu) # remove the keywords
toks_outside <- tokens_remove(toks_news_guardian, pattern = eu, window = 10)

dfmat_inside <- dfm(toks_inside)
dfmat_outside <- dfm(toks_outside)

tstat_key_inside <-
  textstat_keyness(rbind(dfmat_inside, dfmat_outside),
                   target = seq_len(ndoc(dfmat_inside)))

textplot_keyness(tstat_key_inside, n = 10)
```


## â€œå…¥æœ¨ä¸‰åˆ†â€åˆ†æ [@Liu2022]

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_pronoun.png){fig-align="center" height=400}

![](https://drhuyue.site:10002/sammo3182/figure/css_liwcTree.png){.fragment fig-align="center" height=600}
:::


## å°ç»“

:::{style="text-align:center; margin-top: 2em"}
- Weighing &larr; ä»[è¯é¢‘]{.red}æ”«å–ä¿¡æ¯
- N-gram &larr; ä»[é‚»å±…]{.red}æ”«å–ä¿¡æ¯
- Collocation &larr; ä»[å…±ç°]{.red}æ”«å–ä¿¡æ¯
- Keyness &larr; ä»[å…³é”®è¯å®šä½]{.red}æ”«å–ä¿¡æ¯
- Functional words &larr; ä»[ç¤¾ä¼šå¿ƒç†]{.red}æ”«å–ä¿¡æ¯
:::


# æ¦‚å¿µå±‚çº§ä¿¡æ¯æ±‡å…¥

## ä¸»é¢˜æ¨¡å‹èƒ½å¹²ä»€ä¹ˆ

{{< video https://drhuyue.site:10002/sammo3182/video/theory_topicModeling.webm title="What happened in topic modeling" height=600 loading="eager" allowfullscreen>}}

:::{.notes}
åŸºäºè¯é¢‘ä¸å…±çº¿çš„unsupervisedé™ç»´ï¼Œæ˜¯ä¸€ç§frequency-basedçš„é™ç»´
:::

## ä¸»é¢˜æ¨¡å‹ç¼ºä»€ä¹ˆ

> â€œä»–çš„è„¸çªç„¶è¢«é­”æ–çš„å…‰ç…§äº®äº†ã€‚è¿™æ˜¯ä¸€å¼ å› ç—›è‹¦ã€ææƒ§å’Œæ„¤æ€’è€Œå˜å¾—ç”ŸåŠ¨çš„è„¸ã€‚çº¢è‰²çš„çœ¼ç›å‘é‚£ä¸ªçœ‹ä¸è§çš„ç”·å­©ç«™ç€çš„åœ°æ–¹å°„å»ï¼Œä»–çš„éšå½¢æ–—ç¯·é®ä½äº†ä»–ã€‚ä»–çš„å£°éŸ³ï¼Œå½“ä»–å‘å‡ºå£°éŸ³æ—¶ï¼Œå°±åƒä¸€ä¸ªå†¬å¤©çš„å¤œæ™šä¸€æ ·å†·ã€‚ä»–è¯´ï¼Œâ€œæˆ‘å›æ¥äº†ï¼Œæ¯”ä»¥å‰æ›´å¼ºå¤§äº†ã€‚â€

:::{.notes}
å“ˆåˆ©æ³¢ç‰¹ä¸ç«ç„°æ¯
:::

:::{.large .fragment style="text-align:center"}
- ä¸»é¢˜ä¹‹é—´çš„è”ç³»
- ç¯‡ç« ä¹‹é—´çš„è”ç³»
- å†…å®¹èƒŒæ™¯çŸ¥è¯†ï¼ˆå¤–éƒ¨ä¿¡æ¯ï¼‰
:::

:::{.large .fragment style="text-align:center"}
&darr;    
STM/SeedLDA/keyATM
:::

## Correlated Topic Model (CTM)

ä¸»é¢˜ä¹‹é—´å½¼æ­¤å…³è”è¢«çº³å…¥è€ƒé‡

:::{.r-vstack}
![LDA](https://drhuyue.site:10002/sammo3182/figure/cluster_ldaDiagram.png){fig-align="center" height=150}

![CTM](https://drhuyue.site:10002/sammo3182/figure/cluster_ctmDiagram.png){fig-align="center" height=150}
:::

 $$\{\mu,\Sigma\}\sim N(\mu,\Sigma).$$

:::{.notes}
CTMæ¨¡å‹ï¼ˆcorrelated topic modelï¼‰çš„è¿›æ­¥ä¹‹å¤„åœ¨äºå“ªé‡Œå‘¢ï¼Ÿ

æœ€åˆäººä»¬åšLDAæ¨¡å‹çš„æ—¶å€™ï¼Œäººä»¬è®¤ä¸ºä¸åŒçš„ä¸»é¢˜ä¹‹é—´æ˜¯ä¸å­˜åœ¨ä»€ä¹ˆè”ç³»çš„ï¼Œä½†è¿™æ˜¯ä¸å¯èƒ½çš„ã€‚
æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä»ã€Šäººæ°‘æ—¥æŠ¥ã€‹æç‚¼å‡ºå…³äºç»æµå‘å±•ã€æ°‘ç”Ÿå·¥ç¨‹ç­‰ä¸»é¢˜ï¼Œä½†æ˜¯æˆ‘ä»¬ç»å¯¹ä¸å¯èƒ½ä»ä¸­æŠ½å–å‡ºå…³äºå¦‡ç§‘å¹¿å‘Šçš„ä¸»é¢˜ï¼Œå› ä¸ºè¿™æ˜¯è¿™ä»½æŠ¥çº¸çš„æ€§è´¨å†³å®šçš„ã€‚
æ‰€ä»¥æˆ‘ä»¬ä»è¯­æ–™ä¸­æŠ½å–å‡ºæ¥çš„ä¸»é¢˜ï¼Œå½¼æ­¤ä¹‹é—´è‚¯å®šæ˜¯å…·æœ‰é«˜åº¦çš„ç›¸å…³æ€§çš„ã€‚

CTMçš„åå­—ä¸­ä¹‹æ‰€ä»¥æœ‰ä¸€ä¸ªcorrelatedï¼Œå°±æ˜¯å› ä¸ºè¿™ä¸ªæ¨¡å‹å¯ä»¥æŠŠæ‰€æœ‰çš„ä¸»é¢˜æ”¾åœ¨ä¸€èµ·ï¼Œæ”¾åœ¨ä¸€ä¸ªç»“æ„ï¼ˆstructureï¼‰é‡Œé¢å»ç†è§£æ¯ä¸€ä¸ªä¸»é¢˜ã€‚
å› æ­¤CTMæ˜¯ä¸€ç§å±‚æ¬¡åŒ–ä¸»é¢˜æ¨¡å‹ï¼Œå®ƒæ˜ç¡®æŠ“å–äº†ä¸»é¢˜é—´çš„æ½œåœ¨ç›¸å…³æ€§ã€‚

ç›¸æ¯”äºLDAï¼ŒCTMæ¨¡å‹å¤šäº†ä¸¤ä¸ªå‚æ•°Î¼ & Î£ï¼šä¸€ä¸ªKç»´çš„å‡å€¼å’Œåæ–¹å·®çŸ©é˜µ $\{\mu,\Sigma\}\sim N(\mu,\Sigma)$ã€‚
:::

## Sparse Additive Generative Model (SAGE)

æ¯ä¸ªä¸»é¢˜éƒ½è¢«èµ‹äºˆä¸€ä¸ªæ¨¡å‹ï¼Œèƒ½å¤Ÿæè¿°ç»™äºˆæ’å®šèƒŒæ™¯åˆ†å¸ƒå¯¹æ•°é¢‘ç‡çš„åå·®ã€‚

![SAGE](https://drhuyue.site:10002/sammo3182/figure/cluster_sageDiagram.png){fig-align="center" height=400}


## Structure Topic Model [STM, @RobertsEtAl2013]

CTM + SAGE

![STM](https://drhuyue.site:10002/sammo3182/figure/cluster_stmDiagram.png){fig-align="center" height=500}


## æ“ä½œ

ç¤ºä¾‹ï¼šç¾å›½æ€»ç»Ÿå°±èŒæ¼”è¯´æ•°æ®

1. è®¾å®šä¸»é¢˜æ•°ç›®

```{r stm-searchKresult, cache=TRUE}
#å¯¹äºä¸Šè¿°æ¯ä¸€ä¸ªç»“æœï¼Œè®¡ç®—è¿è´¯æ€§å’Œæ’ä»–æ€§
stm_searchK <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/stm_searchK.rds", open = "rb"))

fit_searchK <- map2(stm_searchK, names(stm_searchK), \(result, gName){
  tibble(group = gName,
         exclusivity = exclusivity(result),
         coherence = semanticCoherence(result, dfmat_inaug))
}) |> 
  list_rbind()

#æ±‚å‡ºæ¯ç»„çš„å¹³å‡å€¼
fit_searchK_agg <- group_by(fit_searchK, group) |> 
  summarise(coherence = mean(coherence),
            exclusivity = mean(exclusivity))

#ä½œå›¾
ggplot(fit_searchK_agg, aes(coherence, exclusivity, color = group, size = 3)) +
  geom_point() +
  scale_color_gb(palette = "full")

#é€‰æ‹©ä¸»é¢˜æ•°ä¸º8çš„stmæ¨¡å‹ï¼Œå°†å…¶ä¿å­˜åœ¨stm_selectedä¸­
stm_selected <- stm_searchK$n8
```

## 2. ä¸»é¢˜å½’ç±»

$$Topic \sim Party + s(Year).$$

```{r topicDist, exercise = TRUE, exercise.setup = "stm-searchKresult"}
#æŸ¥çœ‹è¯è¯­åœ¨ä¸»é¢˜ä¸­çš„åˆ†å¸ƒ
labelTopics(stm_selected, topics = c(1:3), n = 5)
```

- Highest Probï¼šé«˜é¢‘è¯
- FREXï¼šä¸»é¢˜é«˜é¢‘è¯
- liftï¼šé€šè¿‡è¯è¯­åœ¨å…¶ä»–ä¸»é¢˜ä¸­çš„é¢‘ç‡ç›¸é™¤æ¥åŠ æƒè¯è¯­
- scoreï¼šå°†è¯è¯­åœ¨ä¸»é¢˜ä¸­çš„å¯¹æ•°é¢‘ç‡é™¤ä»¥è¯è¯­åœ¨å…¶ä»–ä¸»é¢˜ä¸­çš„å¯¹æ•°é¢‘ç‡

:::{.notes}
è¿™è¡Œä»£ç ä¸ºæˆ‘ä»¬è¿”å›äº†æ¯ä¸ªä¸»é¢˜ä¸‹é¢çš„ä¸€äº›è¯è¯­ã€‚

- â€œHighest Probâ€æŒ‡çš„æ˜¯è¿™ä¸ª*è¯­æ–™åº“*ä¸­é¢‘ç‡æœ€é«˜çš„è¯è¯­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‡ºç°äº†é€—å·å’Œå¥å·ï¼Œè¿˜æœ‰â€œamericaâ€ç­‰ã€‚

- FREXçŸ©é˜µï¼ˆFREX matrixï¼‰ä¸­çš„è¯è¯­ä¾ç„¶æ˜¯é«˜é¢‘è¯ï¼Œä½†æ˜¯å®ƒæ˜¯ä»…åœ¨è¿™ä¸ªä¸»é¢˜ä¸­çš„é«˜é¢‘è¯ï¼Œä¹Ÿå°±æ˜¯èƒ½å¤ŸåŒºåˆ†è¿™ä¸€ä¸»é¢˜å’Œå…¶ä»–ä¸»é¢˜ä¹‹é—´ç‰¹æ®Šæ€§çš„é«˜é¢‘è¯ã€‚
å®ƒçš„ç®—æ³•æ˜¯é€šè¿‡è¯è¯­çš„æ•´ä½“é¢‘ç‡åŠå…¶å¯¹ä¸»é¢˜çš„ä¸“æœ‰ç¨‹åº¦æ¥åŠ æƒè¯è¯­ã€‚

- æå‡ï¼ˆliftï¼‰ï¼šä¸FREXç›¸åŒï¼Œä½†æ˜¯ç®—æ³•ä¸ä¸€æ ·ï¼Œå®ƒé€šè¿‡è¯è¯­åœ¨å…¶ä»–ä¸»é¢˜ä¸­çš„é¢‘ç‡ç›¸é™¤æ¥åŠ æƒè¯è¯­ï¼Œå› æ­¤ç»™åœ¨å…¶ä»–ä¸»é¢˜ä¸­è¾ƒå°‘å‡ºç°çš„è¯è¯­èµ‹äºˆæ›´é«˜çš„æƒé‡ã€‚

- èµ‹åˆ†ï¼ˆscoreï¼‰ï¼šä¸FREXç›¸åŒï¼Œä½†æ˜¯ç®—æ³•æ˜¯å°†è¯è¯­åœ¨ä¸»é¢˜ä¸­çš„å¯¹æ•°é¢‘ç‡é™¤ä»¥è¯è¯­åœ¨å…¶ä»–ä¸»é¢˜ä¸­çš„å¯¹æ•°é¢‘ç‡ã€‚

æ‰€ä»¥æˆ‘ä»¬é€šå¸¸å»ºè®®å¤§å®¶ï¼Œå½“ä½ å»å¯¹äºæ¯ä¸ªä¸»é¢˜çš„å«ä¹‰è¿›è¡Œè§£é‡Šçš„æ—¶å€™ï¼Œå¯ä»¥åŸºäºFREXï¼Œliftå’Œscoreæ¥è§£è¯»ï¼Œè€Œä¸æ˜¯åªçœ‹é«˜é¢‘è¯å»è§£é‡Šå®ƒçš„å«ä¹‰ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬è®²çš„è¿™ä¸‰ä¸ªæµ‹é‡è¿™æ˜¯å¸®åŠ©æˆ‘ä»¬å»è§£è¯»ä¸»é¢˜ç”¨çš„ã€‚
:::

## 3. ä¸»é¢˜ç›¸å…³æ€§

ä»¥ä½™å¼¦ç›¸ä¼¼åº¦æ¥è®¡ç®—ç›¸å…³æ€§ï¼Œè¶Šæ¥è¿‘äº1è¡¨ç¤ºä¸¤ä¸ªä¸»é¢˜è¶Šç›¸å…³ï¼Œè¶Šæ¥è¿‘äº0è¡¨ç¤ºä¸¤ä¸ªä¸»é¢˜è¶Šä¸ç›¸å…³ã€‚

```{r topicCorrelation}
#è®¡ç®—STMæ¨¡å‹ä¸­æ‰€æœ‰ä¸»é¢˜ä¹‹é—´çš„ç›¸å…³æ€§
corr_stm8 <- topicCorr(stm_selected)

library(GGally)
library(network)

#è®¡ç®— STM æ¨¡å‹ä¸­ä¸»é¢˜ä¹‹é—´çš„ç›¸å…³æ€§çŸ©é˜µï¼Œå¹¶å–å…¶ç»å¯¹å€¼
net_stm8 <- corr_stm8$cor |> abs() |> as.matrix()
#å°†ç›¸å…³æ€§çŸ©é˜µä¸­çš„å€¼ä¹˜ä»¥10ï¼Œä»¥å¢åŠ å·®å¼‚çš„å¯è§†åŒ–æ•ˆæœã€‚
net_stm8 <- net_stm8 * 10 
#å°†çŸ©é˜µçš„å¯¹è§’çº¿å…ƒç´ è®¾ç½®ä¸º1ï¼Œå› ä¸ºæ¯ä¸ªä¸»é¢˜ä¸è‡ªèº«çš„ç›¸å…³æ€§æ€»æ˜¯1ã€‚
diag(net_stm8) <- 1

#åˆ›å»ºä¸€ä¸ªç½‘ç»œå¯¹è±¡ï¼Œè¡¨ç¤ºä¸»é¢˜ä¹‹é—´çš„ç›¸å…³æ€§
graph_stm8 <- network(net_stm8,
    matrix.type = "adjacency",
    ignore.eval = FALSE,
    names.eval = "weights",
    directed = FALSE
  )

#ç»˜åˆ¶ç½‘ç»œå›¾  
ggnet2(graph_stm8, label = TRUE, edge.size = "weights")
```


## åå˜é‡çš„å½±å“ï¼šå…šæ´¾

```{r covariateParty}
#| fig-height: 6.5

# æ€»ç»Ÿå°±èŒæ•°æ®è¯­æ–™åº“ï¼ˆCorpus of presidential inaugural dataï¼‰

dfmat_inaug <- tokens(data_corpus_inaugural,
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE,
                      remove_separators = TRUE) |>
  dfm() |> 
  dfm_remove(stopwords("en")) %>% 
  dfm_remove()

#æ¨¡å‹å»ºæ„
result_stm8 <- estimateEffect(formula = 1:8 ~ Party + s(Year), 
              #è¦åˆ†æçš„stmæ¨¡å‹
               stmobj = stm_selected,
              #å…ƒæ•°æ®æ¥æº
               metadata = docvars(dfmat_inaug))


#ç»˜åˆ¶å…šæ´¾ï¼ˆPartyï¼‰å¯¹ä¸»é¢˜çš„å½±å“çš„æ£®æ—å›¾
plot(result_stm8, 
     covariate = "Party",
     model = stm_selected,
     method = "difference",
     cov.value1 = "Democratic",
     cov.value2 = "Republican",
     xlim = c(-1, 1), 
     xlab = "Democratic <-> Republican")

```

## åå˜é‡çš„å½±å“ï¼šæ—¶é—´

```{r covariateTime}
#ç»˜åˆ¶ä¸»é¢˜3å’Œä¸»é¢˜7åœ¨å¹´ä»½ä¸Šçš„è¿ç»­æ•ˆåº”å›¾
plot(
  result_stm8,
  "Year",
  method = "continuous",
  model = stm_selected,
  topics = c(3, 7)
)
```


## Seed LDA

ä»¥ç§å­è¯ï¼ˆseedsï¼‰å¼•é¢†ä¸»é¢˜åˆ†ç±»ã€‚

ç¤ºä¾‹æ•°æ®ï¼šã€Šå«æŠ¥ã€‹2016

ç§å­è¯ï¼šç»æµã€æ”¿æ²»ã€ç¤¾ä¼šã€å¤–äº¤å’Œå†›äº‹ï¼ŒåŸºäºå¯¹è¯¥ç±»æ–°é—»çš„è®¤çŸ¥è·å–

```{r seeds}
# é¢„å¤„ç† ###
## è¯»å–æ•°æ®
corp_news_2016 <- corpus_subset(corp_news, year(date) == 2016)

#æ ‡è®°åŒ–ä¸æ¸…æ´—
toks_news_guardian <-
  tokens(
    corp_news_2016,
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbol = TRUE
  ) |>
  tokens_remove(pattern = c(stopwords("en"), "*-time", "updated-*", "gmt", "bst"))

#çŸ©é˜µåŒ–ä¸æ¸…æ´—
dfmat_news_guardian <- dfm(toks_news_guardian) |>
  dfm_trim(
    min_termfreq = 0.8,
    termfreq_type = "quantile",
    max_docfreq = 0.1,
    docfreq_type = "prop"
  )

## è¯»å–è¯å…¸æ•°æ®
dict_topic <- dictionary(file = system.file("extdata", "topics.yml", package = "drhurText"))
dict_topic
```

## æ•ˆæœæ¯”è¾ƒï¼šLDA

```{r seedLda, eval=FALSE}
# åˆ†æ ####

#ä½¿ç”¨ldaæ¨¡å‹
tmod_lda <- textmodel_lda(dfmat_news_guardian, k = 5)
#ä½¿ç”¨seeded ldaæ¨¡å‹
tmod_slda <- textmodel_seededlda(dfmat_news_guardian, dictionary = dict_topic)

save(tmod_lda, tmod_slda, file = here::here("slides", "courses", "governmentalBigData", "data", "text_seedLDA.rda"))
```

```{r lda-out}
load(url("https://drhuyue.site:10002/sammo3182/data/text_seedLDA.rda", open = "rb"))

#æ·»åŠ ä¸€ä¸ªæ–°çš„æ–‡æ¡£çº§åˆ«çš„å˜é‡ä»¥å­˜å‚¨seededLDA æ¨¡å‹ä¸ºæ¯ç¯‡æ–‡ç« åˆ†é…çš„ä¸»é¢˜ã€‚
dfmat_news_guardian$topic_seeded <- topics(tmod_slda) 

# æ¯”è¾ƒ

seededlda::terms(tmod_lda, 8) %>% 
  as.data.frame() %>% 
  tt()
```

## æ•ˆæœæ¯”è¾ƒï¼šSeed LDA

```{r seededlda-out}
seededlda::terms(tmod_slda, 8) %>% 
  as.data.frame() |> 
  tt()
```


:::{.notes}
è€Œæ¯”è¾ƒLDAä¸seeded LDAåšå‡ºçš„ä¸»é¢˜åˆ†ç±»ç»“æœï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å‘ç°ï¼Œåœ¨LDAåˆ†æˆçš„ä¸»é¢˜3ä¸‹é¢ï¼Œä¸ä»…æœ‰â€œclimateâ€å’Œ"water"ï¼Œè¿˜æœ‰"apple"å’Œ"education"ï¼Œå®ƒåˆ†ç±»çš„æ•ˆæœå¾ˆæ˜æ˜¾æ˜¯ä¸å¤ªå¥½çš„ã€‚
ä½†æ˜¯åœ¨seeded LDAä¸­ï¼Œæ¯ä¸ªä¸»é¢˜ä¸‹é¢çš„è¯è¯­å¤§ä½“ä¸Šç¬¦åˆæˆ‘ä»¬çš„è®¤çŸ¥ã€‚
:::


## Keyword-Assisted Topic Models [keyATM, @EshimaEtAl2023]

:::{.r-stack}
- é’ˆå¯¹æ¦‚å¿µæµ‹é‡è€Œè®¾è®¡ï¼Œè€Œéæ¢ç´¢ä¸»é¢˜
- åŸºäºå…³é”®è¯ï¼ˆç§å­è¯ï¼‰ ï¼ˆç±»ä¼¼äºseedLDAï¼‰
- å…è®¸æ²¡æœ‰å…³é”®è¯çš„ä¸»é¢˜ ï¼ˆä¸åŒäºseedLDAï¼‰
- å¯¹è¯é¢‘åŠ æƒé˜²æ­¢â€œè¯é¢‘ä¸»å¯¼â€ç°è±¡ï¼ˆç±»ä¼¼äº weightedLDAï¼‰
- å…è®¸æ–‡æ¡£å‘é‡ã€å…ƒä¿¡æ¯çš„åŠ¨æ€å˜åŒ– ï¼ˆç±»ä¼¼äºSTMï¼‰
- è´å¶æ–¯æ–¹æ³•
- å½“ç§å­è¯[è´¨é‡é«˜]{.red}æ—¶ï¼Œæ€§èƒ½ä¼˜äºåŠ æƒLDAå’ŒSTM

:::{.fragment}
```{r atm-input}
keyATM_docs <- keyATM_read(texts = dfmat_inaug)

# Keywords ####

keywords_inaug <- list(
  Government     = c("laws", "law", "executive"),
  Congress       = c("congress", "party"),
  Peace          = c("peace", "world", "freedom"),
  Constitution   = c("constitution", "rights"),
  ForeignAffairs = c("foreign", "war")
)

visualize_keywords(docs = keyATM_docs, keywords = keywords_inaug)

```
:::

:::


## Model Fit

```{r atm-fit, eval=FALSE}
# Fit ####

atm_inaug <- keyATM(
  docs = keyATM_docs,    # text input
  no_keyword_topics = 5,              # number of topics without keywords
  keywords = keywords_inaug,       # keywords
  model = "base",         # select the model
  options = list(seed = 313)
)

# Variable ####

vars_selected <- docvars(data_corpus_inaugural) |>
  dplyr::mutate(
    cent20 = ifelse(Year <= 1899, 0, 1),
    biparty = dplyr::case_when(
      Party == "Democratic" ~ "Democratic",
      Party == "Republican" ~ "Republican",
      TRUE ~ "Other"
    ) |> factor(levels = c("Other", "Democratic", "Republican"))
  )  |>
  dplyr::select(biparty, cent20)

# Model fit ####

atm_inaugX <- keyATM(
  docs = keyATM_docs,
  no_keyword_topics = 5,
  keywords = keywords_inaug,
  model = "covariates",
  model_settings = list(
    covariates_data    = vars_selected,
    covariates_formula = ~ biparty + cent20
  ),
  options = list(seed = 313)
)

# Model covariate

vars_period <- docvars(data_corpus_inaugural) |>
  mutate(period = (Year - 1780) %/% 10 + 1)

max_time_index <- max(vars_period$period)

# Model dynamic

atm_inaugD <- keyATM(
  docs = keyATM_docs,
  no_keyword_topics = 3,
  keywords = keywords_inaug,
  model = "dynamic",
  model_settings = list(
    time_index = vars_period$period,
    num_states = min(5, max_time_index)  # Ensure num_states doesn't exceed max_time_index
  ),
  options = list(seed = 313, 
                 store_theta = TRUE,
                 thinning = 5)
)

save(atm_inaug, atm_inaugX, atm_inaugD, file = here::here("slides", "courses", "governmentalBigData", "data", "text_atm.rda"))
```

```{r atm-fitness}
load(url("https://drhuyue.site:10002/sammo3182/data/text_atm.rda", open = "rb"))

# Model fit ####

plot_modelfit(atm_inaug)
```

## Base Model Result

```{r atm-result}
# Interpret ####

plot_topicprop(atm_inaug, show_topic = 1:10)
```

## Covariate Effect

```{r atmCov}
# Interpretation ####

## Binary
strata_period <- by_strata_DocTopic(atm_inaugX,
                                    by_var = "cent20",
                                    labels = c("18_19c", "20_21c"))

### Together view
plot(strata_period, var_name = "cent20", by = "covariate")
```

## Topic Dynamics

```{r atmDyn}
plot_timetrend(atm_inaugD, 
               time_index_label = docvars(data_corpus_inaugural)$Year, 
               xlab = "Year", 
               ci = 0.95)
```


## å°ç»“

:::{style="text-align:center; margin-top: 1em"}
:::{.fragment .semi-fade-out}
- Weighing &larr; ä»[è¯é¢‘]{.red}æ”«å–ä¿¡æ¯
- N-gram &larr; ä»[é‚»å±…]{.red}æ”«å–ä¿¡æ¯
- Collocation &larr; ä»[å…±ç°]{.red}æ”«å–ä¿¡æ¯
- Keyness &larr; ä»[å…³é”®è¯å®šä½]{.red}æ”«å–ä¿¡æ¯
- Functional words &larr; ä»[ç¤¾ä¼šå¿ƒç†]{.red}æ”«å–ä¿¡æ¯
:::


:::{.fragment}
- STM &larr; çº³å…¥æ¦‚å¿µ[å…³ç³»]{.red}ä¸[å¤–éƒ¨]{.red}ä¿¡æ¯
- SeedLDA &larr; çº³å…¥[èƒŒæ™¯]{.red}çŸ¥è¯†
- keyATM &larr; çº³å…¥ç ”ç©¶[æ„å›¾]{.red}
:::

:::


# è¯­ä¹‰å±‚çº§ä¿¡æ¯æ±‡å…¥

:::{.notes}
- è¯­æ³•ï¼šGrammar
- è¯­ä¹‰: Semantic
- è¯­ç”¨: Pragmatic
:::

## ç»™è¯ä¹‰å»ºæ¨¡ï¼šè¯åµŒå…¥(Word embedding)

> Words' meanings depend not just on immediate neighbors

![](https://drhuyue.site:10002/sammo3182/figure/theory_wordEmbedding.png){fig-align="center" height=550}

:::{.notes}
The term "embedding" comes from the neural network literature, in which an "embedding layer" is an input function that efficiently compresses high-dimensional data down to a low-dimensional dense representation for input to subsequent neural network layers.

- The embedding model GloVe ("Global Vectors") by @PenningtonEtAl2014 is explicitly designed to construct word vectors encoding local co-occurrence.
- An equally influential word embedding model is Word2Vec [@BengioEtAl2000], which treats each instance of a word and its context as a separate prediction problem that word vectors are chosen to solve.

LSA, NMF, and LDA can also be viewed as producing word embeddings. In particular, the (V Ã— K) matrix B from (2) contains a series of row vectors corresponding to each term in the vocabulary (see also Levy and Goldberg 2014). Those vectors contain information about word co-occurrence at the document level, rather than within a local context.
:::

## èƒ½åšä»€ä¹ˆ

- çº³å…¥è¯è¯­æ„ä¹‰çš„è¡¨è¾¾ 
    - å“ªä¸ªæœ€æ¥è¿‘`paris - france + germany`ï¼ˆæŸæ—ï¼‰ï¼Ÿ
    - å“ªä¸ªæœ€æ¥è¿‘`berlin - germany + uk + england`ï¼ˆä¼¦æ•¦ï¼‰ï¼Ÿ

:::{.fragment}
- ä¸åˆ†ç±»å’Œèšç±»ç›¸æ¯”: Document-feature matrix &rarr; context-feature matrix (FCM)
    - åˆ†ç±»ï¼šä¸å…³æ³¨å•ä¸ªè¯è¯­é—´çš„å…³ç³»ï¼Œè€Œæ˜¯å…³æ³¨è¯è¯­åœ¨æ–‡æ¡£ä¸­çš„åˆ†å¸ƒã€‚
    - èšç±»ï¼šæä¾›å¯¹æ–‡æ¡£é›†åˆå†…å®¹çš„æ›´é«˜çº§ï¼Œè€Œä¸æ˜¯ä¸“æ³¨äºå•ä¸ªè¯è¯­çš„å«ä¹‰ã€‚
    
:::{.notes}
**context-feature matrix (FCM)** æ˜¯`quantada`ä¸­å¦ä¸€ä¸ªé‡è¦çš„çŸ©é˜µå½¢å¼ï¼Œå®ƒç”¨æ¥æµ‹é‡åœ¨ç”¨æˆ·å®šä¹‰çš„ä¸Šä¸‹æ–‡è¯­å¢ƒä¸­ï¼Œä¸€äº›ç‰¹å¾çš„å…±ç°æ€§ï¼Œä¹Ÿå°±æ˜¯åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æ•æ‰è¯æ±‡ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚
å…·ä½“æ¥è¯´ï¼ŒFCM ä½¿ç”¨äº†ä¸€ä¸ªä¸Šä¸‹æ–‡çª—å£ï¼Œå¯¹æ¯ä¸ªè¯æ±‡å»ºæ¨¡æ—¶è€ƒè™‘å…¶å‘¨å›´çš„ä¸Šä¸‹æ–‡ã€‚
è¿™ä½¿å¾— FCM èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è¯æ±‡ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå› ä¸ºå®ƒä¸ä»…è€ƒè™‘äº†è¯æ±‡æœ¬èº«çš„ä¿¡æ¯ï¼Œè¿˜è€ƒè™‘äº†å®ƒä»¬åœ¨è¯­å¢ƒä¸­çš„ä½¿ç”¨æƒ…å†µã€‚
:::
:::

:::{.fragment}
- åº”ç”¨
    - è¯æ¡è¡¨å¾ï¼ˆterm representationï¼‰
    - æƒ…æ„Ÿåˆ†æï¼ˆsentiment analysisï¼‰
:::


## åº”ç”¨ä¸¾ä¾‹ï¼šLatent Semantic Scaling [@Watanabe2021]

- ä¸“é—¨ç”¨äºåŒºåˆ†[å¯¹ç«‹çš„ç«‹åœº]{.red}
- åŸºäºè¯åµŒå…¥æŠ€æœ¯çš„ï¼Œåœ¨æ„å»ºæ¨¡å‹ä¹‹å‰å°†æ–‡æ¡£å’Œç‰¹å¾è½¬æ¢ä¸ºé«˜ç»´åº¦å‘é‡ç©ºé—´
    - GloVe
    - Singular Value Decomposition (SVD)

![](https://drhuyue.site:10002/sammo3182/figure/text_svd.png){.fragment fig-align="center" height=400}

:::{.notes}
SVD is a math tool that helps us break down a big matrix (which you can think of like a giant table of numbers) into smaller parts. 
SVD, a matrix is decomposed into three other matrices: U, Î£, and V . The columns of U and V are called left and right singular vectors, respectively.

Comparison in Semantic Scaling:

- SVD: Better for general-purpose dimensionality reduction and identifying broad patterns in text. It's more of a tool for finding overall structure and relationships, but it may miss finer semantic distinctions.
- GloVe: Superior for scaling semantics in a more nuanced way, capturing both local (nearby words) and global (overall context) word meanings. This makes GloVe more effective for specific tasks like word similarity and analogy.
:::

## åº”ç”¨ï¼šåˆ¤æ–­ã€Šå«æŠ¥ã€‹æ–°é—»çš„æƒ…æ„Ÿèµ°å‘

```{r lss, eval=FALSE}
# Preprocessing ####
# tokenize text corpus and remove various features
corp_sent <- corpus_reshape(corp_news, to =  "sentences")
toks_sent <- corp_sent |>
  tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_url = TRUE
  ) |>
  tokens_remove(stopwords("en", source = "marimo")) |>
  tokens_remove(c("*-time", "*-timeUpdated", "GMT", "BST", "*.com"))  



# create a document feature matrix from the tokens object
dfmat_sent <- toks_sent |> 
    dfm() |> 
    dfm_remove(pattern = "") |> 
    dfm_trim(min_termfreq = 5)

# Seed words ####

seed <- as.seedwords(data_dictionary_sentiment)


# Analyze ####

# identify context words
eco <- char_context(toks_sent, pattern = "econom*", p = 0.05)

# run LSS model (SVD)
tmod_lss_svd <- textmodel_lss(
  dfmat_sent,
  seeds = seed,
  terms = eco,
  k = 300,
  # the number of singular values requested to the SVD engine
  group_data = TRUE,
  # apply `dfm_group()` to `x` to group the sentences into the original documents, effectively reversing the segmentation by `corpus_reshape()`
  include_data = TRUE # save a grouped DFM in the LSS object as `lss$data`
)

fcm_sent <-fcm(dfmat_sent, tri = TRUE)

tmod_lss_glove <- textmodel_lss(
  fcm_sent,
  seeds = seed,
  terms = eco,
  engine = "rsparse" # using gloVe
)

save(tmod_lss_svd, tmod_lss_glove, dfmat_sent, file = here::here("slides", "courses", "governmentalBigData", "data", "text_embedding.rda"))

```

:::{.r-stack}
```{r lss-result-svd}
load(url("https://drhuyue.site:10002/sammo3182/data/text_embedding.rda", open = "rb"))

textplot_terms(tmod_lss_svd, highlighted = data_dictionary_LSD2015["negative"]) # many of the words (but not all of them) have negative meanings in the corpus.
```

:::{.fragment}
```{r lss-result-glove}
textplot_terms(tmod_lss_glove, highlighted = data_dictionary_LSD2015["negative"])
```
:::

:::

:::{.notes}
é«˜äº®çš„ä¸ºè¯å…¸ä¸­çš„è¯æ±‡, æ³¨æ„SVDç»“æœï¼Œæœ‰ä¸€äº›è¯è½åœ¨äº†0å³é¢ï¼Œè€Œgloveéƒ½è½åœ¨äº†0å·¦è¾¹
:::

## è¶‹åŠ¿åˆ†æ

```{r glovePrediction}
# Prediction ####

dat_svd <- docvars(tmod_lss_svd$data)
dat_svd$lss <- predict(tmod_lss_svd)

dfmat_doc <- dfm_group(dfmat_sent) # current glove version does not have the include_data argument
dat_glove <- docvars(dfmat_doc) 
dat_glove$lss <- predict(tmod_lss_glove, newdata = dfmat_doc)


# Draw the polarity score
smo_svd <- smooth_lss(dat_svd, lss_var = "lss", data_var = "date")

plot_svd <- ggplot(smo_svd, aes(x = date, y = fit)) + 
    geom_line() +
    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +
    labs(title = "Sentiment in Guardian (SVD)", x = "Year", y = "Sentiment")

smo_glove <- smooth_lss(dat_glove, lss_var = "lss", data_var = "date")

plot_glove <- ggplot(smo_glove, aes(x = date, y = fit)) + 
    geom_line() +
    geom_ribbon(aes(ymin = fit - se.fit * 1.96, ymax = fit + se.fit * 1.96), alpha = 0.1) +
    labs(title = "Sentiment in Guardian (GloVe)", x = "Year", y = "Sentiment")
    
plot_svd / plot_glove
```

:::{.notes}
2016å¹´ä»¥åæ³¢åŠ¨æ¯”è¾ƒå¤§
:::


## è¿˜ç¼ºä»€ä¹ˆ

> Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy [@IBM2021].

![](https://drhuyue.site:10002/sammo3182/figure/text_machineLearning.jpg){fig-align="center" height=400}

:::{.notes}
- è‡ªåŠ¨æ€§ï¼šself-supervised learning

- åŠ å¼º: reinforcement
:::

## è‡ªç›‘ç£å­¦ä¹ 

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/text_selfSupervised.png){fig-align="center" height=400}

![](https://drhuyue.site:10002/sammo3182/figure/text_pretrain.webp){.fragment fig-align="center" height=600}
:::


:::{.notes}
Self-Supervised Learning: In self-supervised learning, the model generates its own labels from the input data, creating a supervised-like setup. For instance, a model might be given part of a sentence and asked to predict the missing word. The labels (the missing word) come from the data itself. This is often used in tasks like language modeling (e.g., predicting the next word in a sentence).
:::

## å¼ºåŒ–å­¦ä¹ 

![](https://drhuyue.site:10002/sammo3182/figure/text_reinforcement.png){fig-align="center" height=600}


## æ³¨æ„åŠ›æœºåˆ¶ [Attention Mechanism, @VaswaniEtAl2017]

ä¸€èˆ¬çš„word embeddingè®¤ä¸ºæ‰€æœ‰è¯å’Œè¯ä¹‹é—´å…³ç³»éƒ½åŒç­‰é‡è¦[ğŸ¤¦â€â™‚ï¸]{.large}

:::{.fragment .fade-in-then-semi-out}
"Attention is all you need" [@VaswaniEtAl2017]

ä»¥ä¸‹æ˜¯å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ³¨æ„åŠ›æœºåˆ¶çš„ç¤ºä¾‹ç¿»è¯‘ï¼š

> ä½œä¸ºåœ¨_____é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„è½¯ä»¶å·¥ç¨‹å¸ˆã€‚    
> ä½œä¸ºåœ¨_____é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„å¤ªé˜³èƒ½å·¥ç¨‹å¸ˆã€‚

åº”è¯¥åœ¨ "_____" å¡«å…¥ä»€ä¹ˆè¯ï¼Ÿä½ æ˜¯å¦‚ä½•å¾—å‡ºè¿™ä¸ªç»“è®ºçš„ï¼Ÿ

:::


:::{.fragment}
ä½œä¸ºåœ¨*ä¿¡æ¯æŠ€æœ¯*é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„[è½¯ä»¶]{.red}å·¥ç¨‹å¸ˆã€‚    
ä½œä¸ºåœ¨*ç»¿è‰²èƒ½æº*é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„[å¤ªé˜³èƒ½]{.red}å·¥ç¨‹å¸ˆã€‚

:::

:::{.notes}
Self-Attention in Transformers:
Self-Attention: A specific type of attention called self-attention is key to Transformers. In self-attention, every word in a sentence can pay attention to every other word, not just the nearby words. This allows the model to capture complex relationships, even when words are far apart in the sentence.

For instance, in the sentence "The cat that was sitting on the mat is happy," self-attention allows the model to recognize that "cat" is related to "is happy," even though several words separate them.

Multi-Headed Attention: Transformers use multiple attention heads to learn different aspects of relationships between words. Each attention head focuses on a different aspect of the word relationships, which helps the model understand the text in a richer and more nuanced way.

3. During Pre-Training and Fine-Tuning:
Pre-Training Phase (Self-Supervised Learning): The attention mechanism helps the model learn language representations by focusing on relevant parts of the input when predicting the next word or filling in gaps in sentences.

Fine-Tuning Phase (Reinforcement Learning and Beyond): The attention mechanism continues to play a role when the model is fine-tuned using reinforcement learning, such as RLHF. The model uses the attention mechanism to generate responses during interactions, determining which parts of the input prompt to focus on when creating its output.
:::


:::{.fragment .callout-note}
## è‡ªæ³¨æ„åŠ›æœºåˆ¶

è¾“å…¥ä¸€ä¸ªåˆå§‹è¯å…ƒåµŒå…¥åºåˆ—ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ–°çš„è¯å…ƒåµŒå…¥åºåˆ—ï¼Œ[ä½¿åˆå§‹åµŒå…¥èƒ½å¤Ÿç›¸äº’ä½œç”¨]{.red}ã€‚
:::

## ç»„è£…èµ·æ¥

- ç”±å †å çš„*æ³¨æ„åŠ›æœºåˆ¶*å’Œ*å‰é¦ˆç¥ç»ç½‘ç»œå±‚*ç»„æˆçš„å¤§å‹ç¥ç»ç½‘ç»œå¯ä»¥ä½¿ç”¨ä¸“ç”¨å¤„ç†å™¨é«˜æ•ˆå¹¶è¡Œè®­ç»ƒï¼Œå³ **t**ransformerï¼Œé€šå¸¸é€šè¿‡é¢„è®­ç»ƒï¼ˆ**p**re-training)è·å¾—ã€‚
    - &rarr; åº”ç”¨äºåŸºäºå­¦ä¹ æˆæœçš„ç”Ÿæˆå¼ï¼ˆ**g**enerativeï¼‰ä»»åŠ¡


:::{.notes}
- BERT ä½¿ç”¨transformerï¼ˆencoderï¼‰æ¥è¿›è¡Œdiscriminative jobsï¼Œe.g, classification or predicting missing words in sentences
- GPT ä½¿ç”¨transformer (decoder)æ¥è¿›è¡Œgenerative jobs, e.g., generating sentences
:::

:::{.fragment}
- å¼ºåŒ–å­¦ä¹ /å¾®è°ƒè¿‡ç¨‹

![](https://drhuyue.site:10002/sammo3182/figure/text_finetune.webp){fig-align="center" height=400}
:::


:::{.notes}
Fine-Tuning: After pretraining, the model undergoes fine-tuning on more specific data, often with human feedback or additional tasks. This process helps the model adjust its knowledge to be more useful for specific applications. For instance, fine-tuning might include adjusting the model's responses to be more helpful, accurate, or aligned with certain safety guidelines. This phase often involves techniques like Reinforcement Learning from Human Feedback (RLHF), where the model learns from human-provided rankings of response quality.

Interaction with ChatGPT: not fine tune

Interaction with ChatGPT (Ask-Answer Rounds):
When you engage in ask-answer rounds with ChatGPT, the model uses the knowledge it gained during training and fine-tuning to respond to your queries. However:

- No Learning Happens: The model is not updating its parameters or "learning" from your individual interactions. The model's behavior is static, and its responses are generated based on the preexisting knowledge and patterns it has learned.
- Inference: The process youâ€™re engaging in is called inference, where the model is simply generating responses based on what it already knows.


The reason you often get better answers after interacting with ChatGPT is not because the model is learning from your specific questions in real-time (since it doesn't update itself during these interactions). Instead, it's due to the way the model processes context and maintains a conversation. Here's why:

1. **Context Awareness**:
ChatGPT can remember the context of the ongoing conversation within the session. As the interaction progresses, the model uses previous information from the conversation to better understand your current question. This allows it to generate more accurate, relevant, and coherent responses.

For example:
- If you ask a series of related questions, ChatGPT can "build on" the information exchanged earlier in the conversation, leading to more refined answers.
- It can keep track of clarifications youâ€™ve provided and adapt its responses to align with your preferences or focus on specific details youâ€™re interested in.

2. **Clarification and Refinement**:
In a conversation, you often provide more details, clarifications, or corrections as you interact with the model. This helps ChatGPT refine its understanding of your needs, enabling it to give more relevant answers.

For instance:
- If an initial response is too vague or off-target, you might ask follow-up questions or provide additional information. With this new input, ChatGPT can refine its responses to better match what you're looking for.

3. **Pattern Matching**:
ChatGPT has been trained on a vast amount of conversational data, so itâ€™s very good at recognizing patterns in dialogue. As the conversation continues, the model can "hone in" on the patterns that seem most relevant to your questions based on prior exchanges. This can give the appearance of improvement in the quality of responses.

4. **Elaboration and Deeper Explanation**:
With more interaction, you might be guiding ChatGPT to provide deeper or more nuanced explanations. The initial responses might be general, but as you ask for further detail or clarification, the model responds with more in-depth information, which can feel like an improvement.

5. **Conversational Flow**:
ChatGPT is designed to simulate a conversational flow, so as you continue to interact, the model attempts to align more closely with your communication style, the specific topic, and your expressed needs. This ongoing refinement can feel like it's "learning," but itâ€™s really just adapting to the conversation in progress.

Conclusion:
The improvement in the quality of answers you receive during the conversation is due to **contextual understanding** and **clarification through interaction**, rather than real-time learning or fine-tuning. The model is designed to keep track of the dialogue and adjust its responses based on the context of your ongoing discussion.
:::


## æ€»ç»“

> æœ¬è®²çš„æ ¸å¿ƒè®®é¢˜ï¼š[çªç ´è¯åŸºé™åˆ¶ï¼Œæ‰¾å›æœ‰ç”¨ä¿¡æ¯]{.red}

:::: {.columns}

::: {.column width="50%"}
**è¯æ±‡å±‚çº§çš„ä¿¡æ¯æ±‡å…¥**

:::{.fragment}
- Weighing &larr; ä»[è¯é¢‘]{.red}æ”«å–ä¿¡æ¯
- N-gram & Collocation &larr; ä»[é‚»å±…/å…±ç°]{.red}æ”«å–ä¿¡æ¯
- Functional words &larr; ä»[å¿ƒç†]{.red}æ”«å–ä¿¡æ¯
:::


**æ¦‚å¿µå±‚çº§çš„ä¿¡æ¯æ±‡å…¥**

:::{.fragment}
- STM &larr; çº³å…¥æ¦‚å¿µ[å…³ç³»]{.red}ä¸[å¤–éƒ¨]{.red}ä¿¡æ¯
- SeedLDA &larr; çº³å…¥[èƒŒæ™¯]{.red}çŸ¥è¯†
- keyATM &larr; çº³å…¥ç ”ç©¶[æ„å›¾]{.red}
:::

:::

::: {.column width="50%"}
**è¯­ä¹‰å±‚çº§çš„ä¿¡æ¯æ±‡å…¥**

:::{.fragment}
- Word embedding &larr; è¯æ±‡ä¹‹é—´çš„[è¯­ä¹‰]{.red}å…³è”
- Self-supervised learning & Attention model &larr; [è‡ªåŠ¨]{.red}åŒ–å’Œè‡ªæˆ‘[æå‡]{.red}
- Reinforcement learning &larr; å…·ä½“ä»»åŠ¡[æƒ…å¢ƒ]{.red}
:::

:::

::::



# æ„Ÿè°¢å€¾å¬ï¼Œæ¬¢è¿äº¤æµ {background="#43464B"}

:::{style="text-align: right; margin-top: 1em"}  

[`r feather_icons("github")`&nbsp; sammo3182](https://github.com/sammo3182)

[`r feather_icons("mail")`&nbsp; yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

[`r feather_icons("globe")`&nbsp; https://www.drhuyue.site](https://www.drhuyue.site)

![](https://user-images.githubusercontent.com/6463211/232207708-b0e64eee-7fb3-45a4-9779-ec52397f786c.png){height=250}
:::

## å‚è€ƒæ–‡çŒ®

::: {#refs}
:::
