---
title: "ä»¥æ•°çŸ¥æ–‡"
subtitle: "æ·±åœ³å¤§å­¦Â·å½“ä»£ä¸­å›½æ”¿æ²»å‘å±•è®ºå›"
author: "èƒ¡æ‚¦"
institute: 
  - æ¸…åå¤§å­¦ç¤¾ä¼šç§‘å­¦å­¦é™¢
  - æ¸…åå¤§å­¦è®¡ç®—ç¤¾ä¼šç§‘å­¦ä¸å›½å®¶æ²»ç†å®éªŒå®¤
  - ï¼ˆè®¡ç®—ç¤¾ä¼šç§‘å­¦å¹³å°ï¼‰
  - æ¸…åå¤§å­¦æ•°æ®æ²»ç†ä¸­å¿ƒ
bibliography: ../camsTextAnalysis/pre_cams.bib
knitr: 
    opts_chunk:
      echo: false
format: 
  revealjs:
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: false
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: false # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
    default-image-extension: png
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  drhutools, 
  icons,
  flextable, 
  gridExtra,
  knitr, # dependency
  rvest, 
  stringr, 
  broom, jiebaR, 
  tidytext, 
  tidyverse
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```

## ä¸ªäººç®€ä»‹{.Small}

:::: {.columns}

::: {.column width="60%"}
*ä¸ªäººç»å†*

- æ”¿æ²»å­¦åšå£«[ï¼ˆUniversity of Iowa)]{.small}
  - ä¿¡æ¯å­¦[ï¼ˆGraduated Certificate in Informatics)]{.small}
- æ¸…åå¤§å­¦è®¡ç®—ç¤¾ä¼šç§‘å­¦ä¸å›½å®¶æ²»ç†å®éªŒå®¤    
è®¡ç®—ç¤¾ä¼šç§‘å­¦å¹³å°[(å‰¯ä¸»ä»»)]{.small}
  - æ¸…åæ•°æ®ä¸æ²»ç†ä¸­å¿ƒ[(å‰¯ä¸»ä»»)]{.small}
  - è®¡ç®—ç¤¾ä¼šç§‘å­¦ç¼–ç¨‹è¯­è¨€è¯ä¹¦é¡¹ç›®[ï¼ˆè´Ÿè´£äººï¼‰]{.small}
  - Learning R with Dr. Hu & Friends å·¥ä½œåŠ[ï¼ˆåˆ›å§‹äººï¼‰]{.small}

:::{.fragment}
*ç ”ç©¶å…´è¶£ï¼šè®¤çŸ¥ã€è¡Œä¸ºä¸ç°ä»£æ€§*

- **æ–¹æ³•è·¯å¾„ï¼šè®¡ç®—æ”¿æ²»å­¦**
  - å®éªŒå®¤å’Œè°ƒæŸ¥å®éªŒ
  - æ½œå˜é‡åˆ†æã€ç½‘ç»œåˆ†æã€ç©ºé—´åˆ†æ
  - æ–‡æœ¬å¤§æ•°æ®åˆ†æã€æ•°æ®å¯è§†åŒ–
:::

:::

::: {.column .fragment width="40%"}
*ç ”ç©¶é¢†åŸŸï¼šæ¯”è¾ƒæ”¿æ²»ã€å›½å®¶æ²»ç†*

- **W. å¿ƒç†å­¦**
  - æ”¿æ²»è®¤çŸ¥[æ²»ç†]{.red}
  - è¡Œä¸ºå…¬å…±[æ”¿ç­–]{.red}
  - æ”¿æ²»ä¼ æ’­
  - èº«ä»½æ”¿æ²»

- **W. ç»æµå­¦**
  - ç»æµä¸å¹³ç­‰[æ„ŸçŸ¥]{.red}
  - å…¬å…±è®¾æ–½ã€æœåŠ¡å‡ç­‰åŒ–

- **W. è¯­è¨€å­¦**
  - æƒåŠ›èƒŒä¹¦çš„[è¯­è¨€æ•ˆæœ]{.red}ä¸æœºåˆ¶
  - è¯­è¨€æ”¿ç­–çš„æ²»ç†åŠŸèƒ½

:::

::::

## æè¦

> @King2015: [The big-data approach is] the [end]{.red} of the quantitative-qualitative divide.

:::{.notes}
King talked about this issue in many places including Shanghai Jiaotong University
:::

:::: {.columns}

::: {.column .fragment width="30%"}
### è®¤çŸ¥

[è§å­—å¦‚æ•°]{.red}ï¼š    
æ–‡æœ¬ä¸æ–‡æœ¬æ•°æ®

- æ–‡æœ¬&rarr;æ•°æ®
- æ–‡æœ¬åˆ†æ
- åº”ç”¨æ¡ˆä¾‹
:::

::: {.column .fragment width="30%"}
### åŸåˆ™

[ä»¥æ•°çŸ¥æ–‡]{.red}:     
æ–‡æœ¬æ•°æ®åŒ–åŸç†è§„èŒƒ

* æ•°æ®è·å–
* æ•°æ®æ•´ç†
* åŸºæœ¬åˆ†æ

:::

::: {.column .fragment width="40%"}
### æ“ä½œ

[æœ›æ•°ç”Ÿä¹‰]{.red}:     
æ–‡æœ¬æ•°æ®åˆ†æè¿‡ç¨‹ç¤ºèŒƒï¼ˆRï¼‰

- ç½‘ç»œä¿¡æ¯çˆ¬å–ï¼ˆ[`rvest`](https://rvest.tidyverse.org/)ï¼‰
- ä¸­æ–‡å¤„ç†ï¼ˆ[`jiebaR`](https://qinwenfeng.com/jiebaR/)/[`TopWORDS`](http://www.stat.tsinghua.edu.cn/kdeng/r-package/)ï¼‰
- æ–‡æœ¬å…³ç³»åˆ†æï¼ˆ[`tidytext`](https://www.tidytextmining.com/)ï¼‰

:::

::::



# è§å­—å¦‚æ•°ï¼šå»ºç«‹è®¤çŸ¥

## æ–‡æœ¬æ•°æ®

[æ¯å¹´ï¼š 2024å¹´åˆç¤¾äº¤åª’ä½“ç”¨æˆ·50.4äº¿ï¼Œ2023å¹´æ–°å¢2.66äº¿é¦–æ¬¡ä½¿ç”¨ç”¨æˆ· [@Kemp2024]]{.fragment}

:::{.fragment}
æ¯æ—¥ï¼š

+ ç™¾åº¦æ—¥ç”¨æˆ·æœç´¢è¯·æ±‚ï¼Œéœ€[1.7å¤©]{.red}æ‰èƒ½æ‰«æä¸€éï¼›
+ å¾®ä¿¡æ—¥å¢æ•°æ®[500TB]{.red}â€”â€”æ¯”äººç±»æ‰€æœ‰ä¹¦ç±å­˜é‡è¿˜å¤šã€‚
:::

[æ¯ç§’ï¼šå…¨ä¸–ç•Œæ¯ç§’å‘é€290ä¸‡å°emailï¼Œä¸€äººéœ€è¦[5.5å¹´]{.red}æ—¥ä»¥ç»§å¤œæ‰èƒ½è¯»å®Œã€‚]{.fragment}

:::{.fragment .callout-tip}
2023å¹´ï¼Œå…¨å›½æ•°æ®ç”Ÿäº§æ€»é‡è¾¾32.85ZB,ï¼ŒåŒæ¯”å¢é•¿22.44%ï¼Œå³äººå‡ `r round(32.85 * (1024^3) / (1.6 * 10^9))` TB [@YanZhiHongYanFuJing2024]^[1ZB = 1,024EB = 1,024<sup>2</sup>PB = 1,024<sup>3</sup>TB  = 1024<sup>4</sup>GB; 1080p HD 2hrsï¼š6GB.]


é¢„è®¡2027å¹´ï¼Œå…¨çƒ[éç»“æ„åŒ–æ•°æ®]{.red}å°†å åˆ°æ•°æ®æ€»é‡çš„86.8% [@IDCFutureScape2024]
:::


:::{.notes}
Giga Byte < Tera Byte < Peta Byte < Exa Byte < Zetta Byte < Yotta Byte
:::


## æ–‡æœ¬ç ”ç©¶

å†å²æ‚ ä¹…è€Œéä¸»æµ &larr; èµ„æ–™éš¾è·å–ï¼›èŠ±æ—¶é—´ï¼›éš¾æ¨å¹¿ï¼›éš¾ç®¡ç†ï¼›éš¾åˆ†æ

- æ³•å¾‹çš„å¤–éƒ¨æ€§é—®é¢˜ [@Coase1960]
- é€šè¿‡å†å²æ–‡ä»¶ç ”ç©¶â€œpolicy surprisesâ€ [@FriedmanSchwartz2008]

:::{.fragment style="text-align:center; margin-top: 2em"}
æ–°å…´å·¥å…·çš„ç¹è£ï¼š

+ æ–‡æœ¬[èµ„æ–™]{.red}æŒ‡æ•°çº§å¢é•¿ï¼›
+ å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®[é‡‡é›†]{.red}ï¼›
+ å­˜å‚¨å’Œç®¡ç†[èƒ½åŠ›]{.red}å¢å¼ºï¼›
+ å¯æ¨å¹¿ã€ç³»ç»ŸåŒ–å’Œ[å»‰ä»·]{.red}åŒ–ï¼›
+ æ–‡æœ¬åˆ†æ[æ–¹æ³•]{.red}è“¬å‹ƒå‘å±•
:::



## (è®¡ç®—æœºè¾…åŠ©ï¼‰æ–‡æœ¬åˆ†æ

:::: {.columns}

::: {.column width="50%"}
**å¯¹è±¡**

~~æ–‡å­—~~ è¯­è¨€
:::

::: {.column width="50%"}
**ç±»å‹**

* Text analysis vs. content analysis
* Representational analysis vs. Instrumental analysis
* Thematic analysis vs. semantic analysis

:::{.notes}
Representational ç²¾ç¡®è§£ç ï¼Œå…³æ³¨å¤–æ˜¾ï¼ˆmanifestï¼‰ï¼›
Instrumental æ¢ç´¢æ„å›¾ï¼Œå…³æ³¨éšå«ï¼ˆlatentï¼‰
Thematic æ¦‚å¿µæ˜¯å¦å‡ºç°åŠä½•ç§å…³ç³»ï¼ŒåŸºäºè¯é¢‘å’Œå‘é‡ï¼›
Semantic è¯†åˆ«ä¸»é¢˜é—´çš„å…·ä½“å…³ç³»ï¼›è€ƒè™‘è¯­æ³•ã€é€»è¾‘ç­‰

è¯­æ³•å­¦(syntax, how to say it)
è¯­ä¹‰å­¦(semantic, what is said)
è¯­ç”¨å­¦(pragmatic, what is implicated)
:::

:::

::::


## ğŸŒ° I

@Grimmer2010

- Objective
    - The priorities political actors emphasize in statements
- Data
    - An original collection of over 24,000 Senate press releases in 2007
- Method
    - Bayesian Hierarchical Topic Model

## å‘ç°

::: {.panel-tabset}
### Focus

![Committee leaders focus on their committee's issues](https://drhuyue.site:10002/sammo3182/figure/theory_grimmer1.png){fig-align="center" height=500}

:::{.notes}
Fig. 4 Chairman and ranking members of committees allocate more attention to issues under their committees' jurisdiction than other senators. **This figure compares the attention that Senate committee leaders---chairs or ranking members---dedicate to topics under their committee jurisdictions to the attention allocated by the rest of the Senate.** The solid dots represent the expected difference, the thick lines are 80% credible intervals, and the thin lines are 95% intervals. Along the left-hand vertical axis, the topics are listed, and on the right-hand side, the corresponding committee names are listed. In all but seven cases, the dot is to the right of the zero line, indicating that leaders of committees discuss issues that highlight their power in the institution more often than other senators.
:::


### Cluster

![Expressed agendas cluster geographically](https://drhuyue.site:10002/sammo3182/figure/theory_grimmer2.png){fig-align="center" height=500}


:::{.notes}
Fig. 5 Attention to issues follows expected geographic patterns. **This figure demonstrates that senators' expressed agendas are grouped geographically.** The left-hand plot shows that senators from western states allocate substantial attention to public-land issues. Darker shades indicate that the average expected attention from the state's delegation to public-land issues is larger. The center plot carries out a comparison of three different regional issues: public-land and western states (top estimate), hurricanes and gulf coast states (middle estimate), and border-security and states that share an international border (bottom estimate). The point in each plot represents the expected difference between the attention to senators in a geographic area allocate to an issue and the attention senators from other areas of the country dedicate to the same issue. The thick and thin lines are 80% and 95% HPD intervals for this difference. Each point is to the right of the zero, indicating that the issues receive more attention in the geographic areas we would expect. The right-hand plot shows that senators from states with a large number of farms per person also tend to allocate more attention to agriculture issues. The horizontal axis represents the number of farms per resident of the state (one measure of agriculture's importance to a state), and the vertical axis indicates the proportion of press releases allocated to agricultural issues. The gray lines are lowess curves indicates the relationship between the number of farms per capita and the attention to agriculture, whereas the black line is the average relationship.
:::

### Predict

![Attention to appropriations predicts opposition to earmark reform](https://drhuyue.site:10002/sammo3182/figure/theory_grimmer3.png){fig-align="center" height=500}

:::{.notes}
An earmark is a provision inserted into a discretionary spending appropriations bill that directs funds to a specific recipient while bypassing the merit-based or competitive funds allocation process. These provisions are often associated with legislation specifying certain congressional spending priorities or in revenue bills that apply to a limited number of individuals or entities. 

The DeMint-McCain amendment was a significant proposal related to earmark reform. Senator Jim DeMint introduced a one-year earmark moratorium amendment, which was co-sponsored by both Republicans and Democrats. The amendment aimed to end the era of earmarks and was seen as a step towards increased transparency, accountability, and fiscal responsibility in Congress. Despite its support from some senators and advocacy groups, the amendment was defeated in a late-night Senate vote, with a result of 29-71.

Fig. 6 Senators who dedicate more attention to appropriations were more likely to oppose Demint-McCain. This figure shows that **senators who dedicate more attention to appropriations in their press releases are more likely to oppose the Demint-McCain amendment.** The vertical axis plots the vote on the amendment, and along the horizontal axis is the average proportion of press releases dedicate to discussing appropriations secured for fire departments. To generate the light gray lines, I took 100 draws from each senator's posterior expressed agenda and then regressed the earmark vote on the draw from the posterior. The gray lines represent the expected probability of supporting the DemintMcCain amendment, and the solid black line is the expected value of the relationship, averaged over the draws from the posterior distribution on the expressed agenda. 

The left-hand figure shows that senators who discuss fire department grants more often were more likely to oppose the DemintMcCain amendment, and the center plot shows that this relationship was even stronger for an aggregate appropriations category. The right-hand plot shows that the relationship remains even after conditioning upon estimated ideal points of senators, suggesting that consistency explains components of voting behavior beyond ideal point estimates.
:::
:::

## ğŸŒ° II

@BenoitEtAl2016

- Objective
    - Professionals vs. Masses
- Method
    - Crowd-sourced identification
- Data
    - 18,263 natural sentences from *British Conservative, Labour and Liberal Democrat* manifestos
    

## æ“ä½œ

::: {.panel-tabset}
### ç¼–ç 

![](https://drhuyue.site:10002/sammo3182/figure/theory_crowd-sourced.jpg){fig-align="center" height=500}

:::{.notes}
Figure 1: Hierarchical coding scheme for two policy domains with ordinal positioning.
:::

### ç»“æœ

![](https://drhuyue.site:10002/sammo3182/figure/theory_expert-crowd.png){fig-align="center" height=500}

:::{.notes}
Figure 2. British party positions on economic and social policy 1987 â€“ 2010; sequential expert text processing (vertical axis) and independent expert surveys (horizontal).
:::

:::



## ğŸŒ° III

@DietrichEtAl2019

- Objective
    - Speakers' emotional state
- Method
    - Analyses of vocal pitch
- Data
    - 74,158 Congressional floor speeches

## æ“ä½œ

:::{.r-stack}

![](https://drhuyue.site:10002/sammo3182/figure/theory_vocalPitch_hcy.png){fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/theory_vocalPitch.png){.fragment fig-align="center" height=600}

:::


## ğŸŒ° IV

@ZhangPan2019

- Objective
    - Group activities from social media
- Method
    - CNN for images; CNN-RNN for texts
- Data
    - A random sample of 20,000 geocoded posts from Weibo, 2010--2017

:::{.notes}
CNN: Convolutional Neural Network

RNN: Recurrent Neural Network
:::

## æ“ä½œ

:::{.r-stack}

![](https://drhuyue.site:10002/sammo3182/figure/theory_casm.png){fig-align="center" width=1000}

![](https://drhuyue.site:10002/sammo3182/figure/theory_runningMan.png){.fragment fig-align="center" height=400}

![](https://drhuyue.site:10002/sammo3182/figure/theory_banner.png){.fragment fig-align="center" height=500}

:::



## æŒ‘æˆ˜


*ç†è®º*: Single causal mechanism 

[*å‰æ*: Bag of words]{.fragment}

[*æ•°æ®*

* DGP
    + éšæœºæŠ½å–    
    + Only posted   
    + One-time censor
* éç»“æ„åŒ–
* æµ·é‡æ½œåœ¨ç»´åº¦
* å†…å®¹å¤æ‚ä¸”å¾®å¦™]{.fragment}


:::{.notes}

å®¡æŸ¥å·²ç»ä½¿ç”¨ machine learning

Su, Y.-S., Y. Ruan, S. Sun, and Y.-T. Chang. 2020. â€œA Pattern Recognition Framework for Detecting Changes in Chinese Internet Management System.â€ Journal of Social Computing 1(1): 28â€“39.
:::

:::

::::


# ä»¥æ•°çŸ¥æ–‡: ç†è§£åŸåˆ™


æœ¬èŠ‚è¦å›ç­”ä¹‹é—®é¢˜ï¼š

:::{style="text-align:center; margin-top: 2em"}
1. èµ„æºå“ªé‡Œæ‰¾ï¼Ÿ
1. ä¿¡æ¯å¦‚ä½•ç”¨ï¼Ÿ
1. æ•°æ®èƒ½å¹²å•¥ï¼Ÿ
1. **[çº¢çº¿]{.red}åœ¨å“ªé‡Œ**ï¼Ÿ
:::


## åŸç”Ÿæ•°æ®

:::: {.columns}

::: {.column width="20%"}
* Email/çŸ­ä¿¡
* ç½‘ç«™HTML
* RSS feeds
* ç½‘ç»œç¤¾äº¤åª’ä½“
* ç½‘ç»œè®ºå›
* ç½‘ç»œé—®ç­”å¹³å°
* åª’ä½“æ•°æ®åº“
* ç½‘ç»œäº¤æ˜“è¡Œä¸º    
â€¦â€¦
:::

::: {.column width="80%"}

![ç¤¾äº¤åª’ä½“](https://drhuyue.site:10002/sammo3182/figure/text_social-media.gif){fig-align="center" height=500}

:::{.notes}
æ–°æµªå¾®åš1.4äº¿ï¼Œå¾®ä¿¡ç”¨æˆ·5.5äº¿;
å¾®ä¿¡æ—¥å¢æ•°æ®500TBï¼ŒQQæ—¥å¢æ•°æ®200TBã€‚
:::
:::

::::


## å…¬å…±å¼€æ”¾å¹³å°

![](https://drhuyue.site:10002/sammo3182/figure/text_zhongsheng_index.png){fig-align="center" height=600}

## ç½‘ç»œé—®æ”¿å¹³å°

![](https://drhuyue.site:10002/sammo3182/figure/text_henan-Egovernment.png){fig-align="center" height=600}


## ç¤¾ä¼šåŒ–é—®ç­”ç½‘ç«™

![](https://drhuyue.site:10002/sammo3182/figure/text_zhihu.png){fig-align="center" height=600}

:::{.notes}
Quora, Stack Overflow
:::


## åª’ä½“æ•°æ®åº“

![](https://drhuyue.site:10002/sammo3182/figure/text_renminribao.png){fig-align="center" height=600}


## é—®å·å¼€æ”¾æ€§é—®é¢˜

![](https://drhuyue.site:10002/sammo3182/figure/text_survey_openQuestion.png){fig-align="center" height=600}


## äºŒæ‰‹æ•°æ®

::: {.panel-tabset}
### æ¡£æ¡ˆæ•°æ®

* ä¸­å›½çŸ¥ç½‘ç­‰æ•°æ®åº“ï¼ˆæœŸåˆŠã€æŠ¥åˆŠã€å¹´é‰´ç­‰ï¼‰
* Google Booksã€ç™¾åº¦å­¦æœ¯
* Google Trendã€ç™¾åº¦æŒ‡æ•°
* JSTOR Data for Researchâ€¦â€¦

### ç™¾åº¦æŒ‡æ•°

![](https://drhuyue.site:10002/sammo3182/figure/text_baidu_zhishu.png){fig-align="center" height=500}

### æ–‡æœ¬æ•°å­—åŒ–

:::{.r-stack}

![](https://drhuyue.site:10002/sammo3182/figure/theory_hongkong_lib.png){fig-align="center" height=500}

![](https://drhuyue.site:10002/sammo3182/figure/theory_cbdb.png){.fragment fig-align="center" height500}
:::

:::


## æ–‡æœ¬è·å–

* åŸç”Ÿæ•°æ®ï¼šSpider/crawler/scraper
* äºŒæ‰‹æ•°æ®ï¼šæ¡£æ¡ˆæ•°æ®å’Œæ•°å­—åŒ–æ•°æ®

:::{.r-stack}

![](https://drhuyue.site:10002/sammo3182/figure/theory_bazhuayu.png){fig-align="center" height=450}

![](https://drhuyue.site:10002/sammo3182/figure/theory_huochetou.png){.fragment fig-align="center" height=450}

![](https://drhuyue.site:10002/sammo3182/figure/theory_gooseeker.png){.fragment fig-align="center" height=450}
:::

## æ“ä½œæ¼”ç¤º

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/theory_gooseekerI.png){.fragment fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/theory_gooseekerII.png){.fragment fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/theory_gooseekerIII.png){.fragment fig-align="center" height=600}
:::

## ç¼–ç¨‹æŠ“å–

`SelectorGadget` (Chrome add-in)

![](https://drhuyue.site:10002/sammo3182/figure/theory_zhongsheng_page.png){fig-align="center" height=500}

## Scrapping with `rvest`

```{r zhongsheng-eg}
#| eval: false
#| echo: true

ls_zhongsheng <-
  read_html("http://politics.people.com.cn/GB/8198/426918/index.html") |> # index page
  html_nodes("h5 a") |> # the nodes of the links
  html_attr("href") |> # just the links
  str_replace("^/n1", "http://politics.people.com.cn/n1")

df_zhongsheng <- map_df(ls_zhongsheng, function(link) {
  zs_article <- read_html(link, encoding = "GB18030") # read the html
  
  zs_title <- html_nodes(zs_article, "h1") |>
    html_text
  
  zs_time <- html_nodes(zs_article, ".box01 .fl") |>
    html_text |>
    str_extract("\\d{4}å¹´\\d{2}æœˆ\\d{2}æ—¥")
  
  zs_content <- html_nodes(zs_article, "#rwb_zw p") |>
    html_text |>
    str_c(collapse = "") |> # combined the paragraphs
    str_remove_all("\\s|\\n|\\t") # remove the horizontal spaces
  
  zs_data <- data.frame(title = zs_title,
                        time = zs_time,
                        content = zs_content)
})
```

## æ­£åˆ™è¡¨è¾¾å¼

![](https://drhuyue.site:10002/sammo3182/figure/theory_regular_expression.png){fig-align="center" height=600}

## æ–‡æœ¬æ•°æ®ç»“æ„åŒ–

![](https://drhuyue.site:10002/sammo3182/figure/theory_zhongsheng_output.png){fig-align="center" width=1000}


## æ–‡æœ¬åˆ†æçš„åŸºç¡€åŸåˆ™ï¼ˆ[çº¢çº¿]{.red}ï¼‰ [@GrimmerStewart2013]

:::{.incremental .large style="text-align:center; margin-top: 1em"}
1. All quantitative models of language are [wrong]{.red}---but some are useful. 
1. Quantitative methods for text amplify resources and [augment humans]{.red}. 
1. There is [no globally best]{.red} method for automated text analysis. 
1. [Validate, Validate, Validate.]{.fragment .highlight-blue}
:::

# æœ›æ•°ç”Ÿä¹‰

## æ–‡æœ¬åˆ†æï¼ˆä¼ ç»Ÿï¼‰æ–¹æ³•æ¦‚è§ˆ

![@GrimmerStewart2013](https://drhuyue.site:10002/sammo3182/figure/text_analysis-method.png){fig-align="center" height=550}

## ç ”ç©¶å±‚æ¬¡

:::: {.columns}

::: {.column width="50%"}

**æ•°æ®å±‚æ¬¡**

- Corpus
    - Document (volumn, chapter, section)
        - Paragraph [&check;]{.orange}
            - Sentence [&check;]{.green}
                - Clause [&check;]{.green}
                    - **Word (Unigram)** [&check;]{.green}
                        - **Token** [&check;]{.green}
                        

:::{.fragment .callout-tip}
## "Token"ï¼šè¯­è¨€ç‰¹å¾å•å…ƒ

- Token in a document: term
- Token in a group: N-gram 
    - A word = a Unigram
    
:::

:::

::: {.column .fragment width="50%"}

**åˆ†æå±‚æ¬¡**

### æè¿°

è¯é¢‘ã€è¯äº‘(ğŸ‘)ã€ç½‘ç»œ

### èšç±»

çŸ¥ç±»åˆ†æ–‡ã€çŸ¥æ–‡åˆ†ç±»

### è¯­ä¹‰

æƒ…æ„Ÿåˆ†æ(sentiment analysis)

:::

::::


## A Bag of Words

[> All quantitative models of language are [wrong]{.red}---but some are useful [@GrimmerStewart2013].]{.fragment}

:::{.fragment .callout-warning}
## Bag of words (BoW)

A text is represented as the bag (multiset) of its words.
:::

![](https://drhuyue.site:10002/sammo3182/figure/text_bagOfWords.png){.fragment fig-align="center" height=300}


## ä»è‡ªç„¶è¯­è¨€åˆ°è®¡ç®—æœºè¯­è¨€

![Document-Term Matrix (DTM)](https://drhuyue.site:10002/sammo3182/figure/theory_bagOfWords2.jpg){fig-align="center" height=600}

:::{.notes}
In linguistics, the opposite of natural language is artificial/constructed language (conlangs),  like Klingon in "Star Trek," Dothraki in "Game of Thrones"
:::

## é¢„å¤„ç†

:::{style="text-align:center; margin-top: 2em"}
- Segmentation
- Tokenization
- Stopwords (åœè¯ï¼‰/function words removing   
- å…¶ä»–æ ¹æ®ç ”ç©¶ç›®çš„çš„åˆ å‡
:::


## Segmentation

:::: {.columns}

::: {.column width="30%"}
*Scriptio discreta* (e.g., English)

Document &rarr; paragraphs/sentences

:::{.notes}
discrete script
:::

:::

::: {.column .fragment width="70%"}
*Scriptio continua* (e.g, CJK) 

```{r segment-eg}
#| code-fold: false

library(jiebaR)

zhongsheng <- "è¿‘å¹´æ¥ï¼Œç¾å›½ä¸€äº›æ”¿å®¢è¢«â€œç¾å›½ä¼˜å…ˆâ€é®ä½äº†åŒçœ¼ï¼Œå¤§æè´¸æ˜“ä¿æŠ¤ä¸»ä¹‰ã€å•è¾¹ä¸»ä¹‰ï¼Œè‚†æ„æŒ¥èˆå…³ç¨å¤§æ£’ï¼Œå…¨ç„¶ä¸é¡¾ä¸­ç¾ä¸¤å›½äººæ°‘å’Œå…¨ä¸–ç•Œäººæ°‘çš„å¼ºçƒˆåå¯¹ã€‚"
cutter <- worker() # segment engine

segment(zhongsheng, cutter)
```
:::

:::{.notes}
continuous script

ã€Šäººæ°‘æ—¥æŠ¥ã€‹ï¼ˆ2019å¹´05æœˆ30æ—¥02ç‰ˆï¼‰
:::


::::

## Tokenization

:::: {.columns}

::: {.column width="50%"}
- ç›®æ ‡ï¼šå»é™¤Syntax
    + å¤§å°å†™
    + æ ‡ç‚¹
    + éå­—ç¬¦ï¼ˆ@#ï¿¥%â€¦â€¦&*ï¼‰
    - åœè¯
:::

::: {.column .fragment width="50%"}
```{r stop-words}
#| echo: false

read_lines(STOPPATH)[883:903]
read_lines(STOPPATH)[157:177]
```
:::

::::


:::{.fragment .callout-tip}
## ä¸­æ–‡åœè¯è¡¨

* ç™¾åº¦åœè¯è¡¨
* å“ˆå·¥å¤§åœè¯è¡¨
* ç½‘ç»œæœé›†åœè¯è¡¨ï¼Œå¦‚["æœ€å…¨ä¸­æ–‡åœç”¨è¯è¡¨æ•´ç†ï¼ˆ1893ä¸ªï¼‰"](https://blog.csdn.net/shijiebei2009/article/details/39696571)
:::

## Scriptio discreta special

- Lemmatization: (happy, happier, happiness) &rarr; happy
- Stemming: (happy, happier, happiness) &rarr; happi

:::{.fragment .callout-tip}
## More Examples

- Lemmization: (went, leaves, geese, unhappy) &rarr; (go, leaf, goose, unhappy)
- Stemming: (went, leaves, geese, unhappy) &rarr; (went, leav, geese , unhappi)
:::


:::{.fragment style="text-align:center; margin-top: 1em"}
*ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšï¼Ÿ*

:::{.fragment}
è¿˜èƒ½åšä»€ä¹ˆ

- Labeling
    - Content vs. function
    - Linguistic features: n., v., adj., adv., prep., conj.......
:::


:::


:::{.notes}
Issue: Sparse matrix
:::


## ç»¼åˆçš„ğŸŒ°

![2019-05-14 ~ 05-22: ä¸­ç¾è´¸æ˜“æˆ˜](https://drhuyue.site:10002/sammo3182/figure/text_zhongsheng.png){fig-align="center" height=550}

## æ•°æ®æ¸…ç†

*åŸå§‹æ•°æ®*

```{r zhongsheng-clean}
df_zhongsheng <- readRDS(url("https://drhuyue.site:10002/sammo3182/data/zhongsheng.RDS"))

df_zhongsheng$segmented <- map_chr(df_zhongsheng$content, function(content){
  segment(content, cutter) |> paste(collapse = " ")
})

df_zhongsheng$phase <- "US_fail"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-22"] <- "theory_test"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-11"] <- "reassessment"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-08"] <- "optimism"


df_token <- df_zhongsheng |> 
  select(-content) |> 
  unnest_tokens(word, segmented) # tokenization

# Show the word counts
select(df_token, -phase) |> head()
```

:::{.fragment}
*å»æ‰åœè¯*

```{r zhongsheng-stop}
#| code-fold: true

# removing the stop words
df_stopWords <- tibble(word = read_lines(STOPPATH))

df_token <- df_token |> 
  anti_join(df_stopWords) 

select(df_token, -phase) |> head()
```

:::

## è¯é¢‘åˆ†æ

```{r zhongsheng-frequency, fig.align='center'}
df_plot <- df_token |> count(word, sort = TRUE) |> 
  filter(n > 151) |>
  mutate(word = reorder(word, n))

ggplot(df_plot) +
  geom_col(aes(word, n)) +
  xlab("é«˜é¢‘è¯") +
  ylab("è¯é¢‘") + 
  theme(axis.text = element_text(size = 20)) +
  coord_flip()
```

## è¯é¢‘å¼‚è´¨æ€§

```{r zhongsheng-phase-freq}
#| fig-height: 6
#| fig-pos: center

df_plot <- df_token |> 
  group_by(phase) |> 
  count(word, sort = TRUE) |> 
  top_n(10) |>
  ungroup() |> 
  mutate(word = reorder(word, n),
         word = factor(word, levels = rev(unique(word))),
         phase = factor(phase, labels = c("ä¹è§‚è§‚æœ›", "é‡æ–°è¯„ä¼°", "ç†è®ºæ£€éªŒ", "ç¾å›½å¿…è´¥")))

ggplot(df_plot, aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  ylab("è¯é¢‘") + 
  theme(axis.text = element_text(size = 12)) +
  facet_wrap(~ phase, scales = "free")
```

:::{.notes}
The Economist journalist Simon Rabinovitch
:::

## ç›¸å…³æ€§åˆ†æ

```{r zhongsheng-fre-compare}
#| fig-height: 7
#| fig-pos: center

frequency <- df_token |> 
  count(phase, word) |>
  group_by(phase) |>
  mutate(proportion = n / sum(n)) |> 
  select(-n) |> 
  spread(phase, proportion) |> 
  gather(phase, proportion, optimism:theory_test)

library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `US_fail`, color = abs(`US_fail` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, family = "WenQuanYi Micro Hei") +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~phase, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "ç¾å›½å¿…è´¥", x = NULL) +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

:::{.notes}
x = proportion, y = `US_fail`

proportion = n / sum(n)
:::

## å…³é”®è¯è¯†åˆ«ä¸åˆ†æ

2019-05-07 å…³é”®è¯

```{r keyword1}
extractor_keyword <- worker("simhash", topn = 5)

simhash(df_zhongsheng$content[df_zhongsheng$time == "2019-05-07"], extractor_keyword)
```


:::{.fragment}
2019-05-30 å…³é”®è¯

```{r keword2}
simhash(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"], extractor_keyword)
```
:::



## ç›¸ä¼¼åº¦åˆ†æ

é€šè¿‡â€œè·ç¦»â€æµ‹é‡ç›¸ä¼¼åº¦

> Hamming distance: the distance between two strings of equal length is the number of [positions]{.blue} at which the [corresponding symbols are different]{.red}. 

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/theory_hammingDis_org.png){fig-align="center" height=num}
![](https://drhuyue.site:10002/sammo3182/figure/theory_hammingDis_comp.png){fig-align="center" height=num}
:::

:::{style="text-align:center; margin-top: 1em"}
e.g., H(100â†’011) = 3; H(010â†’111) = 2.
:::


:::{.notes}
It measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.
:::

## ç›¸ä¼¼åº¦èƒ½å‘Šè¯‰ä½ ä»€ä¹ˆ

:::: {.columns}

::: {.column width="50%"}
*05-30 vs. 05-[22]{.red}*

```{r keyword-distnace}
distance(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"],
         df_zhongsheng$content[df_zhongsheng$time == "2019-05-22"],
         extractor_keyword)
```
:::

::: {.column width="50%"}
*05-30 vs. 05-[23]{.red}*

```{r keyword-distance2}
distance(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"],
         df_zhongsheng$content[df_zhongsheng$time == "2019-05-23"][1],
         extractor_keyword)
```
:::

::::

## BOWä¹‹ä¸Šï¼šåŠ æƒ

DTMçš„é—®é¢˜ï¼š

1. æœªå°†è¯çš„é‡è¦æ€§çº³å…¥è€ƒé‡
2. è¿‡åº¦ä½“ç°å¸¸è§è¯
3. è½»è§†å°‘è§è¯

[è§£æ³•: è¯é¢‘åŠ æƒ e.g., term frequency-inverse document frequency (TF-IDF)]{.fragment}

## åŠ æƒå¸¦æ¥ä»€ä¹ˆ

- ä¼˜ç‚¹ï¼š
  1. è¯†åˆ«é‡è¦è¯æ±‡
  2. å‡å°‘å¸¸è§è¯æ±‡çš„æƒé‡
  3. æé«˜æœºå™¨å­¦ä¹ æ€§èƒ½

:::{.fragment}
- ç¼ºç‚¹ï¼š
  1. å‡å®šè¯æ±‡ç‹¬ç«‹ï¼ˆä¸DTMç›¸åŒï¼‰
  2. å¯¹åœ¨è¯­æ–™åº“ä¸­éå¸¸ç½•è§ä½†åœ¨ç‰¹å®šæ–‡æ¡£ä¸­å‡ºç°è¿‡å‡ æ¬¡çš„ç½•è§è¯æ±‡ç»™äºˆé«˜æƒé‡
  3. å¯¹è¯­æ–™åº“çš„å¤§å°å’Œå¤šæ ·æ€§æ•æ„Ÿ
:::


## åŠ æƒä¹‹ä¸Šï¼šBring the context back

- ä¸´è¿‘è¯åˆ†æ:  Markov Model of Order N
    + Unigram: æ¸…å å¤§å­¦ æ”¿æ²» ç³»
    + Bigram: æ¸…åå¤§å­¦ æ”¿æ²»ç³»/æ¸…å å¤§å­¦æ”¿æ²» ç³»
    + Trigramï¼šæ¸…åå¤§å­¦æ”¿æ²» ç³»/æ¸…å å¤§å­¦æ”¿æ²»ç³»
    - *Keyness*

:::{.fragment}
- åŠŸèƒ½è¯åˆ†æ

![](https://drhuyue.site:10002/sammo3182/figure/text_pronoun.png){fig-align="center" height=300}
:::


## ç»™è¯ä¹‰å»ºæ¨¡ï¼šè¯åµŒå…¥(Word embedding)

> Words' meanings depend not just on immediate neighbors

![](https://drhuyue.site:10002/sammo3182/figure/theory_wordEmbedding.png){fig-align="center" height=550}

:::{.notes}
The term "embedding" comes from the neural network literature, in which an "embedding layer" is an input function that efficiently compresses high-dimensional data down to a low-dimensional dense representation for input to subsequent neural network layers.

- The embedding model GloVe ("Global Vectors") by @PenningtonEtAl2014 is explicitly designed to construct word vectors encoding local co-occurrence.
- An equally influential word embedding model is Word2Vec [@BengioEtAl2000], which treats each instance of a word and its context as a separate prediction problem that word vectors are chosen to solve.
:::

## ä¸»é¢˜æ¨¡å‹(Topic modeling)

{{< video https://drhuyue.site:10002/sammo3182/video/theory_topicModeling.webm title="What happened in topic modeling" height=600 loading="eager" allowfullscreen>}}

:::{.notes}
LSA, NMF, and LDA can also be viewed as producing word embeddings. In particular, the (V Ã— K) matrix B from (2) contains a series of row vectors corresponding to each term in the vocabulary (see also Levy and Goldberg 2014). Those vectors contain information about word co-occurrence at the document level, rather than within a local context.
:::

## æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanism)

> Word embedding è®¤ä¸ºæ‰€æœ‰è¯å’Œè¯ä¹‹é—´å…³ç³»éƒ½åŒç­‰é‡è¦[ğŸ¤¦â€â™‚ï¸]{.large}

:::{.fragment .fade-in-then-semi-out}
"Attention is all you need" [@VaswaniEtAl2017]

ä»¥ä¸‹æ˜¯å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ³¨æ„åŠ›æœºåˆ¶çš„ç¤ºä¾‹ç¿»è¯‘ï¼š

> ä½œä¸ºåœ¨_____é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„è½¯ä»¶å·¥ç¨‹å¸ˆã€‚    
> ä½œä¸ºåœ¨_____é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„å¤ªé˜³èƒ½å·¥ç¨‹å¸ˆã€‚

åº”è¯¥åœ¨ "_____" å¡«å…¥ä»€ä¹ˆè¯ï¼Ÿä½ æ˜¯å¦‚ä½•å¾—å‡ºè¿™ä¸ªç»“è®ºçš„ï¼Ÿ

:::


:::{.fragment}
ä½œä¸ºåœ¨*ä¿¡æ¯æŠ€æœ¯*é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„[è½¯ä»¶]{.red}å·¥ç¨‹å¸ˆã€‚    
ä½œä¸ºåœ¨*ç»¿è‰²èƒ½æº*é¢†åŸŸçš„å¤´éƒ¨ä¼ä¸šï¼Œæˆ‘ä»¬é›‡ä½£äº†å¤§é‡é«˜æ°´å¹³çš„[å¤ªé˜³èƒ½]{.red}å·¥ç¨‹å¸ˆã€‚

:::


## è¯åµŒå…¥ &rarr; è¯åºåˆ—ï¼ˆWord Sequenceï¼‰

:::{.callout-note}
## è‡ªæ³¨æ„åŠ›æœºåˆ¶

è¾“å…¥ä¸€ä¸ªåˆå§‹è¯å…ƒåµŒå…¥åºåˆ—ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ–°çš„è¯å…ƒåµŒå…¥åºåˆ—ï¼Œ[ä½¿åˆå§‹åµŒå…¥èƒ½å¤Ÿç›¸äº’ä½œç”¨]{.red}ã€‚
:::

- ç”±å †å çš„æ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œå±‚ç»„æˆçš„å¤§å‹ç¥ç»ç½‘ç»œå¯ä»¥ä½¿ç”¨ä¸“ç”¨å¤„ç†å™¨é«˜æ•ˆå¹¶è¡Œè®­ç»ƒï¼Œå³ [Transformer]{.red}
- å¸¸è§çš„ Transformer æ¨¡å‹
    - BERT
        - RoBERTa, PALM
    - [GPT]{.red .large} ç³»åˆ—


## æ€»ç»“

:::: {.columns}

::: {.column width="50%"}
1. è®¤çŸ¥
    + ä¸°å¯Œèµ„æº
    + æŠ€æœ¯é—¨æ§›
1. åŸåˆ™
    + åœ¨â€œé”™è¯¯â€çš„å‰æä¸‹å¯»æ‰¾ä»·å€¼
:::

::: {.column width="50%"}
3. æ“ä½œ
    - æ‰“æ•£ï¼šé¢„å¤„ç†ä¸ç»“æ„åŒ–
    - èšåˆï¼š
        - è¯é¢‘
        - ç›¸å…³æ€§/ç›¸ä¼¼åº¦
        - BOWä¹‹ä¸Šï¼ˆè¯­å¢ƒä¸è”ç³»ï¼‰
:::
::::

![Distant reading](https://drhuyue.site:10002/sammo3182/figure/text_indirect_phone.gif){.fragment fig-align="center" height=300}



# æ„Ÿè°¢å€¾å¬ï¼Œæ•¬è¯·æŒ‡æ­£ {background="#43464B"}

:::{style="text-align: right; margin-top: 1em"}  

[`r feather_icons("github")`&nbsp; sammo3182](https://github.com/sammo3182)

[`r feather_icons("mail")`&nbsp; yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn) 

[`r feather_icons("globe")`&nbsp; https://www.drhuyue.site](https://www.drhuyue.site)

![](https://user-images.githubusercontent.com/6463211/232207708-b0e64eee-7fb3-45a4-9779-ec52397f786c.png){height=250}
:::

## å‚è€ƒæ–‡çŒ®

::: {#refs}
:::
