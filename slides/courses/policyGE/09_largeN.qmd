---
title: |
  Case Illustration: 
  Econometric Analysis
subtitle: "Understanding Policies (10700193-90)"
author: "Yue Hu"
institute: "Tsinghua University" 
bibliography: t_publicPolicy.bib

execute: 
  echo: false
editor_options: 
  chunk_output_type: console
editor: 
  render-on-save: true

format: 
  revealjs:
    number-sections: false
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: true
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
---


```{r}
#| label: setup
#| include: false

if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  icons, 
  tidyverse,
  tinytable,
  modelsummary,
  drhutools,
  dotwhisker
) # data wrangling # data wrangling

# Functions preload
set.seed(313)

# Visualization
theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)
```

## Review how to evaluate an experiment {.center .unnumbered background="#43464B"}

:::: {.columns}

::: {.column .nonincremental width="70%"}

- **Question**: Do people view others differently because of the language they use?  
- **Subjects**: 500 students from Japan and Korea 
- **Design**: 
    1. Watch the video on the right
    1. Answer the survey about the characteristics of Nezha (masculinity, intimacy, powerfulness, literacy, etc.)

:::

::: {.column width="30%"}

{{< video https://drhuyue.site:10002/sammo3182/video/lab_nezhaDialect.mp4 title="é»å”åœ¨åŠ ç­ï¼šã€Šå“ªå’2ã€‹å¤šå›½å¤šåœ°åŒºå¤šè¯­è¨€å¤šæ–¹è¨€é…éŸ³æ¥å•¦ï¼ä½“éªŒä¸ä¸€æ ·çš„å“ªå’ï¼" height=600 preload="auto" controls allowfullscreen>}}

::: {.notes}

https://www.bilibili.com/video/BV1euPkerEnp/?vd_source=f38aeefd0d38cecba9017eeee43e71c8

:::

:::

::::

## Overview {.unnumbered}

:::: {.columns}

::: {.column width="60%"}

+ WHAT is a large-N analysis (LNA)
- WHY LNA
  - Why *not* experiment
  - What do we look for with LNA?
+ HOW
    + One
    + Two
    + More than two

:::

::: {.column width="40%"}

![](https://drhuyue.site:10002/sammo3182/figure/lar_econometrics.jpg){fig-align="center" height=400}

:::

::::


# What

## What's LNA

In lieu of "quantitative"...

:::{.fragment}
Large-[N]{.red}: Number of **observations**---not participants, but trails/units
:::


:::{.notes}

What's a å®šé‡ç ”ç©¶?

- å®šç»“æœçš„é‡
- å®šæ•°æ®çš„é‡
  - éƒ½æ˜¯é‡å—ï¼Ÿbinaryï¼Œcategoricalï¼Œ ordinary

Did you really å®šé‡ï¼Ÿ

:::


::: {.callout-note .fragment}

## How large? 10, 100, 1000, 10000?

- > We don't know......   
- > It depends......  
- > The larger, the better!

[To know how large, you have to know why using this method...]{.fragment}

:::


# Why

## Why Not Experiments?

:::: {.columns}

::: {.column .nonincremental width="50%"}

**Experiment is powerful!**

+ Excluding confounders
+ Clear outcomes
+ Clear causal chain

:::

::: {.column width="50%"}

**But**

- [Difficult]{.red} to conduct
- [Difficult]{.red} to represent
- No "cool" methods

:::

::::


::: {.fragment .large style="text-align:center; margin-top: 2em"}

Alternative: [Quasi]{.grayLight}-experiment^[[*Collins Thesaurus*: Quasi- =  pseudo-, apparent, seeming, semi-, so-called, would-be.]{.fragment}]

[A.k.a., Pseudo random assignment]{.fragment}

:::


## Why Have to Be large

:::{.large style="text-align:center; margin-top: 1em"}

- Sufficient variance to [fake]{.red} the control-treatment structure
- Sufficient amount to [represent]{.red} the population

::: {.callout-tip .fragment}

## Statistical foundation

- Law of Large Numbers
- Central Limit Theorem

:::

:::


## Representation


:::{.callout-important}
## *Law of Large Numbers*(LLN)

As the number of experiments (sample) increases, the ratio of outcomes will converge to the theoretical (population) average.
:::

- &rarr; A rule of thumb: 100

:::{.fragment .callout-important}
## *Central Limit Theorem*(CLT)

The sampling distribution of the sample means approaches a normal distribution as the sample size gets larger.
:::

- &rarr; A rule of thumb: 1000 [*(Where does this number come from? We'll talk about that soon)*]{.fragment}

::: {.notes}

Many natural phenomena and measurements tend to follow a **normal distribution** (a bell-shaped curve) because of how randomness and averages work. Hereâ€™s why:

### **Why Things Often Look "Normal"**
1. **The "Averaging Out" Effect**  
   Imagine rolling a single die: each number (1â€“6) is equally likely. But if you roll **10 dice** and take the *average*, the results will cluster around 3.5 (the middle). This happens because extreme highs and lows cancel out when you average many tries. This is called the **Central Limit Theorem**â€”when you collect enough data, averages tend to form a bell curve, even if the original data isnâ€™t normal.

2. **Many Small Factors**  
   Things like human height or test scores are influenced by *lots of tiny, random factors* (genes, diet, study habits). No single factor dominates, so their combined effect creates a balanced, symmetric pattern around an average.

### **Why Some Things Start Non-Normal But Become Normal**
Some patterns start off skewed or uneven but turn normal as you gather more data:

#### **Example 1: Counting Rare Events (Poisson Distribution)**  
- **Small numbers**: If you count something rare, like "how many shooting stars you see in 10 minutes," most counts will be 0 or 1, with a few higher numbers. This creates a lopsided (right-skewed) distribution.  
- **Large numbers**: If you count shooting stars over *100 nights*, the counts will spread out and average out, forming a bell curve. The randomness balances out with more data.

#### **Example 2: Coin Flips (Binomial Distribution)**  
- **Few flips**: Flip a coin 10 times. The number of heads might be 3, 5, or 7â€”unpredictable and uneven.  
- **Many flips**: Flip it 1,000 times. The number of heads will almost always be close to 500, forming a normal curve. More trials = more predictability.

---

### **Key Takeaway**  
- **Normal distribution** is common because randomness often "smooths out" when you average many small, independent effects.  
- **Non-normal at first?** With enough data (or larger sample sizes), even skewed patterns tend to look normal due to the Central Limit Theorem.  

Think of it like stirring a lumpy cake batter: at first, you see chunks (unevenness), but after enough mixing (more data), it becomes smooth!

:::

## You gonna feel it

::: {style="text-align:center"}

My highly-educated fellows, let's play a [happy-happy]{.golden} kid game!

And we are going outside! ğŸ˜ˆ

1. Toss a fair, single die
    - Odds: [One]{.golden} step to the [&larr;]{.blue}
    - Evens: [One]{.golden} step to the [&rarr;]{.red}
1. Toss [six]{.golden} times
1. Going forward

:::


## Replicate the "magic"

{{< video https://drhuyue.site:10002/sammo3182/video/lar_normalDistribution_bbc.mp4 title="Normal by people" height=600 loading="eager" allowfullscreen>}}

## Why does CLT ask for over 1000

![](https://drhuyue.site:10002/sammo3182/figure/lar_largeNumber.gif){fig-align="center" height=200}

:::{.fragment .callout-tip}

## The larger the better? 

$$\text{Margin of Error (M.E.)} = Z \times \frac{S}{\sqrt n}.$$

| Sample Size (n) | Margin of Error (M.E.) |
|-----------------|------------------------|
| 200             | 7.1%                   |
| 1000            | 3.2%                   |
| 2000            | 2.2%                   |
| 4000            | 1.6%                   |

:::

:::{.notes}

To cut the margin of error in half, like from 3.2% down to 1.6%, you need four times as big of a sample, like going from 1000 to 4000 respondents. To cut the margin of error by a factor of five, you need 25 times as big of a sample

:::

::: {.fragment style="text-align:center"}

*Now you know how large is large! Good for you ğŸ‘ğŸ‰*

:::

## LNA for public policy

:::{.large}

> [ä»»ä½•æ”¿ç­–éƒ½å»ºç«‹åœ¨å¯¹äº‹ç‰©å·®å¼‚æ€§çš„åˆ†æå’ŒæŠŠæ¡ä¹‹ä¸Šï¼Œ**æ²¡æœ‰å·®å¼‚æ€§å°±æ²¡æœ‰æ”¿ç­–**ã€‚æˆ‘å›½ç¤¾ä¼šå·®å¼‚æ€§ç‰¹å¾æ˜æ˜¾ï¼Œåæ˜ åœ¨åœ°åŸŸã€åŸä¹¡ã€æ°‘æ—ã€äººç¾¤ç­‰å¤šä¸ªå±‚é¢ã€‚æˆ‘ä»¬çš„å†³ç­–éƒ¨ç½²æœ‰ç»¼åˆæ€§çš„ï¼Œä¹Ÿæœ‰ä¸“é¡¹æ€§çš„â€¦â€¦è¿™äº›éƒ½éœ€è¦å……åˆ†ç†Ÿæ‚‰æƒ…å†µã€æ·±å…¥åˆ†æè®ºè¯ã€ç§‘å­¦æŠŠæ¡å°ºåº¦ã€‚è¦åšæŒç§‘å­¦å†³ç­–ã€æ°‘ä¸»å†³ç­–ã€ä¾æ³•å†³ç­–ï¼Œå¯¹æƒ…å†µè¿›è¡Œæ·±å…¥åˆ†æ]{.red} [@XiJinPing2019, pp. 118-119]ã€‚

:::

::: {.notes}

ã€Šä¸­å¤®æ”¿æ²»å±€çš„åŒå¿—å¿…é¡»æœ‰å¾ˆå¼ºçš„çœ‹é½æ„è¯†ã€‹(2015å¹´12æœˆ28æ—¥ã€29 æ—¥),ä¹ è¿‘å¹³ã€Šè®ºåšæŒå…šå¯¹ä¸€åˆ‡å·¥ä½œçš„é¢†å¯¼ã€‹,ä¸­å¤®æ–‡çŒ®å‡ºç‰ˆç¤¾2019å¹´ç‰ˆï¼Œç¬¬118-119é¡µ

:::

# How

## Univariate analysis

:::: {.columns}

::: {.column width="60%"}

![](https://drhuyue.site:10002/sammo3182/figure/lar_tooManyCourses.jpg){fig-align="center" height=600}

:::{.notes}

Let's say we'll test a school policy of course management

:::

:::

::: {.column width="40%"}

```{r}
#| label: data

showWatching <- tibble(Student_ID = paste0("202509", sample(1000:9000, size = 7)), Courses = c(9, 9, 12, 10, 8, 10, 15))

tt(showWatching)
```

:::

::::



## How to Describe a Variable

Given the courses students took: (9, 9, 12, 10, 8, 10, 15)

::: {.fragment .nonincremental}

+ Mean: $\frac{9 + 9 + 12 + 10 + 8 + 10 + 15}{7} \approx 11.$
+ Median: 8, 9, 9, [10]{.red}, 10, 12, 15
+ Mode: 8, [9]{.red}, [9]{.red}, [10]{.red}, [10]{.red}, 12, 15

:::

:::: {.columns}

::: {.column .fragment width="40%"}

*Which one is **the best**?*

:::

::: {.column .fragment width="60%"}

![How about this](https://drhuyue.site:10002/sammo3182/figure/desc_sheHulk.png){fig-align="center" height=400}

:::

::::

## Levels of Descriptive Statistics {.scrollable}

:::: {.columns}

::: {.column width="20%"}

- Number
- Statistics
- Plot

:::

::: {.column .fragment width="80%"}

```{r}
#| label: descriptive

df_descrptive <- select(mtcars, -am, -carb) |>
  mutate(across(where(~length(unique(.x)) == 2), as.logical),
  across(where(~length(unique(.x)) < 7 & !is.logical(.x)), as.factor))

datasummary_skim(df_descrptive)
```

:::

::::


## Binary Analysis


```{r}
#| fig-align: center
#| fig-height: 6
#| fig-cap: Weight of a car and how long it can run
#| label: scatter

ggplot(mtcars, aes(wt, mpg)) +
  geom_point() +
  ylab("Mileage per Gallon") +
  xlab("Weight")
```

## Not always work

```{r}
#| fig-align: center
#| fig-height: 6
#| fig-cap: How about this data then?
#| label: scatterMess

data(tobacco, package = "summarytools")

ggplot(tobacco, aes(age, cigs.per.day)) +
  geom_point() +
  ylab("Cigrate per day") +
  xlab("Age")
```

## Multivariate Analysis

:::{.r-stack}

![](https://drhuyue.site:10002/sammo3182/figure/lar_buyerSeller.jpg){fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/lar_matching.png){.fragment fig-align="center" height=600}

![](https://drhuyue.site:10002/sammo3182/figure/lar_fitOLS.png){.fragment fig-align="center" height=600}

:::


## Multivariate analysis not a panacea

{{< video https://drhuyue.site:10002/sammo3182/video/lar_pork.mp4 title="Predicting Pork Price" height=600 loading="eager" allowfullscreen>}}


## Present your model efficiently

```{r}
#| fig-align: center
#| fig-height: 7
#| label: dotwhisker

m1 <- lm(mpg ~ wt + cyl + disp + gear, data = mtcars)
m2 <- update(m1, . ~ . + hp) # add another predictor
m3 <- update(m2, . ~ . + am) # and another

list(m1, m2, m3) |>
  dwplot(
    show_stats = TRUE,
    vline = geom_vline(
      xintercept = 0,
      colour = "grey60",
      linetype = 2
    )
  )
```



## Take-Home Points

:::: {.columns}

::: {.column width="50%"}

+ What: "Quantitative" analysis
  + How large is large?
    + LLN: 100
    + CLT: 1000
- Why
  - Why Not experiment?
  - What do we look for from the LNA?
    - Average and distribution
+ How 
    + Univariate
    + Bivariate
    + Multivariate

:::

::: {.column .fragment width="50%"}

*Bonus: If you wanna know more*

[["æ²»ç†æŠ€æœ¯ä¸“é¢˜ï¼šæ”¿æ²»æ•°æ®åˆ†æ" (70700173)]{.green} ğŸ˜‹]{.large}

:::

::::

## Reference

::: {#refs}
:::