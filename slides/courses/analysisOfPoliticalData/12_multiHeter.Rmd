---
title: "Multicollinearity & Heteroskedasticity"
subtitle: "Analysis of Political Data (70700173)"
author: "Yue Hu"
institute: "Political Science, Tsinghua University"
output:
  xaringan::moon_reader:
    self_contained: FALSE
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - style_ui.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

class: middle, center

Your favorite is coming......

--

.large[.red[Quiz Time!]]

You'll love it~

---

.center[
<video width="500" height="500" controls>
<source src="images/quiz.mp4" type="video/webm">
</video>
]

Pro and con?

---

## Overview

1. Collinearity
    + Violation
    + Diagnosis
    + Solution
1. Heteroscedasticity
    + Violation
    + Diagnosis
    + Solution
        + Sandwich Method
        + Fixed Effect
        + Multilevel Model

---

class: inverse, bottom

# Collinearity

---

class: small

## Perfect Collinearity

$$X_{1i} = \hat\delta_0 + \hat\delta_1X_{2i} + \hat r_{1i}$$

When cov(X<sub>1</sub>, X<sub>2</sub>) = 1, r<sub>1i</sub> = 0, then $\hat\beta_1 = \frac{\sum\hat r_{1i}y_i}{\hat r_{1i}^2}$ cannot be estimated.

--

e.g., Let's say in the PRF Y<sub>i</sub> = &beta;<sub>0</sub> + &beta;<sub>1</sub>X<sub>1</sub> + &beta;<sub>2</sub>X<sub>2</sub> + u, X<sub>2</sub> = 1 + 2X<sub>1</sub>, then

\begin{align}
Y_i =& \hat\beta_0 + \hat\beta_1X_{1} + \hat\beta_2X_{2} + \hat u \\
    =& \hat\beta_0 + \hat\beta_1X_{1} + \hat\beta_2(1 + 2X_1) + \hat u \\
    =& (\hat\beta_0 + \hat\beta_2) + (\hat\beta_1 + 2\hat\beta_2)X_1 + \hat u\\
\Rightarrow Y_i =& \hat\alpha_0 + \hat\alpha_1X_{1} + \hat u
\end{align}

PRF (&beta;) is non-identifiable. 

--

Problematic, but the .red[only] problematic.

???

PRF: population regression function

---

class: small

### Multicollinearity

**X**<sub>2</sub> is not a transformation of X<sub>1</sub>.

In this case, the estimators (**&beta;**) remain .red[unbiased], yet

\begin{align}
var(\hat\beta_1|X) =&\frac{\sigma^2\sum(X_{2i} - \bar X_2)^2}{\sum(X_{1i} - \bar X_1)^2\sum(X_{2i} - \bar X_2)^2 - [\sum(X_{1i} - \bar X_1)(X_{2i} - \bar X_2)]^2}\\
=&\frac{\sigma^2}{\sum(X_{1i} - \bar X_1)^2(1 - \beta_{12}^2)}, \text{where }\beta_{12} = cov(X_1, X_2).
\end{align}


As &beta;<sub>12</sub> increases, the variance also increases.

---

## Diagnosis

**Variance Inflation Factors (VIF, [1, +&infin;])**: A measure of how much the variance of the estimated coefficient &beta;<sub>k</sub> is "inflated" by the correlation among the predictor variables.

$$VIF = \frac{1}{1 - {R}^{2}_{k}} = \frac{1}{Tolerance}$$

--

1 means that no correlation among the k<sup>th</sup> predictor and the remaining predictor variables, and hence the variance of the coefficient is not inflated at all.

--

Rule of thumb: 4, 10    
(In most cases, you don't need this, why?)

???

Perfect then unidentified; but if your core X is insignificant.

---

## Solution(7......)

1. .red[No need for specific solution.]
1. Removed the highly correlated variables.
1. Remeasure the correlated variables (e.g., EFA, CFA, IRT)

---

class: inverse, bottom

# Heteroscedasticity

---

##  (Read after Me) Heteroscedasticity

Homo/heteroscedasticity &asymp;     Homo/heterogeneity (of variance)

???

Regression: Heteroscedasticity

ANOVA: Heterogeneity (of variance)

--

### Assumption

Homoskedasticity (var(u<sub>i</sub>|X) = &sigma;<sup>2</sup>)

--

### Violation

$var(u_i|X) = \sigma_i^2, \sigma_i^2 \neq \sigma_j^2, \forall i, j.$

---

background-image: url("images/heteroscedasticity1.png")
background-position: center
background-size: contain

---

background-image: url("images/heteroscedasticity2.webp")
background-position: center
background-size: contain

---

class: small

## Consequence<sup><img src="images/fsmrof.png" height="20"/></sup>

.red[Unbiased], expected consistent, but, again, inefficient

???

\begin{align}
var(\hat\beta|X) =& var(\frac{\sum(X_i - \bar X)(Y_i - \bar Y)}{\sum(X_i - \bar X)^2}|X)\\
                 =& var(\frac{\sum(X_i - \bar X)Y_i}{\sum(X_i - \bar X)^2}|X)\\
                 =& \frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}var(Y_i|X)\\
                 =& \frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}\sigma_i^2
\end{align}

\begin{align}
H_0: \frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}\sigma_i^2 =& \frac{\sigma^2}{\sum(X_i - \bar X)^2}\\
\sum(X_i - \bar X)^2\frac{\sigma_i^2}{\sigma^2} =& \sum(X_i - \bar X)^2
\end{align}

Only when $\frac{\sigma_i^2}{\sigma^2} = 1$, the estimate is efficient.

--

### Diagnosis

Park test, Goldfeld-Quandt test, Breusck-Pagan(-Godfreg) test, White test

**Goldfeld-Quandt test**:

1. Divide the domain of X to three parts, $\frac{n -c}{2}, c, \frac{n -c}{2}$. 
1. Regress Y on X as usual for each part.
1. Calculate $\sigma_L^2$ from part 1 and $\sigma_H^2 = \frac{\sum^n_{i = \frac{n + c}{2} + 1}}{\frac{n - c}{2} - k}\hat u_i^2$ from part 3.
1. Then test $H_0: \sigma_L^2 = \sigma_H^2$
    + Statistics: $\frac{\sigma_L^2}{\sigma_H^2}\sim F_{\frac{n - c - 2k}{2}, \frac{n - c - 2k}{2}}$


Theoretically, the smaller the c is, the more power the test has. However, too small c may also cause the difference between low and high groups difficult to be identified.

???

### Park test

Ocular-inspection test: Use the scalar points of $\hat u_i^2$ against $X_i$, or $\hat u_i^2$ against $Y_i$

Regress $ln(\hat u_i^2)$ on some $X_{ki}$:

$$ln(\hat u_i) = \hat\delta_0 + \hat\delta_1ln(X_{ki}) + \hat\gamma_i.$$

Do the t-test of coefficient in $ln(X_i)$: $H_0: \hat\delta_1 = 0.$


<img src="images/parkTest.png" height = 200 />


### Breusck-Pagan(-Godfreg) test

Assuming $\boldsymbol{\sigma_i^2} = \boldsymbol{X\delta} + \boldsymbol{v_i}$


Regress $\hat u_i^2$ on $X_1, X_2, \cdots, X_{k - 1}$, 

$H_0: \sigma_1 = \sigma_2 =\cdots=\sigma_{k - 1} = 0$


H_1: At least one of the &sigma;&ne;0, $\sim(\sigma_i = 0)$


Statistics $F_{k - 1, n - k}$

### White Test

1. Regress Y on X to get $\hat u_i^2$
1. Regress $\hat u_i^2$ on $\sum X_i + \sum X_i^2 + \sum X_iX_j$ or $\hat Y_i$.
1. Test this model against a model with no variables (F test).

---

class: small

## Solution

When do you need a solution? 

--

White covariance matrix, weighted least square, FGLS......

???

When your variable of interest isn't sig or you are interested in the heteroscedasticity

### White covariance matrix     
(a.k.a., Heteroscedasitic-consistent covariance matrix).

$E(u_i|X) = E\{\sqrt{[u_i - E(u_i)]^2}|X\}$

Since E(u<sub>i</sub>|X) = 0 by assumption, we can estimate $u_i$ with $\hat u_i^2$ and estimate $var(\hat\beta_1|X)$ with $\frac{\sum(X_i - \bar X)^2}{[\sum(X_i - \bar X)^2]^2}\hat u_i^2.$

The estimates turn out to be .red[biased], but converge asymptotically in n to the true distribution. (What does this imply?) 


Means it's fine when N is large enough.


### Weighted least square

Reduce the substantive effect of the change in X by $\frac{1}{\sigma_i}$, in order to squeeze the value towards the middle:

Given &sigma;<sub>i</sub> is known, 

\begin{align}
\frac{Y_i}{\sigma_i} =& \frac{\beta_0}{\sigma_i} + \frac{\beta_1}{\sigma_i}X_i + \frac{u_i}{\sigma_i};\\
\text{Then, } var(\frac{u_i}{\sigma_i}) =& \frac{1}{\sigma_i^2}var(u_i) = \frac{\sigma_i^2}{\sigma_i^2} = 1;\\
\Rightarrow Y_i^* =& \beta_0X^*_{0i} + \beta_1X^*_{1i} + u_i^*. 
\end{align}

The last equation is homoscedastistic.


However, in most cases, we don't know &sigma;<sub>i</sub>. 


So, usually we assume var(u<sub>i</sub>)&sim; X<sub>1i</sub>, i.e., var(u<sub>i</sub>) = &sigma;<sub>i</sub><sup>2</sup> = &sigma;<sup>2</sup>X<sub>i</sub> = h<sub>i</sub>&sigma;<sup>2</sup>.


Goal: var(u<sup>\*</sup><sub>i</sub>) = var(u<sup>\*</sup><sub>j</sub>), &forall; i,j. 

Then,

\begin{align}
u^*_i =& \frac{u_i}{\sqrt{h_i}}\\
var(\frac{u_i}{\sqrt{h_i}}) =& \frac{var(u_i)}{h_i} = \frac{h_i\sigma^2}{h_i} = \sigma^2\\
\Rightarrow Y_i^* =& \frac{Y_i}{\sqrt{h_i}}\\
            X_{0i}^* =& \frac{1}{\sqrt{h_i}}\\
            X_{1i}^* =& \frac{X_{1i}}{\sqrt{h_i}}, \text{ assuming } X_i\in R^+.
\end{align}


In practice, there are different ways to estimate the weight, one need to carefully choose the proper one.

--

### Feasible Generalized Linear Squares (FGLS)

\begin{align}
\boldsymbol{Y} =& \boldsymbol{X\beta} + \boldsymbol{u}; \\
var(\boldsymbol{u}) =& \Omega_{n\times n} = \left(
       \begin{array}{cccc}
       \sigma_1^2 & 0 & \cdots & 0\\
       0 & \sigma_2^2 & \cdots & 0\\
       \vdots & \vdots & \vdots & \vdots \\
       0 & 0 & \cdots & \sigma_n^2\\
       \end{array}\right)\\
\text{Then, } \boldsymbol{\hat\beta_{GLS}} =& (\boldsymbol{X'\Omega X})^{-1}(\boldsymbol{X'\Omega Y})\\
\text{Let }\boldsymbol{H}: \boldsymbol{\Omega} =& \boldsymbol{HH^{-1}},\\
\text{Then, } \boldsymbol{H^{-1}Y} =& \boldsymbol{H^{-1}X\beta} + \boldsymbol{H^{-1}u}
\end{align}

---

In this case

\begin{align}
\boldsymbol{H^{-1}u} =& \boldsymbol{H^{-1}(H^{-1})'}var(\boldsymbol{u})\\
                     =& (\boldsymbol{HH'})^{-1}\boldsymbol{\Omega} = \boldsymbol{\Omega}^{-1}\boldsymbol{\Omega} = \boldsymbol{I}.\\
var(\boldsymbol{\hat\beta_{GLS}}) =& (\boldsymbol{X'X})^{-1}(\boldsymbol{X'\Omega X})(\boldsymbol{X'X})^{-1}
\end{align}

* If there's no heteroscedasticity, $\boldsymbol{\Omega} = \sigma\boldsymbol{I}.$
* If there is heteroscedasticity and $\boldsymbol{\Omega}$ is known, then WLS (a special type of GLS).
* If $\boldsymbol{\Omega}$ is unknown, run $\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{u}$ to estimate $\boldsymbol{\Omega}$ with $\boldsymbol{\hat\Omega} = \boldsymbol{\hat u\hat u'}$

--

&zwnj;Hint: This method does not get SE, and also .red[biased] for small N.

---

class: small

## "Sandwich" Matrix

The above method is a type of "sandwich" estimator.

In a more general view, let $\boldsymbol{Q} = \boldsymbol{X'X}$ and 

\begin{align}
\boldsymbol{Q} = \left(
\begin{array}{cccc}
\sigma_1^2 & 0 & \cdots & 0\\
0 & \sigma_2^2 & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \sigma_n^2\\
\end{array}\right)
\end{align}

Then for regular OLS, $var(\beta) = \sigma^2(\boldsymbol{X'X})^{-1} = \sigma^2\boldsymbol{Q}^{-1}$.

But when heteroscedasticity occurs, $var(\boldsymbol{\beta}|X)\neq \sigma^2\boldsymbol{Q}^{-1}$.

Instead, let $\boldsymbol{G} = \boldsymbol{X'GX}$, then $var(\boldsymbol{\beta}|X) = \boldsymbol{Q^{-1}GQ}^{-1}.$

---

## Heterogeneity in TSCS Data

TSCS: Time-series cross-sectional data, a.k.a., "pooled data", "panel data"(wrong!)

Dealing with heterogeneity in TSCS data:

1. Robust standard error
1. Fixed effect
1. Multilevel modeling

---

class: small

## Robust Standard Error

### Sandwich SE

In the FGLS version, the "meat" can be identified only when T > N, and before this the autocorrelation has to be eliminated.

???

Autocorrelation: $cov(u_i, u_j|X_i, X_j) = 0, \forall i, j$

--

### Panel-corrected SE

An alternative version of the sandwich SE. 

&Omega; is clustered by time periods: $\hat\Omega_{ij} = \frac{\sum^T_{t = 1}\epsilon_{it}\epsilon_{jt}}{T}$

--

### Cluster SE

Adjust SE to account for correlations within clusters

???

### Within-between model

1. Run separate models in each group
1. Run an aggregate regression: 

Let $\bar Y_i = \frac{\sum^n_{i=1}Y_{it}}{n_i}, \bar X_i = \frac{\sum^n_{i=1}X_{it}}{n_i}, \bar u_i = \frac{\sum^n_{i=1}u_{it}}{n_i}$. Then, 

\begin{align}
\bar{Y_i} =& \beta_0 + \beta_1\bar{X_i} + \epsilon_i + u_i\\
Y_{it} - \bar{Y_i} =& (\beta_0 - \beta_0) + \beta_1(X_{it} - \bar{X_i}) + (u_{it} - u_i)\\
\hat{Y_i} =& \beta_1\hat{X_{it}} + \hat{u_{it}}, \text{(a.k.a., the within model)}\\
Y_{it} =& \beta_0 + \beta_1(X_{it} - \bar{X_i}) + \beta_2\bar{X_i} + u_{it}, \text{(a.k.a., the between model)}
\end{align}

---

class: small

## Fixed Effect

### Least Square with Dummy Variables (LSDV)

Type I: 

$$Y_{it} = \beta_1X_{it} + \alpha_i + u_{it},$$ in which &alpha; is unit-specific mean differences (unit fixed effect). 

--

Type II: 

$$Y_it = \sum^T_{t = 1}\delta_tD_{t_i} + \beta_1X_{it} + u_{it},$$ in which $\delta_tD_{t_i}$ is the fixed effect for time.

---

Concerns of LSDV

1. Adding a lot of variables (risk of using out of the d.f.)
1. Additional high multicollinearity
1. Losing time invariant variables
1. Inefficient estimates of FE on binary and dependent variables

???

For the last two concerns, considering using duration models.

---

Alternatives

1. Demeaning
1. Fixed effect Vector Decomposition (FEVD)

Con : 

1. Can't offer sufficient information (heterogeneous to what extent?).
1. Difficult to calculate substantive effects, e.g., first differences.

---

## Methodologists' Solution: Modeling It

a.k.a., *Pan-Ta*

Heteroscedasticity can be theoretically meaningful,, esp., when researchers focus on how the variance changes. 

e.g., Assuming var(u<sub>i</sub>) = &sigma;<sup>2</sup>e<sup>&gamma;<sub>1</sub>X<sub>2i</sub></sup>. 

If &gamma;<sub>1</sub> = 0, then the model is homoscedastistic; otherwise, one has to use MLE to test H<sub>0</sub>: &gamma;<sub>1</sub> = 0.

---

## Multilevel Modeling

a.k.a., hierarchical modeling

### (Two-Level) Random Intercept

\begin{align}
Y_{ij} = \beta_{0j}& + \beta_{1j}X_{ij} + \epsilon_{ij}, \epsilon_{ij}\sim N(0, \sigma^2)\\
       \beta_{0j}& = \gamma_{00} + \gamma_{01}Z_j + u_{0j}, u_{0j}\sim N(0, \tau^2)
\end{align}

Z is the group indicator.

--

**Intraclass correlation**: $\rho = \frac{\tau^2}{\sigma^2 + \tau^2}.$

???

Represent the similarity between individual and group level. 0 means no group effect.

---

### Random Slop

\begin{align}
Y_{ij} = &\beta_{0j} + \beta_{1j}X_{ij} + \epsilon_{ij}\\
       &\beta_{0j} = \gamma_{00} + \gamma_{01}Z_j + u_{0j}\\
       &\beta_{1j} = \gamma_{10} + u_{1j}.\\
\text{Assume}\left(
\begin{array}{c}
u_{0j}\\ u_{1j}\end{array}\right)&\sim BVN\left[\left(\begin{array}{c}
0\\0\end{array}\right), \left(\begin{array}{cc}
\tau_0^2 & \tau_0\tau_1\\ 
\tau_0\tau_1 & \tau_1^2
\end{array}\right)\right]
\end{align}

???

BVN: Bivariate Normal

The two variables in a bivariate normal are both are normally distributed, and they have a normal distribution when both are added together. Visually, the bivariate normal distribution is a three-dimensional bell curve.

---

Covariate Matrix: 

\begin{align}
\left(\begin{array}{cc}
\tau_0^2 & \tau_0\tau_1\\ 
\tau_0\tau_1 & \tau_1^2
\end{array}\right)
\end{align}

1. Identity: &tau;<sub>0</sub><sup>2</sup> = &tau;<sub>1</sub><sup>2</sup> = 1;
1. .navy[Exchangeable: &tau;<sub>0</sub><sup>2</sup>= &tau;<sub>1</sub><sup>2</sup>;]
1. Independent: &tau;<sub>0</sub><sup>2</sup>&tau;<sub></sub><sup>2</sup> = 0;
1. Unstructured: no specific relationships is assumed for the &tau;s.

---

## Wrap Up

1. Collinearity
    + Violation: Identification
    + Diagnosis: VIF or...just see the results
    + Solution: Hmm...
1. Heteroscedasticity
    + Violation: Homoscedasticity
    + Diagnosis: Park test, Goldfeld-Quandt test, Breusck-Pagan(-Godfreg) test, White test
    + Solution
        + Sandwich Method
        + Fixed Effect(!)
        + Multilevel Model(!!)

