---
title: "Model Specification"
subtitle: "Large N & Leeuwenhoek (70700173)"
author: "Yue Hu"
knitr: 
    opts_chunk: 
      echo: false
editor_options: 
  chunk_output_type: console
format: 
  revealjs:
    
    number-sections: true
    css: https://www.drhuyue.site/slides_gh/css/style_basic.css
    theme: ../../../css/goldenBlack.scss
    # logo: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_zzxx.png?inline=true
    slide-number: true
    filters: [appExclusion.lua] # not count appendices into page number
    incremental: true
    preview-links: true # open an iframe for a link
    link-external-newwindow: true
    self-contained: false
    chalkboard: true # allwoing chalk board B, notes canvas C
    # callout-icon: false
    show-slide-number: all # `speaker` only print in pdf, `all` shows all the time
    title-slide-attributes:
      data-background-image: https://gitlab.com/sammo3182/backup/raw/85b3c1ad4b459d7a9f901f124b936428eda5fcaf/logo_THPS.png?inline=true
      data-background-size: 250px   
      data-background-position: top 10% right 5%
revealjs-plugins:
  - spotlight
lightbox: 
  match: auto
  effect: fade
spotlight:
  size: 50
  presentingCursor: default
  toggleSpotlightOnMouseDown: false
  spotlightOnKeyPressAndHold: 73 # keycode for "i"
---


## Overview {.unnumbered}

```{r setup, include = FALSE}
if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse,
  patchwork,
  drhutools,
  ggeffects,
  dotwhisker
) 


# Functions preload
set.seed(313)

theme_set(
  theme_minimal(base_size = 18)
)

theme_update(
  plot.title = element_text(size = 18), 
  axis.title = element_text(size = 22), 
  axis.text = element_text(size = 18)
)

```

:::: {.columns}

::: {.column .nonincremental width="50%"}
1. [Nonstochastic X]{.golden} ("X is fixed"), and X has positive [noninfinite variance]{.golden} (&sigma;<sub>X</sub> > 0);
1. Correct [specification]{.golden};
1. [Linearity]{.red} in the parameter;
1. [Full rank/identification]{.red}
    - N > K; K = 2 for a simple OLS);
1. [Mean zero errors]{.red}
    - E(&epsilon;<sub>i</sub>|X<sub>i</sub>) = 0;
1. [Exogeneity]{.red}: No covariance between X<sub>i</sub> and &epsilon;<sub>i</sub>
    - E(X<sub>i</sub>&epsilon;<sub>i</sub>) = cov(x<sub>i</sub>, &epsilon;<sub>i</sub>) =0;
:::

::: {.column .nonincremental width="50%"}
7. [No autocorrelation/serial correlation]{.red}:  E(&epsilon;<sub>i</sub>, &epsilon;<sub>J</sub>|X<sub>i</sub>, X<sub>j</sub>) = cov(&epsilon;<sub>i</sub>, &epsilon;<sub>J</sub>|X<sub>i</sub>, X<sub>j</sub>) = 0, &forall; i, j);
1. [Normality]{.red}: $U|X \sim N(0, \sigma^2I).$
1. [Homoskedasticity/Spherical disturbances]{.green}: constant variance of &epsilon;<sub>i</sub>,  var(&epsilon;<sub>i</sub>|X) = &sigma;<sup>2</sup>;
1. No perfect [collinearity]{.green}: there are more than one X,  &nexists; X<sub>i</sub> s.t., X<sub>i</sub> = a + b&sum;<sub>j = 1</sub>b<sub>j</sub>X<sub>j</sub>

:::

::::

## Specification Issues {.unnumbered}

:::{style="text-align:center; margin-bottom: 2em"}
How can you *specify* things wrong?
:::

:::: {.columns}

::: {.column width="50%"}
**[Measurement]{.red} Issues**

- Proxy variable
- Measurement error
- Variable type
:::

::: {.column width="50%"}
**[Specification]{.red} Issues**

- Functional form
- Omitted variable

**[Interpretation]{.red} Issues**

- Nonlinear model

:::

::::


# Measurement Issues

## Why proxy variables?

:::{.nonincremental style="text-align:center"}
- Human capital &larr; years of education
- GDP &larr; economic well-being
- Voter turnout &larr; democratic health
- Partisaship &larr; ideology
:::


![](https://drhuyue.site:10002/sammo3182/figure/spec_proxy.png){.fragment fig-align="center" height=400}

## Proxy Variable in Statistics

True model: Y = &beta;<sub>0</sub>  + &beta;<sub>1</sub>[X<sub>1</sub>]{.red} + &beta;<sub>2</sub>[X<sub>2</sub>]{.red} + &beta;<sub>3</sub>X<sub>3</sub><sup>*</sup> + [&epsilon;]{.red}, &epsilon; ~ &Phi;(0, &sigma;<sub>&epsilon;</sub><sup>2</sup>).

- Unobservable: X<sub>3</sub><sup>*</sup>.
    - Observable X<sub>3</sub><sup>*</sup> = &delta;<sub>0</sub> + &delta;<sub>1</sub>[X<sub>3</sub>]{.blue} + [&nu;]{.blue}, &nu; ~ &Phi;(0, &sigma;<sub>&nu;</sub><sup>2</sup>).
- &rArr; Y = (&beta;<sub>0</sub> + &beta;<sub>3</sub>&delta;<sub>0</sub>) + &beta;<sub>1</sub>X<sub>1</sub> + &beta;<sub>2</sub>X<sub>2</sub> + &beta;<sub>3</sub>&delta;<sub>1</sub>X<sub>3</sub> + (&epsilon; + &beta;<sub>3</sub>&nu;).
    - *Assuming* cov(&epsilon;, &nu;) = 0, then
        1. E(&epsilon; + &beta;<sub>3</sub>&nu;|X) = 0;
        1. var(&epsilon; + &beta;<sub>3</sub>&nu;|X) = &beta;<sub>3</sub><sup>2</sup>&sigma;<sub>&nu;</sub><sup>2</sup> + &sigma;<sub>&epsilon;</sub><sup>2</sup>.

:::{.notes}
cov(u,v) = 0: u, v independent
:::


:::{.callout-important .fragment}
## Outcome of proxy variable

Only when cov([&epsilon;]{.red}, [X<sub>3</sub>]{.blue}) = cov([&nu;]{.blue}, [X<sub>1</sub>]{.red}) = cov([&nu;]{.blue}, [X<sub>2</sub>]{.red}) = 0, can the model produce *unbiased* estimates.
:::


## Measurement Error

:::{.callout-note}
## Relation with proxy variable

Proxy variable is a *source* of measurement error (random error, specifically).
:::

- Nonrandom error &rarr; systematic bias
- Random error in the outcome variable: Y = Y<sup>*</sup> + &epsilon;.
    - \begin{align}
&Y^* = \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + \epsilon;\\
&Y = \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + (\nu + \epsilon)
\end{align}
    - *Consequence*
        + &beta; Unbiased 
        + var(&beta;) &uarr;


## Measurement Error

- Random error in the explanatory: X = X<sup>*</sup> + &nu;. If E(&nu;X) = 0, then \begin{align}
Y =& \beta_0 + \beta_1(X_1 - \nu) + \epsilon,\\
  =& \beta_0 + \beta_1X_1 + (\epsilon - \beta_1\nu).
\end{align}

:::{.fragment}
\begin{align}
cov(\epsilon, X_1) =& E(\epsilon X_1) - E(\epsilon)E(X_1) = E(\epsilon X_1), \\
          =& E[\epsilon (X_1^* + v)] = E(\epsilon X_1^*) + E(\epsilon v) = \sigma_\epsilon^2\neq0.\\
cov(\epsilon - \beta_1\nu, X_1) =& cov(-\beta_1\nu, X_1) = -\beta_1cov(\nu, X_1) = -\beta_1\sigma_\nu^2.\\
\Rightarrow\ plim(\hat\beta_1) =& \beta_1 + \frac{cov(\epsilon - \beta_1\nu, X_1)}{var(X_1)},\\
                               =& \beta_1 + \frac{-\beta_1\sigma_\nu^2}{\sigma_{X_1}^2}
                               = \beta_1 + \frac{-\beta_1\sigma_\nu^2}{\sigma_{X_1}^{*2} + \sigma^2_\epsilon} = \frac{\sigma_{X_1}^{*2}}{\sigma_{X_1}^{*2} + \sigma^2_\nu}\beta_1.
\end{align}
:::


:::{.notes}
plim: probability limit operation, convergence in probability
:::

## Measurement Error

$$plim(\hat\beta_1) = \frac{\sigma_{X_1}^{*2}}{\sigma_{X_1}^{*2} + \sigma^2_\nu}\beta_1.$$

- Unbiased [only]{.red} when var(X<sub>1</sub>) = var(X<sub>1</sub><sup>\*</sup> + &nu;), i.e., &sigma;<sup>2</sup><sub>X<sub>1</sub></sub> = &sigma;X<sub>1</sub><sup>\*2</sup>+ &sigma;<sub>&nu;</sub><sup>2</sup>
- Consequence: $|\hat\beta_1| < \beta_1$, a.k.a., the *attenuation* bias. 
    + &beta;<sub>1</sub>: [underestimated]{.red};
    + Affecting the estimations of others in unknown ways .

## Bonus: Concerns of the variable type

:::{.callout-warning}
In OLS, there is [no requirement]{.red} (assumption) for Xs (but the *parameter*) to be linear. 
:::

* Indicator variable (binary)
    + $Y = \beta_0 + \beta_1X_i + \epsilon_i,$ where X is either male (0) or female(1)
        + &beta;<sub>0</sub>: E(Y|X = male);
        + &beta;<sub>1</sub>: E(Y|X = female) - E(Y|X = male).

* Nominal: 
    + Can't regress unless being broken up into indicators
    + e.g., A race variable: white, black, native
        + H<sub>0</sub>: &beta;<sub>black</sub> = 0, testing the difference between black and white
        + H<sub>0</sub>: &beta;<sub>black</sub> = &beta;<sub>native</sub> = 0, testing if the race has any effect.

:::{.notes}
In a regression, the information of the baseline is captured by the intercept
:::


## If no linearity requirement for Xs, why transformation

You saw log(GDP) or log(income), right? What does the [logarithm transformation]{.red} do? 

:::{.notes}
Some may say making X more linear? However, no assumption on X
:::


:::{.r-stack}

:::{.fragment}
```{r hist}
mammals <- MASS::mammals
mammals_l <- pivot_longer(mammals,
                          cols = everything(),
                          names_to = "variable",
                          values_to = "value")



ggplot(data = mammals_l, aes(x = value, fill = variable)) + 
    geom_histogram(position = "dodge") +
    scale_fill_gb(reverse = TRUE) + 
    theme(legend.position = "top") +
    ggtitle("Brain and Body Weights for 62 Species of Land Mammals")
```
:::

:::{.fragment}
```{r bodyBrain}
ggplot(mammals, aes(body, brain)) + geom_point()
```

:::

:::{.fragment}
```{r bodyLBrain}
ggplot(data = mammals, aes(x = body, y = brain)) +
  geom_point() +
  scale_x_log10() +
  xlab("Log(body)")
```
:::

:::{.fragment}
```{r bodyBrainL}
ggplot(data = mammals, aes(x = body, y = brain)) +
  geom_point() +
  scale_y_log10() +
  ylab("Log(brain)")
```
:::

:::{.fragment}
```{r bodyLBrainL}
ggplot(data = mammals, aes(x = body, y = brain)) +
  geom_point() +
  scale_x_log10() + 
    scale_y_log10() +
  xlab("Log(body)") + ylab("Log(brain)")
```
:::

:::

:::{.fragment style="text-align:center"}
The problem to solve: Violation of A3 ("[Linearity]{.red} in the parameter")
:::

## Model Fit with Logarithm Transformation

:::{.r-vstack}

:::{.fragment}
```{r fit}
#| fig-height: 3

fit <- vector(mode = "list")
fit$original <- lm(brain ~ body, data = mammals)
fit$log <- lm(log(brain) ~ log(body), data = mammals)

dwplot(fit) +
    scale_color_gb(reverse = TRUE) + 
    theme(legend.position = "top") + 
    ggtitle("Coefficient Estimation")
```
:::

:::{.fragment}
```{r residual}
#| fig-height: 3

df_residual <- map_dfc(fit, ~.$residuals) %>% 
    pivot_longer(cols = everything(), names_to = "model", values_to = "residual")

ggplot(df_residual, aes(x = residual)) +
    geom_density() + 
    facet_wrap(~ model, scales = "free") + 
    ggtitle("Model Residuals")
```
:::

:::


## So, when to use Logrithm Transformation

:::{.fragment}
Critical criterion: Skewness
:::


:::{.fragment}
Rule of thumb:

- |Skewness| < 0.5, the distribution is approximately symmetric (0 &hArr; perfect symmetric)
- |Skewness| < 1, moderately skewed
- [|Skewness| > 1, highly skewed]{.red}
:::


:::{.fragment}
Example data:

```{r skew}
map(mammals, e1071::skewness)
```
:::


# Misspecification

## Problem Types

1. Functional form
1. Omitted variable

## Misspecification in the Functional Form

When there are two ways to specify the model:

\begin{align}
Y =& f_1(X_1, X_2, X_3) =\beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \epsilon, \\
Y =& f_2(X_1, X_2, X_3) =\beta_4X_1^2 + \beta_5X_2^3 + \beta_6X_3^4 + \epsilon.
\end{align}

[*Which specification is correct?*]{style="text-align:center"}



:::: {.columns}

::: {.column width="50%"}

:::{.fragment}
**Test out**: 

&beta;<sub>1</sub> in Y = f<sub>1</sub>(X) + f<sub>2</sub>(X) + &epsilon;

:::{.fragment}
*Conduct a joint test*:

H<sub>0</sub>: &beta;<sub>1</sub> = &beta;<sub>2</sub> = &beta;<sub>3</sub> = 0 or 

&beta;<sub>4</sub> = &beta;<sub>5</sub> = &beta;<sub>6</sub> = 0
:::

:::

:::{.notes}
Only one true model!

Let's assume f<sub>1</sub> is the true model:

$$var(\hat\beta_1) = \frac{\sigma^2}{(X_1 - \bar X_1)^2(1 - \rho_{12})}.$$

&rho;<sub>12</sub>: Correlations between X<sub>1</sub> and X<sub>2</sub>, such as between X and X<sup>2</sup>. So, when &rho;<sub>12</sub> &uarr; variance is inflated when &rho;<sub>12</sub> &uarr;
:::
:::

::: {.column .fragment width="50%"}
Statistics: Davidson-Mackinnon test

1. Run $Y = f_2(X) + \epsilon$ and calculate the expected value $\hat Y = E[Y|f_2(X)]$
1. Run $Y = f_1(X) + \theta\hat Y + \epsilon$, and test if &theta; = 0.
:::

::::


## Omitted Variable{.smaller}

\begin{align}
True: Y_i =& \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \epsilon_i;\\
Specified: Y_i =& \tilde\beta_0 + \tilde\beta_1X_{1i} + \tilde \epsilon_i,
\end{align}

How does $\tilde\beta_1$ compare to $\beta_1$?

:::{.fragment}
\begin{align}
Y_i =& \beta_0 + \beta_1X_{1} + \beta_2X_{2} + \epsilon_i,\\
Y_i -\bar Y =& \beta_1(X_{1} - \bar X_1) + \beta_2(X_{2} - \bar X_2) + (\epsilon_i - \bar \epsilon),\\
(Y_i -\bar Y)(X_1 - \bar X_1) =& \beta_1(X_{1} - \bar X_1)^2 + \beta_2(X_{2} - \bar X_2)(X_1 - \bar X_1)\\ 
            &+ (\epsilon_i - \bar \epsilon)(X_1 - \bar X_1),\\
\frac{(Y_i -\bar Y)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} =& \beta_1\frac{\sum(X_{1} - \bar X_1)^2}{\sum(X_1 - \bar X_1)^2} + \beta_2\frac{\sum(X_{2} - \bar X_2)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} + \frac{\sum \epsilon_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}.\\
\text{That is, } \tilde\beta_1 =& \beta_1 + \beta_2\hat\delta_1 + \frac{\sum \epsilon_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2},\\
\Rightarrow E(\tilde\beta_1|X_1) =& E(\beta_1|X_1) + E(\beta_2\hat\delta_1|X_1) + E[\frac{\sum \epsilon_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}|X_1],\\
                     =& \hat\beta_1 + \hat\beta_2E(\hat\delta_1|X_1) + \frac{\sum(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}E(\epsilon_i|X_1),\\
                     =& \hat\beta_1 + \hat\beta_2\delta_1.
\end{align}

:::


## Biased how

$$E(\tilde\beta_1|X_1) = \hat\beta_1 + \hat\beta_2\delta_1,$$ in which $\hat\delta_1$ is the regression coefficient of $X_2$ on $X_1$ ( $X_1 = \delta_0 + \delta_1X_2 + r$).

- $\tilde\beta_1$: [Biased]{.red}, unless X<sub>2</sub> is an irrelevant variable.
    - Even if &delta;<sub>1</sub> = 0, $X_1 = \delta_0 + r$, the model may increase the risk of Type I error

:::{.notes}
False positive
:::


:::{.notes}
How About the Residual?

$$Y_i = \beta_0 + \beta_1X_{1} + \epsilon,$$

$$\epsilon = \beta_2X_{2} + \epsilon_i.$$


$E(\epsilon|X_1) = E(\beta_2X_{2} + \epsilon_i) = \beta\mu_2\neq0$

+ Against the assumption.
  

$var(\epsilon|X_1) = var(\beta_2X_{2} + \epsilon_i|X_1)= \beta_2^2\sigma_2^2 + \sigma_u^2 + 2\beta_2cov(X2, u) = \beta_2^2\sigma_2^2 + \sigma_u^2$

+ All values are fixed, that is, still homoscedasitic.  

$E(X_1\epsilon) = \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}],$ 

+ When cov(X<sub>1</sub>, X<sub>2</sub>)&ne;0 &rArr; Increase the difficulty to isolate X from u.
    
$E(\epsilon_i\epsilon_j|X_i)\neq 0$

\begin{align}
E(\epsilon_i\epsilon_j) - E(\epsilon_i)E(\epsilon_j) =& E[(\beta_2X_{2i} + \epsilon_i)(\beta_2X_{2j} + \epsilon_j)] - \beta_2^2\mu^2_{X_2},\\
=& E(\beta_2^2X_{2i}X_{2j} + \beta_2^2X_{2i}u_{j} + \beta_2^2X_{2j}u_{i} + \epsilon_i\epsilon_j) - \beta_2^2\mu^2_{X_2},\\
=& \beta_2^2E(X_{2i}X_{2j}) - \beta_2^2\mu^2_{X_2},\\
=& \beta_2^2cov(X_{2i}, X_{2j}) - \beta_2^2\mu^2_{X_2} + \beta_2^2\mu^2_{X_2},\\
=& \beta_2^2cov(X_{2i}, X_{2j}).
\end{align}


\begin{align}
E(X_1\epsilon) =& E[X_1(\beta_2X_2 + u)], \\
=& E(X_1\beta_2X_2) + E(X_1u),\\
=& \beta_2E(X_1X_2),\\
=& \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}].
\end{align}

+ When cov(X<sub>1</sub>, X<sub>2</sub>) &ne; 0, E(X<sub>1</sub>&epsilon;) &ne; 0.

:::

:::{.fragment}
Examples: 

- Economic development and democracy
- Election and democracy
- Media freedom and political accountability
:::

# Misinterpretation

## Linear modeling of nonlinear relationship

:::{.r-stack}
![](https://drhuyue.site:10002/sammo3182/figure/spec_nonlinearity.jpg){fig-align="center" height=500}

:::{.fragment}
```{r nonlinear}
#| layout-ncol: 2
#| layout-align: center

df_nl1 <- tibble(x = seq(0, 3,length.out = 1000),
                y = 4*x - x^2 + rnorm(1000,0,sd = 1))

ggplot(data = df_nl1, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u)) + 
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold"),
  legend.text = element_text(size = 20)
  )

df_nl2 <- tibble(x = seq(0, 10,length.out = 1000),
                 y = 3 + log(x) + rnorm(1000,0,sd = 1))

ggplot(data = df_nl2, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0] + beta[1]*ln(X) + u)) + 
  theme(
  plot.title = element_text(size = 28),
  axis.text=element_text(size=20),
  axis.title=element_text(size=20,face="bold"),
  legend.text = element_text(size = 20)
  )
```
:::
:::

:::{.fragment style="text-align:center; margin-top: 1em"}
Believe or not, still unbiased & efficient [linear]{.red} estimates, but what does &beta;<sub>1</sub> mean? 
:::


:::{.notes}
Linear combination means adding together
:::

## One way out

:::{style="text-align:center"}
Marginal Effectsüí´
:::


Discrete:

$$Pr(Y|x = X_{n + 1}) - Pr(Y|x = X_n)$$

Continuous: 

$$\lim_{\Delta x\to0} \frac{ f(x + \Delta x) - f(x)}{\Delta x}$$

:::{.fragment}
That's to say, every value in X (n > 1) has a marginal effect (&Delta;<sub>x</sub>). Which one should we use?
:::

## Marginal Effects in Types

:::: {.columns}

::: {.column .fragment width="30%"}
*Average Marginal Effect (AME)*

1. Calculate the marginal effect of each variable x for [each observation]{.red}.
1. Calculate the average.
:::

::: {.column .fragment width="30%"}
*Marginal Effect at the Mean (MEM)*

Calculate the marginal effect of each variable x for [each's mean value]{.red}.

:::

::: {.column .fragment width="40%"}

*Marginal Effect at Representative Values (MER)* üåü

Calculate the marginal effect of each variable x for [value(s) of interest]{.red}.

![](https://drhuyue.site:10002/sammo3182/figure/spec_interaction2.jpg){fig-align="center" height=350}

:::

::::


## Hypothesis Testing of A Nonlinear Model {auto-animate=true}

$$e.g., Y = \beta_0 + \beta_1X + \beta_2X^2 + u.$$

- Margins: $\frac{\partial Y}{\partial X} = \beta_1 + 2\beta_2X$
- Let &alpha; = 0.05, H<sub>0</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X = 0;
    - The average acceleration is zero

:::{.fragment}
Statistics: 

\begin{align}
\frac{\beta_1 + 2\beta_2X - 0}{SE(\beta_1 + 2\beta_2X)}\sim& t_{n - 3}.\\
SE(\beta_1 + 2\beta_2X) =& \sqrt{var(\beta_1 + 2\beta_2X)}, \\
=& \sqrt{var(\beta_1) + 4X^2var(\beta_2) + 4Xcov(\hat\beta_1,\hat\beta_2)}.
\end{align}
:::



## Interpretation {auto-animate=true}

Statistics: 

\begin{align}
\frac{\beta_1 + 2\beta_2X - 0}{SE(\beta_1 + 2\beta_2X)}\sim& t_{n - 3}.\\
\end{align}

:::{.callout-warning .fragment}
## Interpretation issue

Can't simply say whether the null hypothesis is rejected, because there may [not be a coherent effect]{.red} over the entire domain of X due to the nonlinearity. 
:::


- Way-out: "In the range from a to b, the hypothesis can be rejected."
    * Max - min/First difference
    * Marginal effects across values
    * Just plot it. ([Do this, cool kids!]{.green})

## Take-home point {.unnumbered}

![](https://drhuyue.site:10002/sammo3182/figure/spec_mindmap.png){.r-stretch}

# Appendix {.appendix .unnumbered}

## Stretch {.unnumbered}

{{< video https://drhuyue.site:10002/sammo3182/video/relax.mp4 title="YoyoÂ≠ô‰Ω≥Á•∫:ÊãØÊïë‰πÖÂùêÂÖö‰ΩìÊÄÅÔºÅÂäûÂÖ¨Èó¥ÈöôÁöÑ5ÂàÜÈíüÊãâ‰º∏ÔºåÈáçÊñ∞Êå∫ÊãîËµ∑Êù•ÔΩûÔºÅ" height=600 preload="auto" controls allowfullscreen>}}

:::{.notes}
https://www.bilibili.com/video/BV1E54y1r76o/?buvid=Y045340D2FD9FA3A46419EEFE4578279ECBD&from_spmid=main.space-search.0.0&is_story_h5=false&mid=7RnjBONLRMus4FQZFBWD2g%3D%3D&p=2&plat_id=114&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=79FE08C9-F2B1-4C18-A16C-B31179817B42&share_source=WEIXIN&share_tag=s_i&timestamp=1728732278&unique_k=skSWw7j&up_id=390316092&vd_source=f38aeefd0d38cecba9017eeee43e71c8
:::

## Meditation {.unnumbered}

:::{style="text-align:center; margin-top: 2em"}
![ÊùæËå∏ÁöÑ‰∏ñÁïåÔºö5ÂàÜÈíüÊ≠£ÂøµÂÜ•ÊÉ≥-Ëá™‰ø°‰πãÂøÉ](https://drhuyue.site:10002/sammo3182/figure/ci_songrong.jpg){fig-align="center" height=400}

<audio controls preload="auto" src="https://drhuyue.site:10002/sammo3182/video/meditation.m4a" width=900></audio>
:::


:::{.notes}
https://www.xiaoyuzhoufm.com/episode/65b752ed0bef6c207457f51b?s=eyJ1IjogIjYwNDA1OGJiZTBmNWU3MjNiYmY3MjZiZSJ9
:::