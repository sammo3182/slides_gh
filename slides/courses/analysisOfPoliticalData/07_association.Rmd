---
title: "Association Analysis"
subtitle: "Large N & Leeuwenhoek (70700173)"
author: "Yue Hu"
institution: "Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: FALSE
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - style_ui.css
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: 16:9
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  tidyverse, icons, xaringanExtra
) 


use_xaringan_extra(c("tile_view", # O
                     "broadcast",
                     "panelset",
                     "tachyons",
                     "fit_screen"))
use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = FALSE  #<<
)


# Functions preload
set.seed(313)

theme_set(theme_minimal())
```

## Overview

.pull-left[

### Relations of Variable

+ Covariance
+ Correlation
+ Non-linear comparison

]

.pull-right[

### Nonparametric Tests

+ Sign & ranking tests

### Treatments vs. Control

+ ANOVA
+ Nonparametric version

]

--

.center[Please pay attention to the .red[ASSUMPTIONS]!!!]

---

class: inverse, bottom

# Relations of Variables

---

## Associations: Relations among Two Variables

1. Covariance (dynamic)
1. Correlations (static)
    + Pearson's r
    + Kendall's &tau;
    + Spearman's &rho;


???

covariance measures how one change then the other

correlation measures if see the other high when one high

Covariance is the base of correlation

---

## Covariance

Remember Variance?

.center[&sigma;<sup>2</sup><sub>X</sub> = &sum;(X - &mu;<sub>X</sub>)<sup>2</sup>p(x) = &sum;(X - &mu;<sub>X</sub>).red[(X - &mu;<sub>X</sub>)]p(x)].

--

Then covariance is:

.center[&sigma;<sub>X, Y</sub> = &sum;(X - &mu;<sub>X</sub>).red[(Y - &mu;<sub>Y</sub>)]p(x, y)].

--

In other words, covariance is literally .red[co]-variance.


---

## Correlation (Pearson's r)

For a population

$$\rho_{X,Y} = \frac{\sum(X - \mu_X)(Y - \mu_Y)p(x,y)}{\sqrt{\sum(X - \mu_X)^2p(X)}\sqrt{\sum(Y - \mu_Y)^2p(Y)}} = \frac{\sigma_{X, Y}}{\sigma_X\sigma_Y}.$$

--

For a sample

$$r_{XY}=\frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}.$$

--

Multiple regressions: $partial\ r = \frac{b/SE}{\sqrt{(b/SE)^2 + (n - k -1)}}$, a.k.a., partial correlation.

---

class: middle

.center[
## .red[ASSUMPTION]

1\. Continuous data   
2\. Linear Relationship
]

--

## Properties 

1. &rho;&in; [-1, 1], 0 independent.
1. Greater value indicates stronger .red[linear] relationship.
1. Parametric test<sup>*</sup>
1. .red[Not robust] to skewed data and outliers.

.footnote[\* Which means assuming normal distribution, linearity, and homoscedasticity.]

---

class: bottom, inverse

## When the Assumption Does Not Hold

# Non-Parametric Tests

---

.center[
## .red[ASSUMPTION]

~~1\. Continuous data~~ Ordinal/norminal data;   
~~2\. Linear Relationship~~ Monotonic relationship

Alternative: &tau; & &rho;
]

--

## Terminology: Census of Ranking Pairs

For any pair of observations x<sub>i</sub>, y<sub>i</sub> and x<sub>j</sub>, y<sub>j</sub>, i < j,

+ Concordant pairs: x<sub>i</sub> > x<sub>j</sub> & y<sub>i</sub> > y<sub>j</sub> | x<sub>i</sub> < x<sub>j</sub> & y<sub>i</sub> < y<sub>j</sub>;

+ Discordant pairs: x<sub>i</sub> > x<sub>j</sub> & y<sub>i</sub> < y<sub>j</sub> | x<sub>i</sub> < x<sub>j</sub> & y<sub>i</sub> > y<sub>j</sub>;

+ Tied pairs: x<sub>i</sub> = x<sub>j</sub> & y<sub>i</sub> = y<sub>j</sub>;

???

tie on x or tie on y are also not concordant or discordant

---

## Kendall's &tau;

.left-column[
### &tau;<sub>A</sub>
]

.right-column[
.pull-left[
$$\tau_{A}=\frac{n_c-n_d}{n_c+n_d}=\frac{n_c-n_d}{\binom{n}{2}},$$

* n<sub>c</sub>: concordant pair.
* n<sub>d</sub>: discordant pair.
]

.pull-right[


*Properties*：

1. &tau;<sub>XY</sub>, independent 0.
1. "Non-parametric"<sup>*</sup>

]
]

.footnote[
\* In fact, still based on population parameters, so one can do significant test by $z={3(n_{c}-n_{d}) \over {\sqrt {n(n-1)(2n+5)/2}}}$.
]

--

.right-column[

.center[Problems?]

1. How to account ties?
1. What if the numbers of values in two variables are not identical?

]

???

https://statistics.laerd.com/spss-tutorials/kendalls-tau-b-using-spss-statistics.php
https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient

---

## Kendall's &tau;

.left-column[
### &tau;<sub>A</sub>
### &tau;<sub>B</sub>
]

.right-column[

$$\tau_{B}={\frac {n_{c}-n_{d}}{\sqrt {(n_{0}-n_{1})(n_{0}-n_{2})}}}$$
where

.small[
\begin{aligned}
n_{0}&=n(n-1)/2\\n_{1}&=\sum _{i}t_{i}(t_{i}-1)/2\\
n_{2}&=\sum _{j}u_{j}(u_{j}-1)/2\\n_{c}&={\text{Number of concordant pairs}}\\
n_{d}&={\text{Number of discordant pairs}}\\
t_{i}&={\text{Number of tied values in the }}i^{\text{th}}{\text{ group of ties for the first quantity}}\\u_{j}&={\text{Number of tied values in the }}j^{\text{th}}{\text{ group of ties for the second quantity}}
\end{aligned}
]

]

???

When the scale of the two variables are different, for example, X can take integer values from 1 to 10 while Y can take integer values from 1 to 20. 

---

## Kendall's &tau;

.left-column[
### &tau;<sub>A</sub>
### &tau;<sub>B</sub>
### &tau;<sub>C</sub>
]

.right-column[

$$\tau _{C}={\frac {2(n_{c}-n_{d})}{n^{2}{\frac {(m-1)}{m}}}}$$
where

\begin{aligned}
n_{c}&={\text{Number of concordant pairs}},\\
n_{d}&={\text{Number of discordant pairs}},\\
m&=\min(r,c),\\
r&={\text{Number of rows}},\\
c&={\text{Number of columns}}.
\end{aligned}
]

---

## Alternative: Spearman &rho;

* "Non-parametric" version of Pearson R
* Ranking correlation 

$$\rho _{\operatorname {rg} _{X},\operatorname {rg} _{Y}}={\frac {\operatorname {cov} (\operatorname {rg} _{X},\operatorname {rg} _{Y})}{\sigma _{\operatorname {rg} _{X}}\sigma _{\operatorname {rg} _{Y}}}}.$$

$rg$: The rank of the variable

--

When all n ranks are distinct integers (no tie):

$$\rho_{s}={1-{\frac {6\sum d_{i}^{2}}{n(n^{2}-1)}}},$$ where $d_{i}=\operatorname {rg} (X_{i})-\operatorname {rg} (Y_{i}).$

???

https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php

---

class: center, middle

## r vs. &rho; vs. &tau;

r vs. &tau; & &rho;:

+ r: Linear relationship;
+ &tau; & &rho;: Monotonic relationship.

--

&rho; & &tau;:

+ Non-parametric;
+ Useful when continuity is *violated*;
+ Working for ordinal. 

--

&rho; vs. &tau;: 

+ &tau; is more robust than &rho;;
+ &rho; has less cost than &tau;.

---

## Nominal Variables

\begin{align}
\chi^2 =& \sum_i\sum_j \frac{(Observed - Expected)^2}{Expected}\\
       =& \sum_{i,j}\frac{(n_{i,j}-\frac{n_{i\cdot}n_{\cdot j}}{n})^{2}}{\frac{n_{i\cdot}n_{\cdot j}}{n}}
\end{align}

???

nonparametric,

(What's n<sub>i&bullet;</sub>?)

--

E.g., The partisan distribution of American fathers and sons. How do we know if father's partisanship affect their sons? 


| Father/Son 	| D  	| R  	| I  	| Total 	|
|------------	|----	|----	|----	|-------	|
| D          	| 45 	| 5  	| 10 	| 60    	|
| R          	| 2  	| 23 	| 5  	| 30    	|
| I          	| 3  	| 2  	| 5  	| 10    	|
| Total      	| 50 	| 30 	| 20 	| 100   	|


---

class: small

| Father/Son 	| D  	| R  	| I  	| Total 	|
|------------	|----	|----	|----	|-------	|
| D          	| 45 	| 5  	| 10 	| 60    	|
| R          	| 2  	| 23 	| 5  	| 30    	|
| I          	| 3  	| 2  	| 5  	| 10    	|
| Total      	| 50 	| 30 	| 20 	| 100   	|



Let's set up a hypothesis test: If there's an influence, we may expect

$H_0:$ Sons' party ID has no relation with their fathers' party ID; formally, $\pi_{ij} = \pi_i \pi_j$, for all i, j.

&alpha; = 0.05.

--

Then, let's calculate the expectations, e.g., E(D<sub>f</sub>, D<sub>s</sub>) = 50/100 &times; 60/100 &times; 100 = 30. (What's this?)

$$\chi^2 = \frac{(45 - 30)^2}{30} + \frac{(5 - 18)^2}{18} + \dots + \frac{(5 - 2)^2}{2}\approx 56.07$$. 

d.f.: (r - 1)(c - 1) = (3 - 1)(3 - 1) = 4.

--

$\chi^2_{critical} = \chi^2_{0.05, 4} =$ `r round(qchisq(.975, df = 4), digits = 4)`< $\chi^2_{observed}$. H<sub>0</sub> is rejected.

---

## Limitation of &chi;<sup>2</sup>

1. When sample is too small and/or having too many missing data, the distribution might be different from $\chi^2$
2. When N gets large, $\chi^2$ also increase (esp. over 100,000)

--

Solution: &Phi; and Cramer's V

* $\Phi = \sqrt{\frac{\chi^2}{n}}\in[0, 1]$
    + 2&times;2 table
    + 0 means no association
    + 1 means perfect association
* Cramer's V
    + Beyond a 2*2 table (when $\Phi$ > 1).
    + $V = \sqrt{\frac{\chi^2}{n\times min_{r-1, c-1}}}$.

---

## Association for Matched Samples: Ranking

| Observation | Test 1        | Test 2        | &Delta;                       |
|-------------|---------------|---------------|-------------------------------|
| 1           | X<sub>1</sub> | Y<sub>1</sub> | X<sub>1</sub> - Y<sub>1</sub> |
| 2           | X<sub>2</sub> | Y<sub>2</sub> | X<sub>2</sub> - Y<sub>2</sub> |
| 3           | X<sub>3</sub> | Y<sub>3</sub> | X<sub>3</sub> - Y<sub>3</sub> |

Goal: whether test 1 and test 2 yield identical results.

--

+ \+: X<sub>i</sub> > Y<sub>i</sub>;
+ \-: X<sub>i</sub> < Y<sub>i</sub>;
+ =: X<sub>i</sub> = Y<sub>i</sub>;

--

*Expectation*: if test 1 and 2 are identically distributed, the probability of &Delta; to be + or - should follow the binomial distribution. That is, *H<sub>0</sub>: Pr(+) = Pr(-) = 0.5*.

---

## Sign Test

.pull-left[
| Observation | Test 1 | Test 2 | &Delta; | Sign |
|-------------|--------|--------|---------|------|
| 1           | 37     | 40     | −3      | −    |
| 2           | 72     | 73     | −1      | −    |
| 3           | 57     | 59     | −2      | −    |
| 4           | 44     | 43     | 1       | +    |
| 5           | 43     | 51     | −8      | −    |
| 6           | 64     | 67     | −3      | −    |
| 7           | 55     | 61     | −6      | −    |
| 8           | 65     | 74     | −9      | −    |
]

.pull-right[
H<sub>0</sub>: Pr(+) = Pr(-) = 0.5;   
H<sub>1</sub>: Pr(+) < Pr(-).

&alpha; = 0.05


.small[
\begin{align}
Pr(n_-\geq 7) =& Pr(n_- = 7) + Pr(n_- = 8)\\
=& {8 \choose 7}(0.5)^7(0.5)^1 + {8 \choose 8}(0.5)^8\\
=& 0.03125 + 0.0039 = 0.03516 < 0.05.
\end{align}
]
]

---

## Procedure of Sign Test

.pull-left[
1. Calculate the &Delta; (or d); 
1. Giving the sign accordign to d, if d = 0, ignore the observation;
1. Calculate n<sub>-</sub> and n<sub>+</sub>, n = n<sub>-</sub> + n<sub>+</sub>
1. Set H<sub>0</sub> and H<sub>1</sub>, and &alpha;
1. Calculate Pr(+/-)
    + $Pr(+) = \sum^{n_-}_{i = 0}Pr(n_+ + i)$;
    + $Pr(-) = \sum^{n_+}_{j = 0}Pr(n_- + j)$;
1. Compare the Pr(+/-) with &alpha;.
]

--

.pull-right[
Hint:

1. Work on matched and nonmatched samples.
1. It is essentially a binomial test (again, "non-parametric" test).

```{r signTest, echo = TRUE}
binom.test(7,8, p = .5, alternative = "greater", conf.level = 0.95)
```

]


???

Although claiming non-parametirc, many of them calculate the significance according to populational parameters.

---

## Normal Approximation of Sign Test

(Often used when n > 10, why? Well...)

--

.pull-left[
Using normal distribution to estimate sign tests:

&mu; = np = n/2; &sigma;<sup>2</sup> = np(1 - p) = n/4.

$$Z = \frac{x' - \mu}{\sigma} = \frac{2x' - n}{\sqrt{n}},$$ where<sup>*</sup> x'=
\begin{cases}
x + 0.5, if x > \frac{n}{2}\\
x- 0.5, if x < \frac{n}{2}
\end{cases}
]

.footnote[\* &pm;0.5 approximate to a continuous variable as an expedient, a.k.a., normal approximation.]

--

.pull-right[
e.g., n = 8, then $Pr(n_-\geq 7)$ ?

\begin{align}
Z =& \frac{x' - \mu}{\sigma} = \frac{2x' - n}{\sqrt{n}},\\
=& \frac{2(7 - 0.5) - 8}{\sqrt{8}} = 1.77.
\end{align}

Then, Pr(Z &geq; 1.77) &approx; 0.038 < 0.05.

]

---

## Advanced Version of Sign Test

Problems of sign test? 

&rArr; Wilcoxon signed-rank test

???

Sign test ony identify the sign but not the magnitude, insufficiently using the data.


--

1. Calculate the d and .red[|d|];
1. Sort these quantities, and record their rankings as R<sub>i</sub> so that 0 < |d<sub>R1</sub>| < |d<sub>R2</sub>| <...< |d<sub>Rn</sub>|.
1. Let sign function sgn(d) = 1 if d > 0, sgn(d) = -1, if d < 0, then calculate the signed-rank sum $T = \sum^n_{i=1}sgn(d_i)R_i.$
    + $T^{+}=\sum _{d_{i}>0}R_{i},$
    + $T^{-}=\sum _{d_{i}<0}R_{i}.$

1. Compare the Pr(T) with &alpha;.

---

.pull-left[
| Observation | Test 1 | Test 2 | &Delta; | Ranking |
|-------------|--------|--------|---------|------|
| 1           | 37     | 40     | −3      | .red[4]    |
| 2           | 72     | 73     | −1      | .red[1]    |
| 3           | 57     | 59     | −2      | 3    |
| 4           | 44     | 43     | 1       | .red[1]    |
| 5           | 43     | 51     | −8      | 7    |
| 6           | 64     | 67     | −3      | .red[4]    |
| 7           | 55     | 61     | −6      | 6    |
| 8           | 65     | 74     | −9      | 8    |
]

--

.pull-right[
```{r signedRank_tb}
tb_sr <- tibble::tribble(
  ~Test_1, ~Test_2,
       37,      40,
       72,      73,
       57,      59,
       44,      43,
       43,      51,
       64,      67,
       55,      61,
       65,      74
  )
```

```{r signedRank_test, echo=TRUE, warning=TRUE}
wilcox.test(tb_sr$Test_1, tb_sr$Test_2, paired = TRUE)
```

]

--

+ When tie happens, using Monte Carlo conditional p-value (`coin::wilcox_test`) to conduct outcomes .
+ When n > 25, a normal approximation can be also conducted.

---

## Ranking Test for Nonmatched Samples

Given samples S1 and S2,

.left-column[
### Rank-sum test
]

.right-column[
+ Combining S1 and S2 and rank the pooled observations;
+ Adding the ranking quantities (i.e., Rs in signed-rank test);
+ Comparing the sums.
]

---

## Ranking Test for Nonmatched Samples

Given samples S1 and S2,

.left-column[
### Rank-sum test
### Wald-Wolfowitz Runs Test
]

.right-column[

Meaning: Values of sorted S1 and S2 randomly occure?

Procedure: 

1. Combine S1 and S2
1. Sorting the quantities
1. Giving those from S1 as 0 and those from S2 1 within their ranking order.
1. Testing whether the 0/1 occure stocastically.

When n<sub>1</sub> &geq; 20, n<sub>2</sub> &geq; 20, one can use normal approximation

]

???

Runs test means to test whether an item comes from population 1 and population 2 is random or not

---

## Runs

| Trial | 1  | 2  | 3  | 4  | 5  | 6  | 7  | 8  |
|-------|----|----|----|----|----|----|----|----|
| A     | 9  | 22 | 65 | 34 | 17 | 4  | 31 | 28 |
| B     | 58 | 53 | 26 | 11 | 52 | 51 | 8  |    |

--

.center[&dArr;]

| 4 | 8 | 9 | 11 | 17 | 22 | 26 | 28 | 31 | 34 | 51 | 52 | 53 | 58 | 64 |
|---|---|---|----|----|----|----|----|----|----|----|----|----|----|----|
| 0 | 1 | 0 | 1  | 0  | 0  | 1  | 0  | 0  | 0  | 1  | 1  | 1  | 1  | 0  |

<img src = "images/corr_runs.png">

---

## Example

```{r runs, echo=TRUE}
vec_runs1 <- rep(c(0,1), times = 5)
vec_runs2 <- rep(c(0,1), each = 5)
vec_runs3 <- c(1, 0, 0, 1, 1, 1, 1, 0, 0, 0)

vec_runs1 # nonrandom
vec_runs2 # nonrandom
vec_runs3 # random
```

---

class: small

```{r runstest, echo = TRUE}
library(randtests)

runs.test(vec_runs1)
runs.test(vec_runs2)
runs.test(vec_runs3)
```


---

class: inverse, bottom

# Treatments vs. Control

---

## ANOVA

.pull-left[
Analysis of variance: A systematic way of variable comparison in the context of .red[experiment].

* Completely random design
* Equivalent to t-test
]

--

.pull-right[
When there are more than one treatment group, ANOVA is more useful to reduce the risk of Type I.

<img src = "images/ci_errorType.png">
]

---

## .red[ASSUMPTION]

1. Y normally distributed in eqch group (Type I &darr;)
1. Homogeneity of variance (Type I & II &darr;)
1. Independence of cases (Type I &darr;)
1. No large outliers (Type II &darr;)
1. Large sample (Type I & II &darr;)
    
    
---

## One-Way ANOVA

(Completely randomized design)

+ $\bar X$ is the sample mean; $\bar{\bar{X}} = \frac{\sum \bar{X_i}}{K}$, where K is column number, a.k.a., the grant mean, mean of all the data
+ SST: Variance between the samples;
+ SSE: Variance within the samples.

--

| Source    	| Sum Square                                    	| d.f.  	| Mean Square                      	|
|-----------	|-----------------------------------------------	|-------	|----------------------------------	|
| Treat 	| $SST = \sum n_i (\bar X_i - \bar{\bar{X}})^2$ 	| K - 1 	| MST = SST/(K - 1)                	|
| Error     	| $SSE = \sum \sum (X_{ik} - \bar{X_i})^2$      	| N - K 	| MSE = SSE/(N - K)                	|
| Total     	| $SS = SST + SSE$                              	| N - 1 	| $F_{\alpha, K-1, N-K} = MST/MSE$ 	|

$$F_{\alpha, K-1, N-1} = MST/MSE = \frac{Ratio\ of \ Explained\ Variance}{Ratio\ of\ Unexplained\ Variance}$$

---

## Example

The following table shows the funding applications of six faculty members of Xavier Institution. Does mutants willingness of application relate to the funding type? 

| NS 	| ME 	| BJ 	|
|-----	|-----	|----	|
| 27  	| 23  	| 48 	|
| 22  	| 36  	| 35 	|
| 33  	| 27  	| 46 	|
| 25  	| 44  	| 36 	|
| 38  	| 39  	| 28 	|
| 29  	| 32  	| 29 	|


---

class: small

.pull-left[

| NS 	| ME 	| BJ 	|
|-----	|-----	|----	|
| 27  	| 23  	| 48 	|
| 22  	| 36  	| 35 	|
| 33  	| 27  	| 46 	|
| 25  	| 44  	| 36 	|
| 38  	| 39  	| 28 	|
| 29  	| 32  	| 29 	|
]
.pull-right[
$H_0: \mu_1 = \mu_2 = \mu_3,$     
$H_1: \mu_1 \neq \mu_2 \neq \mu_3.$
&alpha; = 0.05.

\begin{align}
\bar X_{NS} &= 29;\\
\bar X_{ME} &= 33.5;\\
\bar X_{BJ} &= 37;\\
\bar{\bar{X}} &= (29 + 33.5 + 37)/3 \approx 33.17
\end{align}
]

--

| Source    	| Sum Square                                    	| d.f.  	| Mean Square                      	|
|-----------	|-----------------------------------------------	|-------	|----------------------------------	|
| Treat 	| $SST = \sum n_i (\bar X_i - \bar{\bar{X}})^2$ 	| K - 1 	| MST = SST/(K - 1)                	|
| Error     	| $SSE = \sum \sum (X_{ik} - \bar{X_i})^2$      	| N - K 	| MSE = SSE/(N - K)                	|
| Total     	| $SS = SST + SSE$                              	| N - 1 	| $F_{\alpha, K-1, N-K} = MST/MSE$ 	|


$\bar X_{NS} = 29; \bar X_{ME} = 33.5;\bar X_{BJ} = 37;\bar{\bar{X}} = 33.17$

Then, $SST = \sum 6\times (\bar X - \bar{\bar{X}}) = 193$; 
$SSE = [(27 - 29)^2 + (22 - 29)^2 + \dots + (29 - 37)^2] = 819.5$.

Given K = 3, N = 18, $F = \frac{193/(3 - 1)}{819.5/(18 - 3)} \approx 1.77$

We know F at the critical value 0.05, $F_{0.05, 2, 15}$ = `r qf(.975, 2, 15)` > F, so fail to reject $H_0$

---

class: middle, center

ANOVA is usually used when $K \in [2, 3]$;    
For larger K, often use [MANOVA](https://www.sciencedirect.com/topics/medicine-and-dentistry/multivariate-analysis-of-variance) (rarely used in Poli Sci)

---

## Two-Way ANOVA

* Two-way: 
    + Randomized block design (i blocks, j groups)
    + Matched sample in t-test

| Source    	| Sum Square                                                             	| d.f.           	| Mean Square                                    	|
|-----------	|------------------------------------------------------------------------	|----------------	|------------------------------------------------	|
| Treat 	| $SS_A: b\sum (\bar X_i - \bar{\bar{X}})^2$                            	| a - 1          	| MST_A = SS_A/(a - 1)                           	|
| Block     	| $SS_B: a\sum (\bar X_j - \bar{\bar{X}})^2$                            	| b - 1          	| MST_B = SS_B/(b - 1)                           	|
| Error     	| $SS_E: \sum^{ab} (X_{ij} - \bar{X_i} - \bar{X_j} + \bar{\bar{X}})^2$ 	| (a - 1)(b - 1) 	| $MSE = \frac{SSE}{(a - 1)(b - 1)}$                     	|
| Total     	| $SS: SS_A + SS_B + SS_E$                                              	| ab - 1         	| $F_{\alpha, a-1, b-1} = \frac{MST_{A or B}}{MSE}$ 	|

---

class: small

## Example

Students' final scores in three majors were recorded in the following table. Does the scores associate with majors?

| Major 	| Poli Sci 	| Sociology 	| Psychology 	|
|-------	|----------	|-----------	|------------	|
| A+    	| 41       	| 45        	| 51         	|
| A     	| 36       	| 38        	| 45         	|
| B+    	| 27       	| 33        	| 31         	|
| B     	| 32       	| 29        	| 35         	|
| C+    	| 26       	| 21        	| 32         	|
| C     	| 23       	| 25        	| 27         	|

$H_0: \mu_{PS} = \mu_{SO} = \mu_{PY},$  
$H_1: \mu_{PS} \neq \mu_{SO} \neq \mu_{PY}.$

&alpha; = 0.05.

--

$\bar X_{PS} = 30.8; \bar X_{SO} = 33.5; \bar X_{PY} = 36.83$;
$\bar A_+ = 45.67; \bar A = 39.67; \bar B_+ = 30.33; \bar B = 32; \bar C_+ = 29.67; \bar C = 25.$

$\bar{\bar{X}} = 33.72.$

---

$\bar X_{PS} = 30.8; \bar X_{SO} = 33.5; \bar X_{PY} = 36.83$;
$\bar A_+ = 45.67; \bar A = 39.67; \bar B_+ = 30.33; \bar B = 32; \bar C_+ = 29.67; \bar C = 25.$
$\bar{\bar{X}} = 33.72$


For factor A (major), $SS_A = 6\times [(30.8 - 33.72)^2 + (33.5 - 33.72)^2 + (36.83 - 33.72)^2] = 109.48$

$MSA = SS_A/(a - 1) = 109.48/(3 - 1)$

--

For factor B (GPA), $SS_B = 3\times [(45.67 - 33.72)^2 + \dots + (25 - 33.72)^2] = 854.94$

$MSB = 854.94 / (6 - 1)$

--

$SST = \sum^a\sum^b(X_{ij} - \bar{\bar{X}})^2 = 1015.61$
$SSE = SST - SS_A - SS_B = 52.2$

Then, $F_{(3 - 1), (3 - 1)(6 -1)} = MS_A/MSE = \frac{109.5/(3 -1)}{52.2/(3 - 1)(6 - 1)}$ is 10.4 > critical F `r round(qf(.975, 2, 10), digits = 4)`, reject $H_0$.

---

.center[.red[ASSUMPTION]]

1. Y normally distributed in eqch group (Type I &darr;)
1. Homogeneity of variance (Type I & II &darr;)
1. Independence of cases (Type I &darr;)
1. No large outliers (Type II &darr;)
1. Large sample (Type I & II &darr;)

--

When any one of them is not held,

## Nonparametric Version

Analysis of variance.red[-rank]
+ Kruskal-Wallis test;
+ Friedman test.

---

## One-Way Variance-Rank: Kruskal-Wallis Test

An extension of the two-sample Wilcoxon test

Procedure: 

+ Combine k samples and sorting
    + If the value are the same, use the average;
    + Let n<sub>k</sub> as the size of the k sample, and n = &sum;n<sub>i</sub>, i &isin; {1, 2, ..., k}.
+ Calculate the rank sum of each sample, R<sub>i</sub>
+ $H = \frac{12}{n(n + 1)}\sum^k_{i = 1}\frac{R^2_i}{n_i} - 3(n + 1) \sim \chi^2(k - 1).$

---

## Example

Botanists conducted an experiment of plant growth.
The data records the results obtained under a control and two different treatment conditions (as measured by dried weight of plants).

```{r kw-data}
group_by(PlantGrowth, group) %>%
  summarise(
    count = n(),
    mean = mean(weight, na.rm = TRUE),
    sd = sd(weight, na.rm = TRUE),
    median = median(weight, na.rm = TRUE),
    IQR = IQR(weight, na.rm = TRUE)
  ) %>% 
  knitr::kable()
```

--

```{r kw-test, echo=TRUE}
kruskal.test(weight ~ group, data = PlantGrowth)
```

---

## Two-Way Variance-Rank: Friedman Test

An extension of the sign test, and also a special case of Durbin test. 

--

E.g., Person's recovery score after taking four types of drugs. The type 1 is the placebo. 

```{r friedman-data}
#create data
df_drug <- data.frame(person = rep(1:5, each=4),
                   drug = rep(c(1, 2, 3, 4), times=5),
                   score = c(30, 28, 16, 34, 14, 18, 10, 22, 24, 20,
                             18, 30, 38, 34, 20, 44, 26, 28, 14, 30))

glimpse(df_drug)
```

--

```{r friedman-test, echo=TRUE}
friedman.test(y = df_drug$score,
              groups = df_drug$drug,
              blocks = df_drug$person)
```

&rArr; Different type of drugs lead to different type of outcomes.

---

## Wrap Up

.pull-left[

### Relations of Variable

+ Covariance
+ Correlation
    + Pearson's r
    + Kendall's &tau;
    + Spearman's &rho;
+ Non-linear comparison
    + &chi;<sup>2</sup>
    + &Phi; & Cramer's V

]

.pull-right[

### Nonparametric Tests

+ Sign test, Signed-rank test
+ Rank-sum test & runs test

### Treatments vs. Control

+ ANOVA (F test)
    + One-way/Two-way
+ Nonparametric version
    + Kruskal-Wallis Test
    + Friedman Test

]

<img src = "images/ci_fsmrof.png" height = 60>: &Chi;<sup>2</sup> is adding-up of n square normals representing variances; F is the ratio of two &chi;<sup>2</sup>s. 

---

<iframe src="https://player.bilibili.com/player.html?aid=842607310&bvid=BV1E54y1r76o&cid=248007878&page=2" scrolling="no" border="0" frameborder="yes" framespacing="0" allowfullscreen="true" height = "600" width = "1000"> </iframe>

