---
title: "Model Specification"
subtitle: "Analysis of Political Data (70700173)"
author: "Yue Hu"
institute: "Political Science, Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: 
      - ../../../css/zh-CN_custom.css
      - ../../../css/styles.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    chakra: ../../../libs/remark-latest.min.js # to show slides offline
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  flextable, ggeffects,
  knitr, # dependency
  descr, stringr, broom, tidyverse
) # data wrangling # data wrangling

# Functions preload
set.seed(114)
```

class: inverse, bottom

# Model Issues

---

## Common Model Issues

1. Functional form
1. Proxy variable
1. Measurement error
1. Model misspecification

---

class: small

## Functional Form

When there are two ways to specify the model:

\begin{align}
Y =& f_1(X_1, X_2, X_3) + u =\beta_1X_1 + \beta_2X_2 + \beta_3X_3 + u \\
Y =& f_2(X_1, X_2, X_3) + u =\beta_4X_1^2 + \beta_5X_2^3 + \beta_6X_3^4 + u
\end{align}

One may want to test the combined version: $$Y = f_1(X) + f_2(X) + u$$

--

### Concern

When one of the models, e.g., f<sub>1</sub> is the true model, the combined version produces unbiased, consistent, yet inefficient estimates: 

$$var(\hat\beta_1) = \frac{\sigma^2}{(X - \bar X)^2(1 - \rho_{12})}.$$

When &rho;<sub>12</sub> increases, the variance is inflated.

---

### Solution

Joint test: $H_0: \beta_1 = \beta_2 = \beta_3 = 0$ or $\beta_4 = \beta_5 = \beta_6 = 0$

Statistics: Davidson-Mackinnon test

1. Run $Y = f_2(X) + u$ and calculate the expected value $\hat Y = E[Y|f_2(X)]$
1. Run $Y = f_1(X) + \theta\hat Y + u$, and test if &theta; = 0.

---

class: small

## Proxy Variable

Given the true model is $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3^* + u$.

--

However, $X_3^*$ is unobserved, instead, we know $X_3^* = \delta_0 + \delta_1X_3 + v.$ In other words, what we estimate is:

$$Y = (\beta_0 + \beta_3\delta_0) + \beta_1X_1 + \beta_2X_2 + \beta_3\delta_1X_3 + (u + \beta_3v).$$

--

Assuming u and v are independent, then we know $E(u + \beta_3v|X) = 0$, $var(u + \beta_3v|X) = \beta_3^2\sigma_v^2 + \sigma_u^2$.

--

To produce unbiased estimates, one need to ensure that

$$cov(u, X_3) = cov(v, X_1) = cov(v, X_2) = 0.$$

---

## Measurement Error

### Y = Y<sup>*</sup> + e

\begin{align}
Y^* =& \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + u\\
Y =& \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + (u + e)
\end{align}

That is, the estimate of &beta; is unbiaed, yet var(&beta;) goes up.

---

class: small

### X = X<sup>*</sup> + e

If $E(eX) = 0$, then $Y = \beta_0 + \beta_1(X_1 - e) + u = \beta_0 + \beta_1X_1 + (u - \beta_1e)$.

--

If $E(eX) \neq 0$, then

\begin{align}
cov(e, X_1) =& E(eX_1) - E(e)E(X_1) = E(eX_1)\\
          =& E[e(X_1^* + e)] = E(eX_1^*) + E(e^2) = \sigma_e^2\neq0\\
cov(u - \beta_1e, X_1) =& cov(-\beta_1e, X_1) = -\beta_1cov(e, X_1)\\
                       =& -\beta_1\sigma_e^2.\\
\Rightarrow\ plim(\hat\beta_1) =& \beta_1 + \frac{cov(u - \beta_1e, X_1)}{var(X_1)} \\
                               =& \beta_1 + \frac{-\beta_1\sigma_e^2}{\sigma_{X_1}^2}
                               = \beta_1 + \frac{-\beta_1\sigma_e^2}{\sigma_{X_1}^{*2} + \sigma^2_e} \\
                               =& \frac{\sigma_{X_1}^{*2}}{\sigma_{X_1}^{*2} + \sigma^2_e}\beta_1.
\end{align}

???
plim: probability limit operation, convergence in probability

--

In this case, only when $var(X_1) = var(X_1^* + e)$, i.e., $\sigma^2_{X_1} = \sigma^{*2}_{X_1} + \sigma^2_e$, the estimate can be unbiased.

---

Otherwise, $|\hat\beta_1| < \beta_1$, a.k.a., .magenta[attenuation] bias. The parameter is underestimated and may affect the estimates of other variables in unknown ways.

---

## Misspecification

If the true model is $$Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + u_i,$$ but we specify it as $$Y_i = \tilde\beta_0 + \tilde\beta_1X_{1i} + \tilde u_i,$$ how does $\tilde\beta_1$ compare to $\beta_1$?


---

class: small

## &beta;

\begin{align}
Y_i =& \beta_0 + \beta_1X_{1} + \beta_2X_{2} + u_i\\
Y_i -\bar Y =& \beta_1(X_{1} - \bar X_1) + \beta_2(X_{2} - \bar X_2) + (u_i - \bar u)\\
(Y_i -\bar Y)(X_1 - \bar X_1) =& \beta_1(X_{1} - \bar X_1)^2 + \beta_2(X_{2} - \bar X_2)(X_1 - \bar X_1)\\ 
            &+ (u_i - \bar u)(X_1 - \bar X_1)\\
\frac{(Y_i -\bar Y)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} =& \beta_1\frac{\sum(X_{1} - \bar X_1)^2}{\sum(X_1 - \bar X_1)^2} + \beta_2\frac{\sum(X_{2} - \bar X_2)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}\\ 
           &+ \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}\\
\text{That is, } \tilde\beta_1 =& \beta_1 + \beta_2\hat\delta_1 + \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2},
\end{align}

where $\hat\delta_1$ is the regression coefficient of $X_2$ on $X_1$.

---

class: small

\begin{align}
E(\tilde\beta_1|X_1) =& E(\beta_1|X_1) + E(\beta_2\hat\delta_1|X_1) + E[\frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}|X_1]\\
                     =& \hat\beta_1 + \hat\beta_2E(\hat\delta_1|X_1) + \frac{\sum(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}E(u_i|X_1)\\
                     =& \hat\beta_1 + \hat\beta_2\delta_1
\end{align}

--

Given $$X_1 = \delta_0 + \delta_1X_2 + r,$$ if &delta;<sub>1</sub> = 0, $X_1 = \delta_0 + r$, the model may increase the risk of [Type I error](https://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg); 

--

Otherwise, the model missing a variable will always produce an biased estimation unless X<sub>2</sub> is an irrelevant variable.

---

## Error

$$Y_i = \beta_0 + \beta_1X_{1} + \epsilon,$$

$$\epsilon = \beta_2X_{2} + u_i.$$


* $E(\epsilon|X_1) = E(\beta_2X_{2} + u_i) = \beta\mu_2\neq0.$

--

* $var(\epsilon|X_1)$?

--

\begin{align}
var(\epsilon|X_1) =& var(\beta_2X_{2} + u_i|X_1)\\ 
=& \beta_2^2\sigma_2^2 + \sigma_u^2 + 2\beta_2cov(X2, u) = \beta_2^2\sigma_2^2 + \sigma_u^2
\end{align}

All values are fixed, that is, still homoscedasitic.
    
---

* $E(\epsilon_i\epsilon_j|X_i)$ ?

--

\begin{align}
& E(\epsilon_i\epsilon_j) - E(\epsilon_i)E(\epsilon_j) \\
=& E[(\beta_2X_{2i} + u_i)(\beta_2X_{2j} + u_j)] - \beta_2^2\mu^2_{X_2}\\
=& E(\beta_2^2X_{2i}X_{2j} + \beta_2^2X_{2i}u_{j} + \beta_2^2X_{2j}u_{i} + u_iu_j) \\
 &- \beta_2^2\mu^2_{X_2}\\
=& \beta_2^2E(X_{2i}X_{2j}) - \beta_2^2\mu^2_{X_2}\\
=& \beta_2^2cov(X_{2i}, X_{2j}) - \beta_2^2\mu^2_{X_2} + \beta_2^2\mu^2_{X_2} \\
=& \beta_2^2cov(X_{2i}, X_{2j}).
\end{align}

--

Only when $\beta_2^2cov(X_{2i}, X_{2j}) = 0$ (which is often not) there's no autocorrelation


---

* $E(X_1\epsilon)$?

--

\begin{align}
E(X_1\epsilon) =& E[X_1(\beta_2X_2 + u)] \\
=& E(X_1\beta_2X_2) + E(X_1u)\\
=& \beta_2E(X_1X_2)\\
=& \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}]\\
When&\ cov(X_1,X_2)\neq 0.
\end{align}

Increase the difficulty to isolate X from u
    
---

class: small

## Type of Measurement

* Nominal: Order doesn't matter
    + e.g., A race variable: white, black, native
        + H<sub>0</sub>: &beta;<sub>black</sub> = 0, testing the difference between black and white
        + H<sub>0</sub>: &beta;<sub>black</sub> = &beta;<sub>native</sub> = 0, testing if the race has any effect.
    + Can't do regression unless being broken up into indicator variables.
--

* Indicator: Binary usually, "when x has a value of X, Y is 1", &#x1D7D9;(x = X).
    + In a regression, the information of the baseline is captured by the intercept
    + e.g., $Y = \beta_0 + \beta_1X_i + u_i,$ where X is either male (0) or female(1), then

$$\beta_0: E(Y|X = male); \beta_1: E(Y|X = female) - E(Y|X = male)$$

???

blackboard 1

---

* Ordinal: 
    + Non-interval (only the order matters); 
    + Interval (same intervals)

--

* Continuous (ratio): Can credibly calculate the marginal effects, because marginal effects are derivatives, and only continuous variable can do derivative; other types can do this mathematically, but not accurately.

???

Interval: meaningful distance, feeling thermometer's 0 is not nothing but a strong feeling
Ratio: interval variables with meaningful zero value (meaning absence); same ratio conveys the same meaning

e.g., -50 to 0 to 50 find two pair of scores with a ratio of 1:3, transform them into 0-100 scale, the ratio changes.

---

## Nonlinear effect

e.g., Y increases with X ( $\frac{\partial Y}{\partial X} > 0$ ) at a decreasing rate ( $\frac{\partial^2 Y}{\partial^2 X} < 0$ )

---

```{r nonlinear, fig.width=8, fig.height=4, fig.show='hold'}

df_nl1 <- tibble(x = seq(0, 3,length.out = 1000),
                y = 4*x - x^2 + rnorm(1000,0,sd = 1))

ggplot(data = df_nl1, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u))

df_nl2 <- tibble(x = seq(0, 10,length.out = 1000),
                 y = 3 + log(x) + rnorm(1000,0,sd = 1))

ggplot(data = df_nl2, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0] + beta[1]*ln(X) + u))
```

---

## Marginal Effect

Discrete:

$$Pr(Y|x = X_{n + 1}) - Pr(Y|x = X_n)$$

Continuous: 

$$\lim_{\Delta x\to0} \frac{ f(x + \Delta x) - f(x)}{\Delta x}$$

--

.magenta[Hint]: The marignal effect of an OLS is its &beta;s

---

### Averagte Marginal Effect (AME)

1. Calculate the marginal effect of each variable x for .magenta[each observation].
1. Calculate the average.

--

### Marginal Effect at the Mean (MEM)

1. Calculate the marginal effect of each variable x for .magenta[each's mean value].

--

### Marginal Effect at Representative Values (MER)

1. Calculate the marginal effect of each variable x for .magenta[value(s) of interest].

---

class: small

## Marginal Effect of A Nonlinear Transformation

$$Y = \beta_0 + \beta_1X + \beta_2X^2 + u$$

Margins: $\frac{\partial Y}{\partial X} = \beta_1 + 2\beta_2X$

* H<sub>0</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X = 0;
* H<sub>1</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X > 0

That is, if the increasing speed is positive.

Level set: &alpha; = 0.05

Statistics: $\frac{\beta_1 + 2\beta_2X - 0}{SE(\beta_1 + 2\beta_2X)}\sim t_{n - 3}$

\begin{align}
SE(\beta_1 + 2\beta_2X) =& \sqrt{var(\beta_1 + 2\beta_2X)} \\
=& \sqrt{var(\beta_1) + 4X^2var(\beta_2) + 4Xcov(\hat\beta_1,\hat\beta_2)}
\end{align}

---

.magenta[NB]: One can't simply say if the null hypothesis is rejected, because it may not be a coherent conclusion in the entire domain of X, due to the nonlinearity. So, a better way to describe it is "in the range from a to b, the hypothesis can be rejected."

---

## Substantive Significance

* Max - min
* First difference
* Marginal effects

--

```{r substantive, fig.width=10, fig.height=5, fig.show='hold'}

df_nl3 <- tibble(x = seq(-5, 5,length.out = 1000),
                y = x - x^2 + rnorm(1000,0,sd = 10))

fit1 <- lm(y ~ x, data = df_nl3)
me1 <- ggeffect(fit1, terms = "x")

ggplot(data = df_nl3, aes(x, y, color = "Predicted Values")) +
  geom_smooth() + 
  geom_line(data = me1, aes(x, predicted, color = "Marginal Effect")) +
  geom_ribbon(data = me1, aes(x, predicted, ymin=conf.low,ymax=conf.high), alpha = 0.3) +
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u)) +
  ylab("E(Y|X)") + xlab("X")

```
