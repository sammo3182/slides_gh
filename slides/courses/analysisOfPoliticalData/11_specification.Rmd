---
title: "Model Specification"
subtitle: "Analysis of Political Data (70700173)"
author: "Yue Hu"
institute: "Political Science, Tsinghua University"
output:
  xaringan::moon_reader:
    self_contained: FALSE
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - style_ui.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)


if (!require(pacman)) install.packages("pacman")
library(pacman)

p_load(
  flextable, ggeffects,
  knitr, # dependency
  descr, stringr, broom, tidyverse
) # data wrangling # data wrangling

# Functions preload
set.seed(313)
```

## Overview

"How can you set things wrong?"

### Measuring wrong variables

1. Proxy variable
1. Measurement error

### Specifying a wrong model

1. Functional form 
1. Omitted variable

### Interpreting the result wrong

1. Nonlinear & substantive significance

---

class: inverse, bottom

# Measurement Problem

---

class: small

## Proxy Variable

True model: Y = &beta;<sub>0</sub>  + &beta;<sub>1</sub>X<sub>1</sub> + &beta;<sub>2</sub>X<sub>2</sub> + &beta;<sub>3</sub>X<sub>3</sub><sup>*</sup> + u.

--

However, X<sub>3</sub><sup>\*</sup> is unobserved, instead, we know X<sub>3</sub><sup>*</sup> = &delta;<sub>0</sub> + &delta;<sub>1</sub>X<sub>3</sub> + v. 

In other words, 

.center[Y = (&beta;<sub>0</sub> + &beta;<sub>3</sub>&delta;<sub>0</sub>) + &beta;<sub>1</sub>X<sub>1</sub> + &beta;<sub>2</sub>X<sub>2</sub> + &beta;<sub>3</sub>&delta;<sub>1</sub>X<sub>3</sub> + (u + &beta;<sub>3</sub>v).]

--

Assuming cov(u,v) = 0, 

.center[E(u + &beta;<sub>3</sub>v|X) = 0;]

.center[var(u + &beta;<sub>3</sub>v|X) = &beta;<sub>3</sub><sup>2</sup>&sigma;<sub>v</sub><sup>2</sup> + &sigma;<sub>u</sub><sup>2</sup>.]

???

cov(u,v) = 0: u, v independent

--

To produce unbiased estimates:

.center[cov(u, X<sub>3</sub>) = cov(v, X<sub>1</sub>) = cov(v, X<sub>2</sub>) = 0.]

If so, .center[<img src="images/dropMic.gif" height = 100 />]

---

## Measurement Error

1. Measurement in Y;
1. Measurement in X.

---

## Y = Y<sup>*</sup> + e

\begin{align}
Y^* =& \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + u\\
Y =& \beta_0 + \beta_1X_1 +\cdots+ \beta_kX_k + (u + e)
\end{align}

### Consequence

+ &beta; Unbiased 
+ var(&beta;) &uarr;

---

class: small

### X = X<sup>*</sup> + e<sup><img src="images/fsmrof.png" height="20"/></sup>

If E(eX) = 0, then Y = &beta;<sub>0</sub> + &beta;<sub>1</sub>(X<sub>1</sub> - e) + u = &beta;<sub>0</sub> + &beta;<sub>1</sub>X<sub>1</sub> + (u - &beta;<sub>1</sub>e).

--

If E(eX) &ne; 0, then

\begin{align}
\hat\beta_1 =& \beta_1 + \frac{cov(u - \beta_1e, X_1)}{var(X_1)} \\
                               =& \beta_1 + \frac{-\beta_1\sigma_e^2}{\sigma_{X_1}^2}
                               = \beta_1 + \frac{-\beta_1\sigma_e^2}{\sigma_{X_1}^{*2} + \sigma^2_e} \\
                               =& \frac{\sigma_{X_1}^{*2}}{\sigma_{X_1}^{*2} + \sigma^2_e}\beta_1.
\end{align}

???
\begin{align}
cov(e, X_1) =& E(eX_1) - E(e)E(X_1) = E(eX_1)\\
          =& E[e(X_1^* + e)] = E(eX_1^*) + E(e^2) = \sigma_e^2\neq0\\
cov(u - \beta_1e, X_1) =& cov(-\beta_1e, X_1) = -\beta_1cov(e, X_1)\\
                       =& -\beta_1\sigma_e^2.\\
\Rightarrow\ plim(\hat\beta_1) =& \beta_1 + \frac{cov(u - \beta_1e, X_1)}{var(X_1)} \\
                               =& \beta_1 + \frac{-\beta_1\sigma_e^2}{\sigma_{X_1}^2}
                               = \beta_1 + \frac{-\beta_1\sigma_e^2}{\sigma_{X_1}^{*2} + \sigma^2_e} \\
                               =& \frac{\sigma_{X_1}^{*2}}{\sigma_{X_1}^{*2} + \sigma^2_e}\beta_1.
\end{align}


plim: probability limit operation, convergence in probability

--

Unbiased .navy[only] when var(X<sub>1</sub>) = var(X<sub>1</sub><sup>\*</sup> + e), i.e., &sigma;<sup>2</sup><sub>X<sub>1</sub></sub> = &sigma;X<sub>1</sub><sup>\*2</sup>+ &sigma;<sub>e</sub><sup>2</sup>

Otherwise, $|\hat\beta_1| < \beta_1$, a.k.a., .red[attenuation] bias. 

+ &beta;: underestimated;
+ Affecting the estimations of others in unknown ways `r emo::ji("scream")`.

---

class: small

## Type of Measurement

* Nominal: 
    + e.g., A race variable: white, black, native
        + H<sub>0</sub>: &beta;<sub>black</sub> = 0, testing the difference between black and white
        + H<sub>0</sub>: &beta;<sub>black</sub> = &beta;<sub>native</sub> = 0, testing if the race has any effect.
    + Can't do regression unless being .navy[broken up into indicator] variables.
--

* Indicator: Binary usually
    + $Y = \beta_0 + \beta_1X_i + u_i,$ where X is either male (0) or female(1)
        + &beta;<sub>0</sub>: E(Y|X = male);
        + &beta;<sub>1</sub>: E(Y|X = female) - E(Y|X = male).

???

In a regression, the information of the baseline is captured by the intercept

---

class: inverse, bottom

# Specification Problems

---

## Functional Form

When there are two ways to specify the model:

\begin{align}
Y =& f_1(X_1, X_2, X_3) + u =\beta_1X_1 + \beta_2X_2 + \beta_3X_3 + u \\
Y =& f_2(X_1, X_2, X_3) + u =\beta_4X_1^2 + \beta_5X_2^3 + \beta_6X_3^4 + u
\end{align}

Which specification is .red[correct]?

---

Test Y = f<sub>1</sub>(X) + f<sub>2</sub>(X) + u?

--

### Concern

Hey, buddy, how many models are correct?

???

Only one true model 

--

Let's assume f<sub>1</sub> is the true model:

$$var(\hat\beta_1) = \frac{\sigma^2}{(X_1 - \bar X_1)^2(1 - \rho_{12})}.$$

When &rho;<sub>12</sub> &uarr;......

???

&rho;<sub>12</sub>: correlations between x1 and x2, such as between x and x……2

Variance is inflated when &rho;<sub>12</sub> &uarr;

---

### Solution

.center[Y = f<sub>1</sub>(X) + f<sub>2</sub>(X) + u]

Joint test: H<sub>0</sub>: &beta;<sub>1</sub> = &beta;<sub>2</sub> = &beta;<sub>3</sub> = 0 or &beta;<sub>4</sub> = &beta;<sub>5</sub> = &beta;<sub>6</sub> = 0

Statistics: Davidson-Mackinnon test

1. Run $Y = f_2(X) + u$ and calculate the expected value $\hat Y = E[Y|f_2(X)]$
1. Run $Y = f_1(X) + \theta\hat Y + u$, and test if &theta; = 0.

--

.center[<img src="images/dropMic.gif" height = 150 />]

---

class: small

## Omitted Variable<sup><img src="images/fsmrof.png" height="20"/></sup>

\begin{align}
True: Y_i =& \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + u_i\\
Specified: Y_i =& \tilde\beta_0 + \tilde\beta_1X_{1i} + \tilde u_i
\end{align}

How does $\tilde\beta_1$ compare to $\beta_1$?

--

\begin{align}
\tilde\beta_1 =& \beta_1 + \beta_2\hat\delta_1 + \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2},\\
E(\tilde\beta_1|X_1) =& \hat\beta_1 + \hat\beta_2\delta_1
\end{align}

where $\hat\delta_1$ is the regression coefficient of $X_2$ on $X_1$ ( $X_1 = \delta_0 + \delta_1X_2 + r$).


$\tilde\beta_1$: .red[Biased], unless X<sub>2</sub> is an irrelevant variable.

Even if &delta;<sub>1</sub> = 0, $X_1 = \delta_0 + r$, the model may increase the risk of [Type I error](https://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg).

???

\begin{align}
Y_i =& \beta_0 + \beta_1X_{1} + \beta_2X_{2} + u_i\\
Y_i -\bar Y =& \beta_1(X_{1} - \bar X_1) + \beta_2(X_{2} - \bar X_2) + (u_i - \bar u)\\
(Y_i -\bar Y)(X_1 - \bar X_1) =& \beta_1(X_{1} - \bar X_1)^2 + \beta_2(X_{2} - \bar X_2)(X_1 - \bar X_1)\\ 
            &+ (u_i - \bar u)(X_1 - \bar X_1)\\
\frac{(Y_i -\bar Y)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2} =& \beta_1\frac{\sum(X_{1} - \bar X_1)^2}{\sum(X_1 - \bar X_1)^2} + \beta_2\frac{\sum(X_{2} - \bar X_2)(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}\\ 
           &+ \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}\\
\text{That is, } \tilde\beta_1 =& \beta_1 + \beta_2\hat\delta_1 + \frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2},
\end{align}

\begin{align}
E(\tilde\beta_1|X_1) =& E(\beta_1|X_1) + E(\beta_2\hat\delta_1|X_1) + E[\frac{\sum u_i(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}|X_1]\\
                     =& \hat\beta_1 + \hat\beta_2E(\hat\delta_1|X_1) + \frac{\sum(X_1 - \bar X_1)}{\sum(X_1 - \bar X_1)^2}E(u_i|X_1)\\
                     =& \hat\beta_1 + \hat\beta_2\delta_1
\end{align}

---

background-image: url("images/errorType.png")
background-position: center
background-size: contain

---

class: small

## How About the Residual<sup><img src="images/fsmrof.png" height="20"/></sup>

$$Y_i = \beta_0 + \beta_1X_{1} + \epsilon,$$

$$\epsilon = \beta_2X_{2} + u_i.$$


+ $E(\epsilon|X_1) = E(\beta_2X_{2} + u_i) = \beta\mu_2\neq0$
    + Against the assumption.
    
--

+ $E(X_1\epsilon) = \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}],$ when cov(X<sub>1</sub>, X<sub>2</sub>)&ne;0
    + Increase the difficulty to isolate X from u.
    
--

+ $E(\epsilon_i\epsilon_j|X_i) = \beta_2^2cov(X_{2i}, X_{2j})$
    + Only when $\beta_2^2cov(X_{2i}, X_{2j}) = 0$ (which is often not) there's no autocorrelation.


???

$var(\epsilon|X_1)$?


\begin{align}
var(\epsilon|X_1) =& var(\beta_2X_{2} + u_i|X_1)\\ 
=& \beta_2^2\sigma_2^2 + \sigma_u^2 + 2\beta_2cov(X2, u) = \beta_2^2\sigma_2^2 + \sigma_u^2
\end{align}

All values are fixed, that is, still homoscedasitic.

* $E(\epsilon_i\epsilon_j|X_i)$ ?

\begin{align}
& E(\epsilon_i\epsilon_j) - E(\epsilon_i)E(\epsilon_j) \\
=& E[(\beta_2X_{2i} + u_i)(\beta_2X_{2j} + u_j)] - \beta_2^2\mu^2_{X_2}\\
=& E(\beta_2^2X_{2i}X_{2j} + \beta_2^2X_{2i}u_{j} + \beta_2^2X_{2j}u_{i} + u_iu_j) \\
 &- \beta_2^2\mu^2_{X_2}\\
=& \beta_2^2E(X_{2i}X_{2j}) - \beta_2^2\mu^2_{X_2}\\
=& \beta_2^2cov(X_{2i}, X_{2j}) - \beta_2^2\mu^2_{X_2} + \beta_2^2\mu^2_{X_2} \\
=& \beta_2^2cov(X_{2i}, X_{2j}).
\end{align}

* $E(X_1\epsilon)$?

\begin{align}
E(X_1\epsilon) =& E[X_1(\beta_2X_2 + u)] \\
=& E(X_1\beta_2X_2) + E(X_1u)\\
=& \beta_2E(X_1X_2)\\
=& \beta_2[cov(X_1,X_2) - \mu_{X_1}\mu_{X_2}]\\
When&\ cov(X_1,X_2)\neq 0.
\end{align}

---

## Nonlinear effect

e.g., Y increases with X ( $\frac{\partial Y}{\partial X} > 0$ ) at a decreasing rate ( $\frac{\partial^2 Y}{\partial^2 X} < 0$ )

---

```{r nonlinear, fig.width=8, fig.height=4, fig.show='hold'}

df_nl1 <- tibble(x = seq(0, 3,length.out = 1000),
                y = 4*x - x^2 + rnorm(1000,0,sd = 1))

ggplot(data = df_nl1, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u))

df_nl2 <- tibble(x = seq(0, 10,length.out = 1000),
                 y = 3 + log(x) + rnorm(1000,0,sd = 1))

ggplot(data = df_nl2, aes(x, y)) +
  geom_smooth() + 
  ggtitle(expression(Y == beta[0] + beta[1]*ln(X) + u))
```

---

Unbiased efficient estimates, but how to explain it? 

--

## Marginal Effect

Discrete:

$$Pr(Y|x = X_{n + 1}) - Pr(Y|x = X_n)$$

Continuous: 

$$\lim_{\Delta x\to0} \frac{ f(x + \Delta x) - f(x)}{\Delta x}$$


.navy[Hint]: The marignal effect of an OLS is its &beta;s

---

### Averagte Marginal Effect (AME)

1. Calculate the marginal effect of each variable x for .red[each observation].
1. Calculate the average.

--

### Marginal Effect at the Mean (MEM)

1. Calculate the marginal effect of each variable x for .red[each's mean value].

--

### Marginal Effect at Representative Values (MER)

1. Calculate the marginal effect of each variable x for .red[value(s) of interest].

---

class: small

## Marginal Effect of A Nonlinear Transformation

$$Y = \beta_0 + \beta_1X + \beta_2X^2 + u$$

Margins: $\frac{\partial Y}{\partial X} = \beta_1 + 2\beta_2X$

* H<sub>0</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X = 0;
* H<sub>1</sub>: &beta;<sub>1</sub> + 2&beta;<sub>2</sub>X > 0

That is, if the increasing speed is positive.

Level set: &alpha; = 0.05

Statistics: $\frac{\beta_1 + 2\beta_2X - 0}{SE(\beta_1 + 2\beta_2X)}\sim t_{n - 3}$

\begin{align}
SE(\beta_1 + 2\beta_2X) =& \sqrt{var(\beta_1 + 2\beta_2X)} \\
=& \sqrt{var(\beta_1) + 4X^2var(\beta_2) + 4Xcov(\hat\beta_1,\hat\beta_2)}
\end{align}

---

## Attention!!

One can't simply say if the null hypothesis is rejected, because it may .red[not be a coherent conclusion] in the entire domain of X, due to the .navy[nonlinearity]. 

A better way to describe: "In the range from a to b, the hypothesis can be rejected."

Or

Just plot it.

---

## Substantive Significance

* Max - min
* First difference
* Marginal effects

--

```{r substantive, fig.width=10, fig.height=5, fig.show='hold'}

df_nl3 <- tibble(x = seq(-5, 5,length.out = 1000),
                y = x - x^2 + rnorm(1000,0,sd = 10))

fit1 <- lm(y ~ x, data = df_nl3)
me1 <- ggeffect(fit1, terms = "x")

ggplot(data = df_nl3, aes(x, y, color = "Predicted Values")) +
  geom_smooth() + 
  geom_line(data = me1, aes(x, predicted, color = "Marginal Effect")) +
  geom_ribbon(data = me1, aes(x, predicted, ymin=conf.low,ymax=conf.high), alpha = 0.3) +
  ggtitle(expression(Y == beta[0]*X + beta[1]*X^2 + u)) +
  ylab("E(Y|X)") + xlab("X")

```

---

## Wrap Up

### Measuring wrong variables

1. Proxy variable
    + Unbiased when cov(u, X<sub>3</sub>) = cov(v, X<sub>1</sub>) = cov(v, X<sub>2</sub>) = 0.
    
1. Measurement error
    + In Y
        + &beta; Unbiased, var(&beta;) &uarr;
    + In X
        + Attenuation bias (&darr;)

1. Type of measurements (normal/binary)

---

### Specifying a wrong model

1. Functional form 
    + Davidson-Mackinnon test
1. Omitted variable: .red[Biased]

## Interpreting issues

1. Nonlinear &rArr; marginal effect
1. Substantive significance
